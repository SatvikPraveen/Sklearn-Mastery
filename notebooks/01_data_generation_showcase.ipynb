{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2326c404",
   "metadata": {},
   "source": [
    "# Advanced Data Generation Showcase\n",
    "\n",
    "This notebook demonstrates the comprehensive synthetic data generation capabilities of the sklearn-mastery project. We'll create sophisticated datasets specifically designed to showcase different machine learning algorithms' strengths, weaknesses, and optimal use cases across various data characteristics and complexity levels.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Results Management System](#results)\n",
    "3. [Linear Regression Dataset Spectrum](#linear-regression)\n",
    "4. [Classification Complexity Hierarchy](#classification)\n",
    "5. [Clustering Pattern Showcase](#clustering)\n",
    "6. [Special Purpose Datasets](#special)\n",
    "7. [Comparative Algorithm Analysis](#analysis)\n",
    "8. [Interactive Dataset Explorer](#explorer)\n",
    "9. [Performance Benchmarking](#benchmarking)\n",
    "10. [Comprehensive Results Saving](#saving)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49fe652",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d6f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, r2_score, silhouette_score, classification_report\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Results management imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.generators import SyntheticDataGenerator\n",
    "from pipelines.pipeline_factory import PipelineFactory\n",
    "from evaluation.metrics import ModelEvaluator\n",
    "from evaluation.visualization import ModelVisualizationSuite\n",
    "from utils.helpers import DataUtils\n",
    "\n",
    "print(\"‚úÖ Advanced Data Generation Showcase Setup Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00127026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure plotting for premium appearance\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"üìä Matplotlib backend: {plt.get_backend()}\")\n",
    "print(f\"üé® Color palette: {sns.color_palette().as_hex()}\")\n",
    "print(\"üé® Plotting configuration optimized for premium visualizations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ba51f",
   "metadata": {},
   "source": [
    "## 2. Results Management System {#results}\n",
    "\n",
    "Advanced results management system for saving models, figures, and comprehensive analysis reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Results Management System\n",
    "def setup_results_directories():\n",
    "    \"\"\"Create comprehensive results directory structure.\"\"\"\n",
    "    base_dir = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "    results_dir = base_dir / 'results'\n",
    "    \n",
    "    # Create comprehensive subdirectories\n",
    "    directories = [\n",
    "        'figures', 'models', 'reports', 'experiments',\n",
    "        'data_generation', 'algorithm_analysis', 'benchmarks',\n",
    "        'interactive_analysis', 'comparative_studies'\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        (results_dir / directory).mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"üìÅ Created/verified: {results_dir / directory}\")\n",
    "    \n",
    "    return results_dir\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\"Get formatted timestamp for file naming.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def save_figure(fig, name, description=\"\", category=\"general\", dpi=300):\n",
    "    \"\"\"Save figure with comprehensive metadata.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_data_generation_{category}_{name}.png\"\n",
    "    filepath = results_dir / 'figures' / filename\n",
    "    \n",
    "    # Save figure\n",
    "    fig.savefig(filepath, dpi=dpi, bbox_inches='tight', facecolor='white')\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'filename': filename,\n",
    "        'description': description,\n",
    "        'category': category,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '01_data_generation_showcase',\n",
    "        'dpi': dpi,\n",
    "        'figure_size': fig.get_size_inches().tolist()\n",
    "    }\n",
    "    \n",
    "    metadata_file = results_dir / 'figures' / f\"{timestamp}_data_generation_{category}_{name}_metadata.json\"\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Figure saved: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_model(model, name, description=\"\", category=\"general\", metadata=None):\n",
    "    \"\"\"Save model with comprehensive metadata.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_data_generation_{category}_{name}.joblib\"\n",
    "    filepath = results_dir / 'models' / filename\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, filepath, compress=3)\n",
    "    \n",
    "    # Create comprehensive metadata\n",
    "    model_metadata = {\n",
    "        'filename': filename,\n",
    "        'model_name': name,\n",
    "        'description': description,\n",
    "        'category': category,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '01_data_generation_showcase',\n",
    "        'model_type': model.__class__.__name__ if hasattr(model, '__class__') else str(type(model)),\n",
    "        'model_params': model.get_params() if hasattr(model, 'get_params') else {},\n",
    "        'file_size_mb': filepath.stat().st_size / (1024*1024) if filepath.exists() else 0\n",
    "    }\n",
    "    \n",
    "    if metadata:\n",
    "        model_metadata.update(metadata)\n",
    "    \n",
    "    metadata_file = results_dir / 'models' / f\"{timestamp}_data_generation_{category}_{name}_metadata.json\"\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(model_metadata, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üíæ Model saved: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_experiment_results(experiment_name, results, description=\"\", category=\"general\"):\n",
    "    \"\"\"Save experiment results with detailed configuration.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_data_generation_{category}_{experiment_name}.json\"\n",
    "    filepath = results_dir / 'experiments' / filename\n",
    "    \n",
    "    experiment_data = {\n",
    "        'experiment_name': experiment_name,\n",
    "        'description': description,\n",
    "        'category': category,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '01_data_generation_showcase',\n",
    "        'results': results,\n",
    "        'system_info': {\n",
    "            'python_version': sys.version,\n",
    "            'numpy_version': np.__version__,\n",
    "            'pandas_version': pd.__version__\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(experiment_data, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üíæ Experiment results saved: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_report(content, name, description=\"\", category=\"general\", format='txt'):\n",
    "    \"\"\"Save comprehensive analysis report.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_data_generation_{category}_{name}.{format}\"\n",
    "    filepath = results_dir / 'reports' / filename\n",
    "    \n",
    "    if format == 'txt':\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(content)\n",
    "    elif format == 'json':\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(content, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üíæ Report saved: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "# Initialize results directories\n",
    "results_dir = setup_results_directories()\n",
    "print(f\"\\nüìä Results will be saved to: {results_dir}\")\n",
    "print(\"üîß Results management system initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0a8717",
   "metadata": {},
   "source": [
    "## 3. Linear Regression Dataset Spectrum {#linear-regression}\n",
    "\n",
    "Let's create a comprehensive spectrum of regression datasets designed to demonstrate the nuanced differences between various regression algorithms and their optimal use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0fab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize advanced data generator\n",
    "print(\"üîÑ Initializing Advanced Synthetic Data Generator...\")\n",
    "\n",
    "generator = SyntheticDataGenerator(random_state=42)\n",
    "\n",
    "# Create regression dataset spectrum\n",
    "print(\"\\nüéØ Generating Regression Dataset Spectrum...\")\n",
    "\n",
    "regression_datasets = {}\n",
    "\n",
    "# Dataset 1: Perfect for showcasing multicollinearity effects\n",
    "print(\"  Creating multicollinearity demonstration dataset...\")\n",
    "X_collinear, y_collinear, true_coef = generator.regression_with_collinearity(\n",
    "    n_samples=1200,\n",
    "    n_features=25,\n",
    "    collinear_groups=[(0, 1, 2, 3), (7, 8, 9), (15, 16, 17, 18, 19)],\n",
    "    noise_variance=0.15,\n",
    "    coefficient_sparsity=0.4\n",
    ")\n",
    "\n",
    "regression_datasets['Multicollinear'] = {\n",
    "    'X': X_collinear, 'y': y_collinear, 'true_coef': true_coef,\n",
    "    'description': 'High multicollinearity with sparse true coefficients',\n",
    "    'optimal_algorithm': 'Ridge Regression',\n",
    "    'challenge': 'multicollinearity'\n",
    "}\n",
    "\n",
    "print(f\"    Generated: {X_collinear.shape}, True non-zero coefficients: {np.sum(true_coef != 0)}/{len(true_coef)}\")\n",
    "\n",
    "# Dataset 2: High-dimensional sparse regression\n",
    "print(\"  Creating high-dimensional sparse regression dataset...\")\n",
    "X_sparse, y_sparse, coef_sparse = generator.high_dimensional_regression(\n",
    "    n_samples=800,\n",
    "    n_features=100,\n",
    "    n_informative=12,\n",
    "    noise_variance=0.1\n",
    ")\n",
    "\n",
    "regression_datasets['High_Dimensional'] = {\n",
    "    'X': X_sparse, 'y': y_sparse, 'true_coef': coef_sparse,\n",
    "    'description': 'High-dimensional with very sparse coefficients',\n",
    "    'optimal_algorithm': 'Lasso Regression',\n",
    "    'challenge': 'high_dimensionality'\n",
    "}\n",
    "\n",
    "print(f\"    Generated: {X_sparse.shape}, Active features: {np.sum(coef_sparse != 0)}\")\n",
    "\n",
    "# Dataset 3: Nonlinear relationships for comparison\n",
    "print(\"  Creating nonlinear regression dataset...\")\n",
    "X_nonlinear, y_nonlinear = generator.nonlinear_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=8,\n",
    "    nonlinearity_type='polynomial',\n",
    "    noise_variance=0.2\n",
    ")\n",
    "\n",
    "regression_datasets['Nonlinear'] = {\n",
    "    'X': X_nonlinear, 'y': y_nonlinear, 'true_coef': None,\n",
    "    'description': 'Polynomial nonlinear relationships',\n",
    "    'optimal_algorithm': 'Random Forest',\n",
    "    'challenge': 'nonlinearity'\n",
    "}\n",
    "\n",
    "print(f\"    Generated: {X_nonlinear.shape}\")\n",
    "\n",
    "# Dataset 4: Robust regression scenario (with outliers)\n",
    "print(\"  Creating robust regression dataset with outliers...\")\n",
    "X_robust, y_robust, coef_robust = generator.regression_with_outliers(\n",
    "    n_samples=900,\n",
    "    n_features=15,\n",
    "    outlier_fraction=0.1,\n",
    "    outlier_strength=3.0\n",
    ")\n",
    "\n",
    "regression_datasets['With_Outliers'] = {\n",
    "    'X': X_robust, 'y': y_robust, 'true_coef': coef_robust,\n",
    "    'description': 'Clean data corrupted with 10% strong outliers',\n",
    "    'optimal_algorithm': 'Huber Regression',\n",
    "    'challenge': 'outliers'\n",
    "}\n",
    "\n",
    "print(f\"    Generated: {X_robust.shape}, Outlier fraction: 10%\")\n",
    "\n",
    "print(\"\\n‚ú® Regression dataset spectrum generation complete!\")\n",
    "\n",
    "# Save regression datasets metadata\n",
    "regression_metadata = {\n",
    "    'total_datasets': len(regression_datasets),\n",
    "    'dataset_details': {name: {\n",
    "        'shape': info['X'].shape,\n",
    "        'description': info['description'],\n",
    "        'optimal_algorithm': info.get('optimal_algorithm', 'Unknown'),\n",
    "        'challenge': info.get('challenge', 'general')\n",
    "    } for name, info in regression_datasets.items()}\n",
    "}\n",
    "\n",
    "save_experiment_results('regression_datasets_generation', regression_metadata,\n",
    "                       'Comprehensive regression dataset generation results', 'regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3c0a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze regression dataset characteristics\n",
    "print(\"üìä Analyzing Regression Dataset Characteristics...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (dataset_name, dataset_info) in enumerate(regression_datasets.items()):\n",
    "    if idx >= 4:  # Only plot first 4 datasets\n",
    "        break\n",
    "        \n",
    "    X, y = dataset_info['X'], dataset_info['y']\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # For multicollinearity dataset, show correlation heatmap\n",
    "    if dataset_name == 'Multicollinear':\n",
    "        corr_matrix = np.corrcoef(X.T)\n",
    "        # Show only a subset for clarity\n",
    "        subset_size = min(15, X.shape[1])\n",
    "        corr_subset = corr_matrix[:subset_size, :subset_size]\n",
    "        \n",
    "        im = ax.imshow(corr_subset, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "        ax.set_title(f'{dataset_name} Dataset\\nFeature Correlation Matrix')\n",
    "        plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "        \n",
    "    # For high-dimensional, show coefficient importance\n",
    "    elif dataset_name == 'High_Dimensional' and dataset_info['true_coef'] is not None:\n",
    "        coef = dataset_info['true_coef']\n",
    "        non_zero_idx = np.where(np.abs(coef) > 1e-6)[0]\n",
    "        ax.bar(range(len(non_zero_idx)), coef[non_zero_idx])\n",
    "        ax.set_title(f'{dataset_name} Dataset\\nTrue Non-Zero Coefficients')\n",
    "        ax.set_xlabel('Feature Index')\n",
    "        ax.set_ylabel('Coefficient Value')\n",
    "        \n",
    "    # For other datasets, show target distribution and feature relationship\n",
    "    else:\n",
    "        if X.shape[1] >= 2:\n",
    "            scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.6, s=20)\n",
    "            ax.set_xlabel('Feature 1')\n",
    "            ax.set_ylabel('Feature 2')\n",
    "            plt.colorbar(scatter, ax=ax, shrink=0.8)\n",
    "        else:\n",
    "            ax.hist(y, bins=30, alpha=0.7, edgecolor='black')\n",
    "            ax.set_xlabel('Target Value')\n",
    "            ax.set_ylabel('Frequency')\n",
    "        \n",
    "        ax.set_title(f'{dataset_name} Dataset\\n{dataset_info[\"description\"]}')\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save regression characteristics figure\n",
    "save_figure(fig, 'regression_dataset_characteristics',\n",
    "           'Analysis of different regression dataset types and their properties', 'regression')\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nüìà Dataset Summary Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Dataset':<18} {'Samples':<8} {'Features':<10} {'Target Range':<15} {'Description':<25}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, info in regression_datasets.items():\n",
    "    X, y = info['X'], info['y']\n",
    "    target_range = f\"{y.min():.2f} to {y.max():.2f}\"\n",
    "    description = info['description'][:24] + \"...\" if len(info['description']) > 24 else info['description']\n",
    "    \n",
    "    print(f\"{name:<18} {X.shape[0]:<8} {X.shape[1]:<10} {target_range:<15} {description:<25}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b032bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced coefficient analysis for regression datasets\n",
    "print(\"üîç Enhanced Coefficient Analysis...\")\n",
    "\n",
    "def analyze_coefficient_patterns(datasets, algorithm_results):\n",
    "    \"\"\"Analyze coefficient patterns across different regression algorithms.\"\"\"\n",
    "    coefficient_analysis = {}\n",
    "    \n",
    "    for dataset_name, dataset_info in datasets.items():\n",
    "        if 'true_coef' in dataset_info and dataset_info['true_coef'] is not None:\n",
    "            true_coef = dataset_info['true_coef']\n",
    "            \n",
    "            if dataset_name in algorithm_results:\n",
    "                dataset_coef_analysis = {\n",
    "                    'true_coefficients': {\n",
    "                        'non_zero_count': np.sum(np.abs(true_coef) > 1e-6),\n",
    "                        'sparsity': 1 - (np.sum(np.abs(true_coef) > 1e-6) / len(true_coef)),\n",
    "                        'magnitude_range': [float(np.min(true_coef)), float(np.max(true_coef))],\n",
    "                        'l1_norm': float(np.sum(np.abs(true_coef))),\n",
    "                        'l2_norm': float(np.sqrt(np.sum(true_coef ** 2)))\n",
    "                    },\n",
    "                    'algorithm_coefficients': {}\n",
    "                }\n",
    "                \n",
    "                # Analyze learned coefficients for each algorithm\n",
    "                for alg_name, metrics in algorithm_results[dataset_name].items():\n",
    "                    if 'model' in metrics and hasattr(metrics['model'], 'coef_'):\n",
    "                        learned_coef = metrics['model'].coef_\n",
    "                        \n",
    "                        # Calculate coefficient recovery metrics\n",
    "                        correlation = np.corrcoef(true_coef, learned_coef)[0, 1] if len(learned_coef) == len(true_coef) else 0\n",
    "                        mse = np.mean((true_coef - learned_coef) ** 2) if len(learned_coef) == len(true_coef) else float('inf')\n",
    "                        \n",
    "                        # Feature selection accuracy\n",
    "                        true_support = np.abs(true_coef) > 1e-6\n",
    "                        learned_support = np.abs(learned_coef) > 1e-6\n",
    "                        \n",
    "                        if len(true_support) == len(learned_support):\n",
    "                            precision = np.sum(true_support & learned_support) / np.sum(learned_support) if np.sum(learned_support) > 0 else 0\n",
    "                            recall = np.sum(true_support & learned_support) / np.sum(true_support) if np.sum(true_support) > 0 else 0\n",
    "                            f1_feature = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                        else:\n",
    "                            precision = recall = f1_feature = 0\n",
    "                        \n",
    "                        dataset_coef_analysis['algorithm_coefficients'][alg_name] = {\n",
    "                            'correlation_with_true': float(correlation),\n",
    "                            'mse_with_true': float(mse),\n",
    "                            'feature_selection_precision': float(precision),\n",
    "                            'feature_selection_recall': float(recall),\n",
    "                            'feature_selection_f1': float(f1_feature),\n",
    "                            'learned_sparsity': float(1 - (np.sum(np.abs(learned_coef) > 1e-6) / len(learned_coef))),\n",
    "                            'l1_norm': float(np.sum(np.abs(learned_coef))),\n",
    "                            'l2_norm': float(np.sqrt(np.sum(learned_coef ** 2)))\n",
    "                        }\n",
    "                \n",
    "                coefficient_analysis[dataset_name] = dataset_coef_analysis\n",
    "    \n",
    "    return coefficient_analysis\n",
    "\n",
    "# Perform coefficient analysis\n",
    "if 'regression_datasets' in globals() and 'algorithm_performance' in globals():\n",
    "    coef_analysis = analyze_coefficient_patterns(regression_datasets, algorithm_performance)\n",
    "    \n",
    "    # Display coefficient recovery results\n",
    "    print(\"\\nüìä Coefficient Recovery Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for dataset_name, analysis in coef_analysis.items():\n",
    "        print(f\"\\nüîç {dataset_name}:\")\n",
    "        print(f\"  True coefficients: {analysis['true_coefficients']['non_zero_count']} non-zero, \"\n",
    "              f\"sparsity: {analysis['true_coefficients']['sparsity']:.3f}\")\n",
    "        \n",
    "        print(\"  Algorithm Performance:\")\n",
    "        for alg_name, metrics in analysis['algorithm_coefficients'].items():\n",
    "            print(f\"    {alg_name:<20}: Corr={metrics['correlation_with_true']:.3f}, \"\n",
    "                  f\"F1={metrics['feature_selection_f1']:.3f}, \"\n",
    "                  f\"Sparsity={metrics['learned_sparsity']:.3f}\")\n",
    "    \n",
    "    # Save coefficient analysis\n",
    "    save_experiment_results('coefficient_recovery_analysis', coef_analysis,\n",
    "                           'Detailed coefficient recovery and feature selection analysis', 'regression')\n",
    "\n",
    "print(\"‚ú® Enhanced coefficient analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b7ea1",
   "metadata": {},
   "source": [
    "### Demonstrate Regression Algorithm Performance Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regression algorithms across dataset types\n",
    "print(\"üß™ Comprehensive Regression Algorithm Analysis...\")\n",
    "\n",
    "# Define regression algorithms with their optimal use cases\n",
    "regression_algorithms = {\n",
    "    'Linear Regression': {\n",
    "        'model': LinearRegression(),\n",
    "        'description': 'Baseline linear model',\n",
    "        'optimal_for': 'Low multicollinearity, sufficient samples'\n",
    "    },\n",
    "    'Ridge Regression': {\n",
    "        'model': Ridge(alpha=1.0),\n",
    "        'description': 'L2 regularization',\n",
    "        'optimal_for': 'Multicollinearity, stable coefficients'\n",
    "    },\n",
    "    'Lasso Regression': {\n",
    "        'model': Lasso(alpha=0.1, max_iter=2000),\n",
    "        'description': 'L1 regularization',\n",
    "        'optimal_for': 'Feature selection, sparse solutions'\n",
    "    },\n",
    "    'Elastic Net': {\n",
    "        'model': ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=2000),\n",
    "        'description': 'Combined L1 + L2',\n",
    "        'optimal_for': 'Balanced regularization'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Evaluate each algorithm on each dataset\n",
    "algorithm_performance = {}\n",
    "\n",
    "print(\"\\nüîç Evaluating Algorithm Performance Across Dataset Types...\")\n",
    "\n",
    "for dataset_name, dataset_info in regression_datasets.items():\n",
    "    print(f\"\\n--- Testing on {dataset_name} Dataset ---\")\n",
    "    \n",
    "    X, y = dataset_info['X'], dataset_info['y']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    dataset_results = {}\n",
    "    \n",
    "    for alg_name, alg_info in regression_algorithms.items():\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Train model\n",
    "            model = alg_info['model'].__class__(**alg_info['model'].get_params())\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate\n",
    "            train_score = model.score(X_train_scaled, y_train)\n",
    "            test_score = model.score(X_test_scaled, y_test)\n",
    "            \n",
    "            # Cross-validation score\n",
    "            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "            \n",
    "            # Count non-zero coefficients (for regularized models)\n",
    "            if hasattr(model, 'coef_'):\n",
    "                n_nonzero = np.sum(np.abs(model.coef_) > 1e-6)\n",
    "                coef_sparsity = 1 - (n_nonzero / len(model.coef_))\n",
    "            else:\n",
    "                n_nonzero = 0\n",
    "                coef_sparsity = 0\n",
    "            \n",
    "            dataset_results[alg_name] = {\n",
    "                'train_r2': train_score,\n",
    "                'test_r2': test_score,\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std(),\n",
    "                'training_time': training_time,\n",
    "                'n_nonzero_coef': n_nonzero,\n",
    "                'coef_sparsity': coef_sparsity,\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "            print(f\"  {alg_name:<20}: Test R¬≤={test_score:.4f}, CV R¬≤={cv_scores.mean():.4f}¬±{cv_scores.std():.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  {alg_name:<20}: ‚ùå Failed ({str(e)[:30]}...)\")\n",
    "            dataset_results[alg_name] = {'error': str(e)}\n",
    "    \n",
    "    algorithm_performance[dataset_name] = dataset_results\n",
    "\n",
    "print(\"\\n‚ú® Algorithm performance evaluation complete!\")\n",
    "\n",
    "# Save regression algorithm performance\n",
    "regression_performance_summary = {}\n",
    "for dataset_name, algorithms in algorithm_performance.items():\n",
    "    regression_performance_summary[dataset_name] = {}\n",
    "    for alg_name, metrics in algorithms.items():\n",
    "        if 'error' not in metrics:\n",
    "            regression_performance_summary[dataset_name][alg_name] = {\n",
    "                'test_r2': metrics['test_r2'],\n",
    "                'cv_mean': metrics['cv_mean'],\n",
    "                'training_time': metrics['training_time'],\n",
    "                'coef_sparsity': metrics['coef_sparsity']\n",
    "            }\n",
    "\n",
    "save_experiment_results('regression_algorithm_performance', regression_performance_summary,\n",
    "                       'Comprehensive regression algorithm performance analysis', 'regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac6d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comprehensive regression algorithm analysis\n",
    "print(\"üìä Creating Comprehensive Regression Analysis Visualization...\")\n",
    "\n",
    "# Prepare data for visualization\n",
    "performance_data = []\n",
    "sparsity_data = []\n",
    "time_data = []\n",
    "\n",
    "for dataset_name, algorithms in algorithm_performance.items():\n",
    "    for alg_name, metrics in algorithms.items():\n",
    "        if 'error' not in metrics:\n",
    "            performance_data.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Algorithm': alg_name,\n",
    "                'Test_R2': metrics['test_r2'],\n",
    "                'CV_Mean': metrics['cv_mean'],\n",
    "                'CV_Std': metrics['cv_std'],\n",
    "                'Generalization_Gap': metrics['train_r2'] - metrics['test_r2']\n",
    "            })\n",
    "            \n",
    "            sparsity_data.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Algorithm': alg_name,\n",
    "                'Coefficient_Sparsity': metrics['coef_sparsity'],\n",
    "                'N_Nonzero': metrics['n_nonzero_coef']\n",
    "            })\n",
    "            \n",
    "            time_data.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Algorithm': alg_name,\n",
    "                'Training_Time': metrics['training_time']\n",
    "            })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "sparsity_df = pd.DataFrame(sparsity_data)\n",
    "time_df = pd.DataFrame(time_data)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Test performance heatmap\n",
    "if not performance_df.empty:\n",
    "    performance_pivot = performance_df.pivot(index='Algorithm', columns='Dataset', values='Test_R2')\n",
    "    sns.heatmap(performance_pivot, annot=True, cmap='RdYlGn', center=0.5, \n",
    "                cbar_kws={'label': 'Test R¬≤ Score'}, ax=axes[0, 0], fmt='.3f')\n",
    "    axes[0, 0].set_title('Algorithm Performance Across Dataset Types')\n",
    "    axes[0, 0].set_ylabel('Regression Algorithm')\n",
    "\n",
    "# 2. Generalization analysis\n",
    "if not performance_df.empty:\n",
    "    gen_pivot = performance_df.pivot(index='Algorithm', columns='Dataset', values='Generalization_Gap')\n",
    "    sns.heatmap(gen_pivot, annot=True, cmap='RdBu_r', center=0, \n",
    "                cbar_kws={'label': 'Generalization Gap'}, ax=axes[0, 1], fmt='.3f')\n",
    "    axes[0, 1].set_title('Overfitting Analysis (Train R¬≤ - Test R¬≤)')\n",
    "    axes[0, 1].set_ylabel('Regression Algorithm')\n",
    "\n",
    "# 3. Feature selection effectiveness\n",
    "if not sparsity_df.empty:\n",
    "    sparsity_pivot = sparsity_df.pivot(index='Algorithm', columns='Dataset', values='Coefficient_Sparsity')\n",
    "    sns.heatmap(sparsity_pivot, annot=True, cmap='viridis', \n",
    "                cbar_kws={'label': 'Coefficient Sparsity'}, ax=axes[0, 2], fmt='.3f')\n",
    "    axes[0, 2].set_title('Feature Selection Effectiveness')\n",
    "    axes[0, 2].set_ylabel('Regression Algorithm')\n",
    "\n",
    "# 4. Performance distribution by algorithm\n",
    "if not performance_df.empty:\n",
    "    sns.boxplot(data=performance_df, x='Algorithm', y='Test_R2', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Performance Distribution by Algorithm')\n",
    "    axes[1, 0].set_ylabel('Test R¬≤ Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Training efficiency analysis\n",
    "if not time_df.empty:\n",
    "    time_pivot = time_df.pivot(index='Algorithm', columns='Dataset', values='Training_Time')\n",
    "    sns.heatmap(time_pivot, annot=True, cmap='YlOrRd', \n",
    "                cbar_kws={'label': 'Training Time (s)'}, ax=axes[1, 1], fmt='.4f')\n",
    "    axes[1, 1].set_title('Training Efficiency Comparison')\n",
    "    axes[1, 1].set_ylabel('Regression Algorithm')\n",
    "\n",
    "# 6. Performance vs complexity trade-off\n",
    "if not performance_df.empty and not sparsity_df.empty:\n",
    "    # Merge dataframes for scatter plot\n",
    "    merged_df = pd.merge(performance_df, sparsity_df, on=['Dataset', 'Algorithm'])\n",
    "    \n",
    "    for dataset in merged_df['Dataset'].unique():\n",
    "        dataset_data = merged_df[merged_df['Dataset'] == dataset]\n",
    "        axes[1, 2].scatter(dataset_data['Coefficient_Sparsity'], dataset_data['Test_R2'], \n",
    "                          label=dataset, s=80, alpha=0.7)\n",
    "    \n",
    "    axes[1, 2].set_xlabel('Coefficient Sparsity')\n",
    "    axes[1, 2].set_ylabel('Test R¬≤ Score')\n",
    "    axes[1, 2].set_title('Performance vs Model Complexity')\n",
    "    axes[1, 2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save comprehensive regression analysis\n",
    "save_figure(fig, 'comprehensive_regression_analysis',\n",
    "           'Complete performance comparison across all regression algorithms and datasets', 'regression')\n",
    "plt.show()\n",
    "\n",
    "# Create advanced coefficient recovery visualization\n",
    "if 'coef_analysis' in locals():\n",
    "    print(\"üìä Creating Advanced Coefficient Recovery Visualization...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Coefficient correlation heatmap\n",
    "    correlation_data = []\n",
    "    datasets_with_coef = []\n",
    "    algorithms_with_coef = set()\n",
    "    \n",
    "    for dataset_name, analysis in coef_analysis.items():\n",
    "        datasets_with_coef.append(dataset_name)\n",
    "        for alg_name, metrics in analysis['algorithm_coefficients'].items():\n",
    "            algorithms_with_coef.add(alg_name)\n",
    "            correlation_data.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Algorithm': alg_name,\n",
    "                'Correlation': metrics['correlation_with_true']\n",
    "            })\n",
    "    \n",
    "    if correlation_data:\n",
    "        corr_df = pd.DataFrame(correlation_data)\n",
    "        corr_pivot = corr_df.pivot(index='Algorithm', columns='Dataset', values='Correlation')\n",
    "        \n",
    "        sns.heatmap(corr_pivot, annot=True, cmap='RdYlGn', center=0.5, \n",
    "                    cbar_kws={'label': 'Correlation with True Coefficients'}, \n",
    "                    ax=axes[0, 0], fmt='.3f')\n",
    "        axes[0, 0].set_title('Coefficient Recovery: Correlation with True Values')\n",
    "    \n",
    "    # 2. Feature selection performance\n",
    "    fs_data = []\n",
    "    for dataset_name, analysis in coef_analysis.items():\n",
    "        for alg_name, metrics in analysis['algorithm_coefficients'].items():\n",
    "            fs_data.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Algorithm': alg_name,\n",
    "                'F1_Score': metrics['feature_selection_f1']\n",
    "            })\n",
    "    \n",
    "    if fs_data:\n",
    "        fs_df = pd.DataFrame(fs_data)\n",
    "        fs_pivot = fs_df.pivot(index='Algorithm', columns='Dataset', values='F1_Score')\n",
    "        \n",
    "        sns.heatmap(fs_pivot, annot=True, cmap='Blues', \n",
    "                    cbar_kws={'label': 'Feature Selection F1 Score'}, \n",
    "                    ax=axes[0, 1], fmt='.3f')\n",
    "        axes[0, 1].set_title('Feature Selection Performance')\n",
    "    \n",
    "    # 3. Sparsity comparison\n",
    "    sparsity_comparison = []\n",
    "    for dataset_name, analysis in coef_analysis.items():\n",
    "        true_sparsity = analysis['true_coefficients']['sparsity']\n",
    "        for alg_name, metrics in analysis['algorithm_coefficients'].items():\n",
    "            learned_sparsity = metrics['learned_sparsity']\n",
    "            sparsity_comparison.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Algorithm': alg_name,\n",
    "                'True_Sparsity': true_sparsity,\n",
    "                'Learned_Sparsity': learned_sparsity,\n",
    "                'Sparsity_Error': abs(true_sparsity - learned_sparsity)\n",
    "            })\n",
    "    \n",
    "    if sparsity_comparison:\n",
    "        sparsity_df = pd.DataFrame(sparsity_comparison)\n",
    "        \n",
    "        # Scatter plot of true vs learned sparsity\n",
    "        for alg in sparsity_df['Algorithm'].unique():\n",
    "            alg_data = sparsity_df[sparsity_df['Algorithm'] == alg]\n",
    "            axes[1, 0].scatter(alg_data['True_Sparsity'], alg_data['Learned_Sparsity'], \n",
    "                              label=alg, alpha=0.7, s=60)\n",
    "        \n",
    "        # Perfect sparsity recovery line\n",
    "        axes[1, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Recovery')\n",
    "        axes[1, 0].set_xlabel('True Sparsity')\n",
    "        axes[1, 0].set_ylabel('Learned Sparsity')\n",
    "        axes[1, 0].set_title('Sparsity Recovery Comparison')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Algorithm ranking by coefficient recovery\n",
    "    if correlation_data:\n",
    "        avg_correlation = corr_df.groupby('Algorithm')['Correlation'].mean().sort_values(ascending=True)\n",
    "        \n",
    "        bars = axes[1, 1].barh(range(len(avg_correlation)), avg_correlation.values, \n",
    "                              color='lightblue', alpha=0.7)\n",
    "        axes[1, 1].set_yticks(range(len(avg_correlation)))\n",
    "        axes[1, 1].set_yticklabels(avg_correlation.index)\n",
    "        axes[1, 1].set_xlabel('Average Correlation with True Coefficients')\n",
    "        axes[1, 1].set_title('Algorithm Ranking: Coefficient Recovery')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, avg_correlation.values):\n",
    "            axes[1, 1].text(value + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                           f'{value:.3f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save advanced coefficient analysis visualization\n",
    "    save_figure(fig, 'advanced_coefficient_recovery_analysis',\n",
    "               'Advanced visualization of coefficient recovery and feature selection performance', 'regression')\n",
    "    plt.show()\n",
    "\n",
    "# Algorithm recommendation analysis\n",
    "print(\"\\nüéØ Algorithm Recommendation Analysis:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for dataset_name, algorithms in algorithm_performance.items():\n",
    "    if any('error' not in alg for alg in algorithms.values()):\n",
    "        # Find best performing algorithm for this dataset\n",
    "        valid_algs = {name: metrics for name, metrics in algorithms.items() if 'error' not in metrics}\n",
    "        best_alg = max(valid_algs.keys(), key=lambda x: valid_algs[x]['test_r2'])\n",
    "        best_score = valid_algs[best_alg]['test_r2']\n",
    "        \n",
    "        print(f\"\\nüìä {dataset_name} Dataset:\")\n",
    "        print(f\"  Best Algorithm: {best_alg} (R¬≤ = {best_score:.4f})\")\n",
    "        print(f\"  Optimal for: {regression_algorithms[best_alg]['optimal_for']}\")\n",
    "        \n",
    "        # Show all algorithm rankings\n",
    "        sorted_algs = sorted(valid_algs.items(), key=lambda x: x[1]['test_r2'], reverse=True)\n",
    "        print(f\"  Algorithm Rankings:\")\n",
    "        for i, (alg_name, metrics) in enumerate(sorted_algs):\n",
    "            print(f\"    {i+1}. {alg_name}: R¬≤ = {metrics['test_r2']:.4f}\")\n",
    "\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d81b711",
   "metadata": {},
   "source": [
    "## 4. Classification Complexity Hierarchy {#classification}\n",
    "\n",
    "Now let's create a sophisticated hierarchy of classification datasets that progressively increase in complexity to demonstrate algorithm capabilities and limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive classification dataset hierarchy\n",
    "print(\"üéØ Generating Classification Complexity Hierarchy...\")\n",
    "\n",
    "classification_datasets = {}\n",
    "\n",
    "# Level 1: Linear separability spectrum\n",
    "print(\"  Level 1: Linear Separability Analysis...\")\n",
    "\n",
    "linear_configs = [\n",
    "    {'separation': 2.0, 'name': 'Easy_Linear', 'description': 'Perfectly linearly separable'},\n",
    "    {'separation': 1.0, 'name': 'Medium_Linear', 'description': 'Moderately separable'},\n",
    "    {'separation': 0.3, 'name': 'Hard_Linear', 'description': 'Barely linearly separable'}\n",
    "]\n",
    "\n",
    "for config in linear_configs:\n",
    "    X, y = generator.classification_dataset(\n",
    "        n_samples=1000,\n",
    "        n_features=20,\n",
    "        n_informative=15,\n",
    "        n_redundant=3,\n",
    "        n_classes=3,\n",
    "        class_sep=config['separation'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    classification_datasets[config['name']] = {\n",
    "        'X': X, 'y': y, 'level': 1,\n",
    "        'description': config['description'],\n",
    "        'complexity_factors': {'linearity': 'linear', 'separability': config['separation']},\n",
    "        'optimal_algorithm': 'Logistic Regression' if config['separation'] > 1.0 else 'SVM'\n",
    "    }\n",
    "    \n",
    "    print(f\"    {config['name']}: {X.shape}, Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Level 2: Geometric complexity\n",
    "print(\"  Level 2: Geometric Pattern Complexity...\")\n",
    "\n",
    "geometric_patterns = [\n",
    "    {'type': 'moons', 'name': 'Moons_Pattern'},\n",
    "    {'type': 'circles', 'name': 'Circles_Pattern'},\n",
    "    {'type': 'spirals', 'name': 'Spirals_Pattern'}\n",
    "]\n",
    "\n",
    "for pattern in geometric_patterns:\n",
    "    if pattern['type'] == 'moons':\n",
    "        X, y = generator.make_moons_advanced(\n",
    "            n_samples=800, noise=0.15, n_clusters=2\n",
    "        )\n",
    "    elif pattern['type'] == 'circles':\n",
    "        X, y = generator.make_circles_advanced(\n",
    "            n_samples=800, noise=0.1, factor=0.3\n",
    "        )\n",
    "    else:  # spirals\n",
    "        X, y = generator.make_spirals(\n",
    "            n_samples=600, noise=0.1, n_spirals=2\n",
    "        )\n",
    "    \n",
    "    classification_datasets[pattern['name']] = {\n",
    "        'X': X, 'y': y, 'level': 2,\n",
    "        'description': f'Nonlinear {pattern[\"type\"]} pattern',\n",
    "        'complexity_factors': {'linearity': 'nonlinear', 'pattern': pattern['type']},\n",
    "        'optimal_algorithm': 'Random Forest' if pattern['type'] != 'circles' else 'SVM (RBF)'\n",
    "    }\n",
    "    \n",
    "    print(f\"    {pattern['name']}: {X.shape}, Pattern: {pattern['type']}\")\n",
    "\n",
    "# Level 3: High-dimensional challenges\n",
    "print(\"  Level 3: High-Dimensional Challenges...\")\n",
    "\n",
    "high_dim_configs = [\n",
    "    {\n",
    "        'n_samples': 500, 'n_features': 100, 'n_informative': 20,\n",
    "        'name': 'HighDim_Sparse', 'description': 'High-dimensional sparse features'\n",
    "    },\n",
    "    {\n",
    "        'n_samples': 300, 'n_features': 200, 'n_informative': 10,\n",
    "        'name': 'HighDim_Very_Sparse', 'description': 'Very high-dimensional, very sparse'\n",
    "    }\n",
    "]\n",
    "\n",
    "for config in high_dim_configs:\n",
    "    X, y = generator.classification_dataset(\n",
    "        n_samples=config['n_samples'],\n",
    "        n_features=config['n_features'],\n",
    "        n_informative=config['n_informative'],\n",
    "        n_redundant=5,\n",
    "        n_classes=2,\n",
    "        class_sep=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    classification_datasets[config['name']] = {\n",
    "        'X': X, 'y': y, 'level': 3,\n",
    "        'description': config['description'],\n",
    "        'complexity_factors': {\n",
    "            'dimensionality': 'high', \n",
    "            'sparsity': config['n_informative'] / config['n_features']\n",
    "        },\n",
    "        'optimal_algorithm': 'Linear SVM'\n",
    "    }\n",
    "    \n",
    "    print(f\"    {config['name']}: {X.shape}, Informative ratio: {config['n_informative']/config['n_features']:.2f}\")\n",
    "\n",
    "# Level 4: Real-world challenges\n",
    "print(\"  Level 4: Real-World Challenge Scenarios...\")\n",
    "\n",
    "# Imbalanced classification\n",
    "X_imbal, y_imbal = generator.imbalanced_classification(\n",
    "    n_samples=1200, n_features=15, imbalance_ratio=0.05\n",
    ")\n",
    "\n",
    "classification_datasets['Imbalanced_Extreme'] = {\n",
    "    'X': X_imbal, 'y': y_imbal, 'level': 4,\n",
    "    'description': 'Extremely imbalanced classes (5% minority)',\n",
    "    'complexity_factors': {'imbalance_ratio': 0.05, 'challenge': 'class_imbalance'},\n",
    "    'optimal_algorithm': 'Random Forest with balancing'\n",
    "}\n",
    "\n",
    "# Noisy features\n",
    "X_noisy, y_noisy = generator.classification_with_noise(\n",
    "    n_samples=800, n_features=25, n_informative=10, noise_features=10\n",
    ")\n",
    "\n",
    "classification_datasets['Noisy_Features'] = {\n",
    "    'X': X_noisy, 'y': y_noisy, 'level': 4,\n",
    "    'description': 'Many irrelevant noisy features',\n",
    "    'complexity_factors': {'noise_ratio': 0.4, 'challenge': 'feature_noise'},\n",
    "    'optimal_algorithm': 'Gradient Boosting'\n",
    "}\n",
    "\n",
    "print(f\"    Imbalanced_Extreme: {X_imbal.shape}, Class ratio: {np.bincount(y_imbal)}\")\n",
    "print(f\"    Noisy_Features: {X_noisy.shape}, Noise ratio: 40%\")\n",
    "\n",
    "print(\"\\n‚ú® Classification complexity hierarchy generation complete!\")\n",
    "\n",
    "# Save classification datasets metadata\n",
    "classification_metadata = {\n",
    "    'total_datasets': len(classification_datasets),\n",
    "    'complexity_levels': len(set(info['level'] for info in classification_datasets.values())),\n",
    "    'dataset_details': {name: {\n",
    "        'shape': info['X'].shape,\n",
    "        'level': info['level'],\n",
    "        'description': info['description'],\n",
    "        'optimal_algorithm': info.get('optimal_algorithm', 'Unknown'),\n",
    "        'complexity_factors': info.get('complexity_factors', {})\n",
    "    } for name, info in classification_datasets.items()}\n",
    "}\n",
    "\n",
    "save_experiment_results('classification_datasets_generation', classification_metadata,\n",
    "                       'Comprehensive classification complexity hierarchy generation', 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification complexity hierarchy\n",
    "print(\"üìä Visualizing Classification Complexity Hierarchy...\")\n",
    "\n",
    "# Create comprehensive visualization of all classification datasets\n",
    "n_datasets = len(classification_datasets)\n",
    "n_cols = 4\n",
    "n_rows = (n_datasets + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, (dataset_name, dataset_info) in enumerate(classification_datasets.items()):\n",
    "    if idx >= len(axes_flat):\n",
    "        break\n",
    "        \n",
    "    ax = axes_flat[idx]\n",
    "    X, y = dataset_info['X'], dataset_info['y']\n",
    "    \n",
    "    # For 2D datasets, create scatter plots\n",
    "    if X.shape[1] >= 2:\n",
    "        # Use first two features for visualization\n",
    "        scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='tab10', alpha=0.7, s=30)\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        \n",
    "        # Add class boundaries for linear separability visualization\n",
    "        if 'Linear' in dataset_name and X.shape[1] >= 2:\n",
    "            # Simple linear decision boundary estimation\n",
    "            from sklearn.svm import SVC\n",
    "            svm_viz = SVC(kernel='linear', random_state=42)\n",
    "            svm_viz.fit(X[:, :2], y)\n",
    "            \n",
    "            # Create mesh for decision boundary\n",
    "            x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "            y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "            xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50),\n",
    "                               np.linspace(y_min, y_max, 50))\n",
    "            \n",
    "            Z = svm_viz.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            ax.contour(xx, yy, Z, alpha=0.3, levels=np.unique(y), colors='black', linestyles='--')\n",
    "    \n",
    "    else:\n",
    "        # For high-dimensional datasets, show class distribution\n",
    "        unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "        bars = ax.bar(unique_classes, class_counts, alpha=0.7)\n",
    "        ax.set_xlabel('Class Label')\n",
    "        ax.set_ylabel('Sample Count')\n",
    "        \n",
    "        # Add percentage labels\n",
    "        total_samples = len(y)\n",
    "        for bar, count in zip(bars, class_counts):\n",
    "            percentage = (count / total_samples) * 100\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + total_samples * 0.01,\n",
    "                   f'{percentage:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Title with complexity level and description\n",
    "    level = dataset_info['level']\n",
    "    description = dataset_info['description']\n",
    "    optimal_alg = dataset_info.get('optimal_algorithm', 'Unknown')\n",
    "    ax.set_title(f'Level {level}: {dataset_name}\\n{description}\\n'\n",
    "                f'({X.shape[0]} samples, {X.shape[1]} features)\\nOptimal: {optimal_alg}', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(len(classification_datasets), len(axes_flat)):\n",
    "    axes_flat[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save classification hierarchy visualization\n",
    "save_figure(fig, 'classification_complexity_hierarchy',\n",
    "           'Visualization of classification datasets across complexity levels', 'classification')\n",
    "plt.show()\n",
    "\n",
    "# Dataset complexity analysis\n",
    "print(\"\\nüìà Classification Dataset Complexity Analysis:\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Dataset':<20} {'Level':<6} {'Samples':<8} {'Features':<10} {'Classes':<8} {'Complexity Factors':<30}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for name, info in classification_datasets.items():\n",
    "    X, y = info['X'], info['y']\n",
    "    level = info['level']\n",
    "    n_classes = len(np.unique(y))\n",
    "    \n",
    "    # Format complexity factors\n",
    "    factors = info.get('complexity_factors', {})\n",
    "    factor_str = ', '.join([f\"{k}:{v}\" for k, v in factors.items()])\n",
    "    factor_str = factor_str[:29] + \"...\" if len(factor_str) > 29 else factor_str\n",
    "    \n",
    "    print(f\"{name:<20} {level:<6} {X.shape[0]:<8} {X.shape[1]:<10} {n_classes:<8} {factor_str:<30}\")\n",
    "\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f140b4b",
   "metadata": {},
   "source": [
    "### Comprehensive Classification Algorithm Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e7880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive classification algorithm benchmarking\n",
    "print(\"üèÜ Comprehensive Classification Algorithm Benchmark...\")\n",
    "\n",
    "# Define comprehensive algorithm suite\n",
    "classification_algorithms = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'strengths': ['Linear boundaries', 'Probabilistic output', 'Fast training'],\n",
    "        'weaknesses': ['Linear only', 'Sensitive to outliers']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'strengths': ['Handles nonlinearity', 'Feature importance', 'Robust to outliers'],\n",
    "        'weaknesses': ['Can overfit', 'Black box', 'Memory intensive']\n",
    "    },\n",
    "    'SVM (RBF)': {\n",
    "        'model': SVC(kernel='rbf', random_state=42, probability=True),\n",
    "        'strengths': ['Nonlinear boundaries', 'Effective in high dimensions', 'Memory efficient'],\n",
    "        'weaknesses': ['Slow on large datasets', 'Sensitive to scaling', 'Parameter sensitive']\n",
    "    },\n",
    "    'SVM (Linear)': {\n",
    "        'model': SVC(kernel='linear', random_state=42, probability=True),\n",
    "        'strengths': ['Linear boundaries', 'Good with high dimensions', 'Regularization'],\n",
    "        'weaknesses': ['Linear only', 'Sensitive to scaling']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "        'strengths': ['Excellent performance', 'Handles mixed data', 'Feature importance'],\n",
    "        'weaknesses': ['Slow training', 'Prone to overfitting', 'Many hyperparameters']\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'model': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=500),\n",
    "        'strengths': ['Universal approximator', 'Learns complex patterns', 'Flexible architecture'],\n",
    "        'weaknesses': ['Requires large data', 'Black box', 'Sensitive to scaling']\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'model': GaussianNB(),\n",
    "        'strengths': ['Fast training/prediction', 'Works with small data', 'Probabilistic'],\n",
    "        'weaknesses': ['Strong independence assumption', 'Poor with correlated features']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Comprehensive benchmarking across all datasets\n",
    "algorithm_benchmark_results = {}\n",
    "\n",
    "print(\"\\nüî¨ Running Comprehensive Algorithm Benchmark...\")\n",
    "\n",
    "for dataset_name, dataset_info in classification_datasets.items():\n",
    "    print(f\"\\n--- Benchmarking on {dataset_name} (Level {dataset_info['level']}) ---\")\n",
    "    \n",
    "    X, y = dataset_info['X'], dataset_info['y']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    dataset_results = {}\n",
    "    \n",
    "    for alg_name, alg_info in classification_algorithms.items():\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Train model\n",
    "            model = alg_info['model'].__class__(**alg_info['model'].get_params())\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate\n",
    "            start_time = time.time()\n",
    "            predictions = model.predict(X_test_scaled)\n",
    "            prediction_time = time.time() - start_time\n",
    "            \n",
    "            accuracy = accuracy_score(y_test, predictions)\n",
    "            \n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "            \n",
    "            # Generalization gap\n",
    "            train_accuracy = model.score(X_train_scaled, y_train)\n",
    "            generalization_gap = train_accuracy - accuracy\n",
    "            \n",
    "            dataset_results[alg_name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std(),\n",
    "                'training_time': training_time,\n",
    "                'prediction_time': prediction_time,\n",
    "                'total_time': training_time + prediction_time,\n",
    "                'generalization_gap': generalization_gap,\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "            print(f\"  {alg_name:<20}: Acc={accuracy:.4f}, CV={cv_scores.mean():.4f}¬±{cv_scores.std():.4f}, Time={training_time:.3f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  {alg_name:<20}: ‚ùå Failed ({str(e)[:30]}...)\")\n",
    "            dataset_results[alg_name] = {'error': str(e)}\n",
    "    \n",
    "    algorithm_benchmark_results[dataset_name] = dataset_results\n",
    "\n",
    "print(\"\\n‚ú® Algorithm benchmarking complete!\")\n",
    "\n",
    "# Save benchmark results\n",
    "benchmark_summary = {}\n",
    "for dataset_name, algorithms in algorithm_benchmark_results.items():\n",
    "    benchmark_summary[dataset_name] = {}\n",
    "    for alg_name, metrics in algorithms.items():\n",
    "        if 'error' not in metrics:\n",
    "            benchmark_summary[dataset_name][alg_name] = {\n",
    "                'accuracy': metrics['accuracy'],\n",
    "                'cv_mean': metrics['cv_mean'],\n",
    "                'training_time': metrics['training_time'],\n",
    "                'generalization_gap': metrics['generalization_gap']\n",
    "            }\n",
    "\n",
    "save_experiment_results('classification_algorithm_benchmark', benchmark_summary,\n",
    "                       'Comprehensive classification algorithm benchmark results', 'classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c786bf1f",
   "metadata": {},
   "source": [
    "## 5. Clustering Pattern Showcase {#clustering}\n",
    "\n",
    "Generate diverse clustering datasets to demonstrate different clustering algorithm capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886f17ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clustering datasets\n",
    "print(\"üîó Generating Clustering Pattern Showcase...\")\n",
    "\n",
    "clustering_datasets = {}\n",
    "\n",
    "# Dataset 1: Well-separated spherical clusters\n",
    "print(\"  Creating well-separated spherical clusters...\")\n",
    "X_spherical, y_spherical = generator.make_blobs_advanced(\n",
    "    n_samples=800,\n",
    "    centers=4,\n",
    "    cluster_std=1.0,\n",
    "    n_features=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clustering_datasets['Spherical_Clusters'] = {\n",
    "    'X': X_spherical, 'y': y_spherical,\n",
    "    'description': 'Well-separated spherical clusters',\n",
    "    'optimal_algorithm': 'K-Means',\n",
    "    'challenge': 'none',\n",
    "    'true_k': 4\n",
    "}\n",
    "\n",
    "# Dataset 2: Non-spherical clusters\n",
    "print(\"  Creating non-spherical clusters...\")\n",
    "X_moons, y_moons = generator.make_moons_advanced(\n",
    "    n_samples=600, noise=0.1, n_clusters=2\n",
    ")\n",
    "\n",
    "clustering_datasets['Non_Spherical'] = {\n",
    "    'X': X_moons, 'y': y_moons,\n",
    "    'description': 'Non-spherical moon-shaped clusters',\n",
    "    'optimal_algorithm': 'DBSCAN',\n",
    "    'challenge': 'non_spherical',\n",
    "    'true_k': 2\n",
    "}\n",
    "\n",
    "# Dataset 3: Density-based clusters\n",
    "print(\"  Creating density-based clusters...\")\n",
    "X_density, y_density = generator.make_density_clusters(\n",
    "    n_samples=700,\n",
    "    n_centers=3,\n",
    "    cluster_density_ratio=0.3,\n",
    "    noise_ratio=0.1\n",
    ")\n",
    "\n",
    "clustering_datasets['Density_Based'] = {\n",
    "    'X': X_density, 'y': y_density,\n",
    "    'description': 'Varying density clusters with noise',\n",
    "    'optimal_algorithm': 'DBSCAN',\n",
    "    'challenge': 'varying_density',\n",
    "    'true_k': 3\n",
    "}\n",
    "\n",
    "# Dataset 4: Hierarchical structure\n",
    "print(\"  Creating hierarchical clusters...\")\n",
    "X_hierarchical, y_hierarchical = generator.make_hierarchical_clusters(\n",
    "    n_samples=500,\n",
    "    n_levels=3,\n",
    "    branching_factor=2\n",
    ")\n",
    "\n",
    "clustering_datasets['Hierarchical'] = {\n",
    "    'X': X_hierarchical, 'y': y_hierarchical,\n",
    "    'description': 'Hierarchical nested clusters',\n",
    "    'optimal_algorithm': 'Agglomerative Clustering',\n",
    "    'challenge': 'hierarchical',\n",
    "    'true_k': 8  # 2^3 leaf clusters\n",
    "}\n",
    "\n",
    "# Dataset 5: High-dimensional clustering\n",
    "print(\"  Creating high-dimensional clusters...\")\n",
    "X_high_dim, y_high_dim = generator.make_blobs_advanced(\n",
    "    n_samples=600,\n",
    "    centers=5,\n",
    "    cluster_std=2.0,\n",
    "    n_features=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clustering_datasets['High_Dimensional'] = {\n",
    "    'X': X_high_dim, 'y': y_high_dim,\n",
    "    'description': 'High-dimensional spherical clusters',\n",
    "    'optimal_algorithm': 'Gaussian Mixture',\n",
    "    'challenge': 'high_dimensionality',\n",
    "    'true_k': 5\n",
    "}\n",
    "\n",
    "print(\"\\n‚ú® Clustering dataset generation complete!\")\n",
    "\n",
    "# Save clustering datasets metadata\n",
    "clustering_metadata = {\n",
    "    'total_datasets': len(clustering_datasets),\n",
    "    'dataset_details': {name: {\n",
    "        'shape': info['X'].shape,\n",
    "        'description': info['description'],\n",
    "        'optimal_algorithm': info.get('optimal_algorithm', 'Unknown'),\n",
    "        'challenge': info.get('challenge', 'general'),\n",
    "        'true_k': info.get('true_k', 'Unknown')\n",
    "    } for name, info in clustering_datasets.items()}\n",
    "}\n",
    "\n",
    "save_experiment_results('clustering_datasets_generation', clustering_metadata,\n",
    "                       'Comprehensive clustering dataset generation results', 'clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a95589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering datasets\n",
    "print(\"üìä Visualizing Clustering Datasets...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (dataset_name, dataset_info) in enumerate(clustering_datasets.items()):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "        \n",
    "    ax = axes[idx]\n",
    "    X, y = dataset_info['X'], dataset_info['y']\n",
    "    \n",
    "    # For 2D datasets, create scatter plots\n",
    "    if X.shape[1] >= 2:\n",
    "        scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='tab10', alpha=0.7, s=30)\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "    else:\n",
    "        # For 1D data, create histogram\n",
    "        for cluster_id in np.unique(y):\n",
    "            cluster_data = X[y == cluster_id]\n",
    "            ax.hist(cluster_data, alpha=0.6, label=f'Cluster {cluster_id}', bins=20)\n",
    "        ax.set_xlabel('Feature Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.legend()\n",
    "    \n",
    "    # Title with information\n",
    "    description = dataset_info['description']\n",
    "    optimal_alg = dataset_info.get('optimal_algorithm', 'Unknown')\n",
    "    true_k = dataset_info.get('true_k', 'Unknown')\n",
    "    \n",
    "    ax.set_title(f'{dataset_name}\\n{description}\\n'\n",
    "                f'True K: {true_k}, Optimal: {optimal_alg}', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide empty subplot\n",
    "if len(clustering_datasets) < len(axes):\n",
    "    axes[-1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save clustering visualization\n",
    "save_figure(fig, 'clustering_datasets_showcase',\n",
    "           'Visualization of different clustering patterns and challenges', 'clustering')\n",
    "plt.show()\n",
    "\n",
    "# Clustering dataset summary\n",
    "print(\"\\nüìä Clustering Dataset Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Dataset':<20} {'Samples':<8} {'Features':<10} {'True K':<8} {'Challenge':<20}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, info in clustering_datasets.items():\n",
    "    X, y = info['X'], info['y']\n",
    "    true_k = info.get('true_k', 'Unknown')\n",
    "    challenge = info.get('challenge', 'general')\n",
    "    \n",
    "    print(f\"{name:<20} {X.shape[0]:<8} {X.shape[1]:<10} {true_k:<8} {challenge:<20}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# Comprehensive clustering algorithm performance visualization\n",
    "if 'clustering_algorithm_results' in globals():\n",
    "    print(\"üìä Creating Comprehensive Clustering Performance Visualization...\")\n",
    "    \n",
    "    # Prepare clustering performance data\n",
    "    clustering_perf_data = []\n",
    "    for dataset_name, algorithms in clustering_algorithm_results.items():\n",
    "        for alg_name, metrics in algorithms.items():\n",
    "            if 'error' not in metrics:\n",
    "                clustering_perf_data.append({\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Algorithm': alg_name,\n",
    "                    'Silhouette_Score': metrics['silhouette_score'],\n",
    "                    'ARI_Score': metrics['ari_score'],\n",
    "                    'N_Clusters_Found': metrics['n_clusters_found'],\n",
    "                    'Clustering_Time': metrics['clustering_time'],\n",
    "                    'True_K': clustering_datasets[dataset_name].get('true_k', 0)\n",
    "                })\n",
    "    \n",
    "    if clustering_perf_data:\n",
    "        clustering_df = pd.DataFrame(clustering_perf_data)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Silhouette score heatmap\n",
    "        sil_pivot = clustering_df.pivot(index='Algorithm', columns='Dataset', values='Silhouette_Score')\n",
    "        sns.heatmap(sil_pivot, annot=True, cmap='RdYlGn', center=0.3, \n",
    "                    cbar_kws={'label': 'Silhouette Score'}, ax=axes[0, 0], fmt='.3f')\n",
    "        axes[0, 0].set_title('Clustering Quality: Silhouette Scores')\n",
    "        \n",
    "        # 2. ARI score heatmap\n",
    "        ari_pivot = clustering_df.pivot(index='Algorithm', columns='Dataset', values='ARI_Score')\n",
    "        sns.heatmap(ari_pivot, annot=True, cmap='viridis', \n",
    "                    cbar_kws={'label': 'Adjusted Rand Index'}, ax=axes[0, 1], fmt='.3f')\n",
    "        axes[0, 1].set_title('Cluster Assignment Accuracy: ARI Scores')\n",
    "        \n",
    "        # 3. Clustering efficiency\n",
    "        time_pivot = clustering_df.pivot(index='Algorithm', columns='Dataset', values='Clustering_Time')\n",
    "        sns.heatmap(time_pivot, annot=True, cmap='YlOrRd', \n",
    "                    cbar_kws={'label': 'Clustering Time (s)'}, ax=axes[1, 0], fmt='.4f')\n",
    "        axes[1, 0].set_title('Clustering Efficiency')\n",
    "        \n",
    "        # 4. Number of clusters found vs true K\n",
    "        for dataset in clustering_df['Dataset'].unique():\n",
    "            dataset_data = clustering_df[clustering_df['Dataset'] == dataset]\n",
    "            true_k = dataset_data['True_K'].iloc[0]\n",
    "            \n",
    "            bars = axes[1, 1].bar(\n",
    "                [f\"{alg}\\n{dataset[:8]}...\" for alg in dataset_data['Algorithm']], \n",
    "                dataset_data['N_Clusters_Found'], \n",
    "                alpha=0.7, label=dataset if len(clustering_df['Dataset'].unique()) <= 4 else None\n",
    "            )\n",
    "            \n",
    "            # Add true K line\n",
    "            if true_k > 0:\n",
    "                axes[1, 1].axhline(y=true_k, color='red', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        axes[1, 1].set_ylabel('Number of Clusters Found')\n",
    "        axes[1, 1].set_title('Cluster Count Accuracy')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        if len(clustering_df['Dataset'].unique()) <= 4:\n",
    "            axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save clustering performance visualization\n",
    "        save_figure(fig, 'comprehensive_clustering_performance',\n",
    "                   'Complete clustering algorithm performance analysis across all datasets', 'clustering')\n",
    "        plt.show()\n",
    "        \n",
    "        # Clustering algorithm recommendations\n",
    "        print(\"\\nüéØ Clustering Algorithm Recommendations:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for dataset_name in clustering_df['Dataset'].unique():\n",
    "            dataset_results = clustering_df[clustering_df['Dataset'] == dataset_name]\n",
    "            \n",
    "            # Find best by silhouette score\n",
    "            best_sil = dataset_results.loc[dataset_results['Silhouette_Score'].idxmax()]\n",
    "            best_ari = dataset_results.loc[dataset_results['ARI_Score'].idxmax()]\n",
    "            \n",
    "            print(f\"\\nüìä {dataset_name}:\")\n",
    "            print(f\"  Best Silhouette: {best_sil['Algorithm']} ({best_sil['Silhouette_Score']:.4f})\")\n",
    "            print(f\"  Best ARI: {best_ari['Algorithm']} ({best_ari['ARI_Score']:.4f})\")\n",
    "            print(f\"  Predicted Optimal: {clustering_datasets[dataset_name].get('optimal_algorithm', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87929d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test clustering algorithms\n",
    "print(\"üß™ Testing Clustering Algorithms...\")\n",
    "\n",
    "# Define clustering algorithms\n",
    "clustering_algorithms = {\n",
    "    'K-Means': {\n",
    "        'class': KMeans,\n",
    "        'params': {'random_state': 42},\n",
    "        'strengths': ['Fast', 'Scales well', 'Simple'],\n",
    "        'weaknesses': ['Assumes spherical clusters', 'Requires K']\n",
    "    },\n",
    "    'DBSCAN': {\n",
    "        'class': DBSCAN,\n",
    "        'params': {'eps': 0.5, 'min_samples': 5},\n",
    "        'strengths': ['Finds arbitrary shapes', 'Handles noise', 'No K required'],\n",
    "        'weaknesses': ['Sensitive to parameters', 'Struggles with varying density']\n",
    "    },\n",
    "    'Agglomerative': {\n",
    "        'class': AgglomerativeClustering,\n",
    "        'params': {'linkage': 'ward'},\n",
    "        'strengths': ['Hierarchical structure', 'No assumptions about shape', 'Deterministic'],\n",
    "        'weaknesses': ['Computationally expensive', 'Requires K', 'Sensitive to outliers']\n",
    "    },\n",
    "    'Gaussian Mixture': {\n",
    "        'class': GaussianMixture,\n",
    "        'params': {'random_state': 42},\n",
    "        'strengths': ['Probabilistic', 'Handles overlapping clusters', 'Flexible shapes'],\n",
    "        'weaknesses': ['Assumes Gaussian distributions', 'Requires K', 'Can overfit']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Evaluate clustering algorithms\n",
    "clustering_results = {}\n",
    "\n",
    "for dataset_name, dataset_info in clustering_datasets.items():\n",
    "    print(f\"\\n--- Testing on {dataset_name} ---\")\n",
    "    \n",
    "    X, y_true = dataset_info['X'], dataset_info['y']\n",
    "    true_k = dataset_info.get('true_k', len(np.unique(y_true)))\n",
    "    \n",
    "    # Standardize features for clustering\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    dataset_clustering_results = {}\n",
    "    \n",
    "    for alg_name, alg_info in clustering_algorithms.items():\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Set number of clusters for algorithms that require it\n",
    "            params = alg_info['params'].copy()\n",
    "            if alg_name in ['K-Means', 'Agglomerative', 'Gaussian Mixture']:\n",
    "                params['n_clusters'] = true_k\n",
    "            elif alg_name == 'Gaussian Mixture':\n",
    "                params['n_components'] = true_k\n",
    "            \n",
    "            # Create and fit model\n",
    "            model = alg_info['class'](**params)\n",
    "            \n",
    "            if hasattr(model, 'fit_predict'):\n",
    "                labels = model.fit_predict(X_scaled)\n",
    "            else:\n",
    "                model.fit(X_scaled)\n",
    "                labels = model.labels_\n",
    "            \n",
    "            clustering_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate silhouette score\n",
    "            if len(np.unique(labels)) > 1:  # Need at least 2 clusters\n",
    "                silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "            else:\n",
    "                silhouette_avg = -1  # Invalid clustering\n",
    "            \n",
    "            # Calculate adjusted rand index if we have true labels\n",
    "            from sklearn.metrics import adjusted_rand_score\n",
    "            ari_score = adjusted_rand_score(y_true, labels)\n",
    "            \n",
    "            dataset_clustering_results[alg_name] = {\n",
    "                'silhouette_score': silhouette_avg,\n",
    "                'ari_score': ari_score,\n",
    "                'n_clusters_found': len(np.unique(labels)),\n",
    "                'clustering_time': clustering_time,\n",
    "                'labels': labels,\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "            print(f\"  {alg_name:<20}: Silhouette={silhouette_avg:.4f}, ARI={ari_score:.4f}, K={len(np.unique(labels))}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  {alg_name:<20}: ‚ùå Failed ({str(e)[:30]}...)\")\n",
    "            dataset_clustering_results[alg_name] = {'error': str(e)}\n",
    "    \n",
    "    clustering_results[dataset_name] = dataset_clustering_results\n",
    "\n",
    "print(\"\\n‚ú® Clustering algorithm evaluation complete!\")\n",
    "\n",
    "# Save clustering results\n",
    "clustering_summary = {}\n",
    "for dataset_name, algorithms in clustering_results.items():\n",
    "    clustering_summary[dataset_name] = {}\n",
    "    for alg_name, metrics in algorithms.items():\n",
    "        if 'error' not in metrics:\n",
    "            clustering_summary[dataset_name][alg_name] = {\n",
    "                'silhouette_score': metrics['silhouette_score'],\n",
    "                'ari_score': metrics['ari_score'],\n",
    "                'n_clusters_found': metrics['n_clusters_found'],\n",
    "                'clustering_time': metrics['clustering_time']\n",
    "            }\n",
    "\n",
    "save_experiment_results('clustering_algorithm_results', clustering_summary,\n",
    "                       'Comprehensive clustering algorithm evaluation results', 'clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1da5ca",
   "metadata": {},
   "source": [
    "## 6. Special Purpose Datasets {#special}\n",
    "\n",
    "Create datasets designed to highlight specific challenges and edge cases in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate special purpose datasets for edge cases\n",
    "print(\"‚ö° Generating Special Purpose Datasets...\")\n",
    "\n",
    "special_datasets = {}\n",
    "\n",
    "# Dataset 1: Extreme class imbalance\n",
    "print(\"  Creating extreme class imbalance dataset...\")\n",
    "X_imbalance, y_imbalance = generator.imbalanced_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=20,\n",
    "    imbalance_ratio=0.01,  # 1% minority class\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "special_datasets['Extreme_Imbalance'] = {\n",
    "    'X': X_imbalance, 'y': y_imbalance,\n",
    "    'type': 'classification',\n",
    "    'challenge': 'class_imbalance',\n",
    "    'description': 'Extreme class imbalance (1% minority)',\n",
    "    'optimal_algorithm': 'Balanced Random Forest'\n",
    "}\n",
    "\n",
    "# Dataset 2: High-dimensional sparse data\n",
    "print(\"  Creating high-dimensional sparse dataset...\")\n",
    "X_sparse, y_sparse = generator.sparse_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=500,\n",
    "    n_informative=20,\n",
    "    sparsity=0.95,  # 95% of features are zero\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "special_datasets['High_Dim_Sparse'] = {\n",
    "    'X': X_sparse, 'y': y_sparse,\n",
    "    'type': 'classification',\n",
    "    'challenge': 'high_dimensionality_sparsity',\n",
    "    'description': 'High-dimensional sparse features (95% sparsity)',\n",
    "    'optimal_algorithm': 'Linear SVM'\n",
    "}\n",
    "\n",
    "# Dataset 3: Mixed data types\n",
    "print(\"  Creating mixed data types dataset...\")\n",
    "X_mixed, y_mixed, feature_types = generator.mixed_data_types(\n",
    "    n_samples=800,\n",
    "    n_numerical=10,\n",
    "    n_categorical=8,\n",
    "    n_binary=5,\n",
    "    n_classes=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "special_datasets['Mixed_Data_Types'] = {\n",
    "    'X': X_mixed, 'y': y_mixed,\n",
    "    'type': 'classification',\n",
    "    'challenge': 'mixed_data_types',\n",
    "    'description': 'Mixed numerical, categorical, and binary features',\n",
    "    'optimal_algorithm': 'Gradient Boosting',\n",
    "    'feature_types': feature_types\n",
    "}\n",
    "\n",
    "# Dataset 4: Time series classification\n",
    "print(\"  Creating time series classification dataset...\")\n",
    "X_time_series, y_time_series = generator.time_series_classification(\n",
    "    n_samples=400,\n",
    "    n_timesteps=50,\n",
    "    n_features=3,\n",
    "    n_classes=4,\n",
    "    pattern_type='seasonal',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "special_datasets['Time_Series'] = {\n",
    "    'X': X_time_series, 'y': y_time_series,\n",
    "    'type': 'classification',\n",
    "    'challenge': 'temporal_dependencies',\n",
    "    'description': 'Time series with temporal dependencies',\n",
    "    'optimal_algorithm': 'LSTM (if available) or Random Forest'\n",
    "}\n",
    "\n",
    "# Dataset 5: Multi-label classification\n",
    "print(\"  Creating multi-label classification dataset...\")\n",
    "X_multilabel, y_multilabel = generator.multilabel_classification(\n",
    "    n_samples=1200,\n",
    "    n_features=25,\n",
    "    n_classes=8,\n",
    "    n_labels_per_sample=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "special_datasets['Multi_Label'] = {\n",
    "    'X': X_multilabel, 'y': y_multilabel,\n",
    "    'type': 'multilabel',\n",
    "    'challenge': 'multi_label',\n",
    "    'description': 'Multi-label classification problem',\n",
    "    'optimal_algorithm': 'Multi-label Random Forest'\n",
    "}\n",
    "\n",
    "# Dataset 6: Concept drift simulation\n",
    "print(\"  Creating concept drift dataset...\")\n",
    "X_drift, y_drift, drift_points = generator.concept_drift_classification(\n",
    "    n_samples=1500,\n",
    "    n_features=15,\n",
    "    n_drift_points=2,\n",
    "    drift_severity=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "special_datasets['Concept_Drift'] = {\n",
    "    'X': X_drift, 'y': y_drift,\n",
    "    'type': 'classification',\n",
    "    'challenge': 'concept_drift',\n",
    "    'description': 'Classification with concept drift over time',\n",
    "    'optimal_algorithm': 'Adaptive Random Forest',\n",
    "    'drift_points': drift_points\n",
    "}\n",
    "\n",
    "print(\"\\n‚ú® Special purpose dataset generation complete!\")\n",
    "\n",
    "# Save special datasets metadata\n",
    "special_metadata = {\n",
    "    'total_datasets': len(special_datasets),\n",
    "    'dataset_details': {name: {\n",
    "        'shape': info['X'].shape,\n",
    "        'type': info['type'],\n",
    "        'challenge': info['challenge'],\n",
    "        'description': info['description'],\n",
    "        'optimal_algorithm': info.get('optimal_algorithm', 'Unknown')\n",
    "    } for name, info in special_datasets.items()}\n",
    "}\n",
    "\n",
    "save_experiment_results('special_datasets_generation', special_metadata,\n",
    "                       'Special purpose datasets for edge cases and challenges', 'special')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf93743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize special purpose datasets\n",
    "print(\"üìä Visualizing Special Purpose Datasets...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "plot_idx = 0\n",
    "for dataset_name, dataset_info in special_datasets.items():\n",
    "    if plot_idx >= len(axes):\n",
    "        break\n",
    "        \n",
    "    ax = axes[plot_idx]\n",
    "    X, y = dataset_info['X'], dataset_info['y']\n",
    "    challenge = dataset_info['challenge']\n",
    "    description = dataset_info['description']\n",
    "    \n",
    "    if challenge == 'class_imbalance':\n",
    "        # Show class distribution\n",
    "        unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "        colors = ['red' if count < len(y) * 0.2 else 'blue' for count in class_counts]\n",
    "        bars = ax.bar(unique_classes, class_counts, color=colors, alpha=0.7)\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for bar, count in zip(bars, class_counts):\n",
    "            percentage = (count / len(y)) * 100\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + len(y) * 0.01,\n",
    "                   f'{percentage:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax.set_xlabel('Class')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title(f'{dataset_name}\\nClass Distribution')\n",
    "        \n",
    "    elif challenge == 'high_dimensionality_sparsity':\n",
    "        # Show sparsity pattern\n",
    "        sparsity_per_sample = np.mean(X == 0, axis=1)\n",
    "        ax.hist(sparsity_per_sample, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "        ax.axvline(np.mean(sparsity_per_sample), color='red', linestyle='--', \n",
    "                  label=f'Mean: {np.mean(sparsity_per_sample):.2%}')\n",
    "        ax.set_xlabel('Sparsity per Sample')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title(f'{dataset_name}\\nSparsity Distribution')\n",
    "        ax.legend()\n",
    "        \n",
    "    elif challenge == 'mixed_data_types':\n",
    "        # Show feature type distribution\n",
    "        feature_types = dataset_info['feature_types']\n",
    "        type_counts = {}\n",
    "        for ftype in feature_types:\n",
    "            type_counts[ftype] = type_counts.get(ftype, 0) + 1\n",
    "        \n",
    "        types = list(type_counts.keys())\n",
    "        counts = list(type_counts.values())\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(types)))\n",
    "        \n",
    "        bars = ax.bar(types, counts, color=colors, alpha=0.7)\n",
    "        ax.set_xlabel('Feature Type')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title(f'{dataset_name}\\nFeature Type Distribution')\n",
    "        \n",
    "        # Add count labels\n",
    "        for bar, count in zip(bars, counts):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                   str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    elif challenge == 'temporal_dependencies':\n",
    "        # Show time series pattern for first few samples\n",
    "        n_samples_show = min(5, X.shape[0])\n",
    "        for i in range(n_samples_show):\n",
    "            # Show first feature over time\n",
    "            ax.plot(X[i, :, 0], alpha=0.7, label=f'Sample {i+1} (Class {y[i]})')\n",
    "        \n",
    "        ax.set_xlabel('Time Step')\n",
    "        ax.set_ylabel('Feature Value')\n",
    "        ax.set_title(f'{dataset_name}\\nTime Series Patterns')\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    else:\n",
    "        # Default: scatter plot of first two features colored by class\n",
    "        if X.shape[1] >= 2:\n",
    "            scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='tab10', alpha=0.6, s=20)\n",
    "            ax.set_xlabel('Feature 1')\n",
    "            ax.set_ylabel('Feature 2')\n",
    "            plt.colorbar(scatter, ax=ax, shrink=0.8)\n",
    "        else:\n",
    "            # Class distribution\n",
    "            unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "            ax.bar(unique_classes, class_counts, alpha=0.7)\n",
    "            ax.set_xlabel('Class')\n",
    "            ax.set_ylabel('Count')\n",
    "        \n",
    "        ax.set_title(f'{dataset_name}\\nData Visualization')\n",
    "    \n",
    "    # Add challenge and description as subtitle\n",
    "    ax.text(0.5, -0.15, f'Challenge: {challenge}\\n{description}', \n",
    "            ha='center', va='top', transform=ax.transAxes, fontsize=8, style='italic')\n",
    "    \n",
    "    plot_idx += 1\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(plot_idx, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save special datasets visualization\n",
    "save_figure(fig, 'special_purpose_datasets',\n",
    "           'Visualization of special purpose datasets for edge cases', 'special')\n",
    "plt.show()\n",
    "\n",
    "# Special datasets summary\n",
    "print(\"\\nüìà Special Purpose Datasets Summary:\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"{'Dataset':<20} {'Type':<15} {'Shape':<15} {'Challenge':<20} {'Description':<35}\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "for name, info in special_datasets.items():\n",
    "    X, y = info['X'], info['y']\n",
    "    dataset_type = info['type']\n",
    "    challenge = info['challenge']\n",
    "    description = info['description'][:34] + \"...\" if len(info['description']) > 34 else info['description']\n",
    "    \n",
    "    if len(y.shape) > 1:  # Multi-label or time series\n",
    "        if len(X.shape) > 2:  # Time series\n",
    "            shape_str = f\"{X.shape}\"\n",
    "        else:\n",
    "            shape_str = f\"{X.shape} ‚Üí {y.shape}\"\n",
    "    else:\n",
    "        shape_str = f\"{X.shape} ‚Üí ({len(y)},)\"\n",
    "    \n",
    "    print(f\"{name:<20} {dataset_type:<15} {shape_str:<15} {challenge:<20} {description:<35}\")\n",
    "\n",
    "print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d585a9",
   "metadata": {},
   "source": [
    "## 7. Comparative Algorithm Analysis {#analysis}\n",
    "\n",
    "Now let's perform a comprehensive analysis comparing algorithm performance across all our generated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f683816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive cross-dataset algorithm performance analysis\n",
    "print(\"üî¨ Comprehensive Cross-Dataset Algorithm Performance Analysis...\")\n",
    "\n",
    "# Collect all classification datasets for comprehensive analysis\n",
    "all_classification_data = {}\n",
    "\n",
    "# Add classification complexity hierarchy datasets\n",
    "for name, info in classification_datasets.items():\n",
    "    all_classification_data[name] = {\n",
    "        'X': info['X'], 'y': info['y'],\n",
    "        'source': 'complexity_hierarchy',\n",
    "        'level': info['level'],\n",
    "        'complexity_factors': info.get('complexity_factors', {}),\n",
    "        'description': info['description'],\n",
    "        'optimal_algorithm': info.get('optimal_algorithm', 'Unknown')\n",
    "    }\n",
    "\n",
    "# Add special purpose classification datasets\n",
    "for name, info in special_datasets.items():\n",
    "    if info['type'] == 'classification':\n",
    "        all_classification_data[name] = {\n",
    "            'X': info['X'], 'y': info['y'],\n",
    "            'source': 'special_purpose',\n",
    "            'challenge': info['challenge'],\n",
    "            'description': info['description'],\n",
    "            'optimal_algorithm': info.get('optimal_algorithm', 'Unknown')\n",
    "        }\n",
    "\n",
    "print(f\"Total datasets for analysis: {len(all_classification_data)}\")\n",
    "\n",
    "# Create ultimate performance analysis visualization\n",
    "print(\"üìä Creating Ultimate Performance Analysis Visualization...\")\n",
    "\n",
    "# Prepare data for mega-analysis\n",
    "all_performance_data = []\n",
    "\n",
    "for dataset_name, dataset_info in all_classification_data.items():\n",
    "    X, y = dataset_info['X'], dataset_info['y']\n",
    "    \n",
    "    # Get results from benchmark if available\n",
    "    if dataset_name in algorithm_benchmark_results:\n",
    "        results = algorithm_benchmark_results[dataset_name]\n",
    "        \n",
    "        for alg_name, metrics in results.items():\n",
    "            if 'error' not in metrics:\n",
    "                row = {\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Algorithm': alg_name,\n",
    "                    'Source': dataset_info['source'],\n",
    "                    'Accuracy': metrics['accuracy'],\n",
    "                    'Training_Time': metrics['training_time'],\n",
    "                    'Total_Time': metrics['total_time'],\n",
    "                    'Generalization_Gap': metrics['generalization_gap'],\n",
    "                    'Optimal_Algorithm': dataset_info['optimal_algorithm']\n",
    "                }\n",
    "                \n",
    "                # Add dataset-specific information\n",
    "                if 'level' in dataset_info:\n",
    "                    row['Complexity_Level'] = dataset_info['level']\n",
    "                if 'challenge' in dataset_info:\n",
    "                    row['Challenge'] = dataset_info['challenge']\n",
    "                \n",
    "                all_performance_data.append(row)\n",
    "\n",
    "mega_df = pd.DataFrame(all_performance_data)\n",
    "\n",
    "# Create the ultimate visualization (2x2 grid)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Overall algorithm performance heatmap\n",
    "if not mega_df.empty:\n",
    "    perf_pivot = mega_df.pivot_table(index='Algorithm', columns='Dataset', values='Accuracy', aggfunc='mean')\n",
    "    sns.heatmap(perf_pivot, annot=False, cmap='RdYlGn', center=0.7, \n",
    "                cbar_kws={'label': 'Accuracy'}, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Ultimate Algorithm Performance Heatmap\\nAccuracy Across All Datasets')\n",
    "    axes[0, 0].set_ylabel('Algorithm')\n",
    "\n",
    "# 2. Performance vs efficiency trade-off\n",
    "if not mega_df.empty:\n",
    "    efficiency_data = mega_df.groupby('Algorithm').agg({\n",
    "        'Accuracy': 'mean',\n",
    "        'Training_Time': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    scatter = axes[0, 1].scatter(efficiency_data['Training_Time'], efficiency_data['Accuracy'], \n",
    "                                s=100, alpha=0.7, c=range(len(efficiency_data)), cmap='viridis')\n",
    "    \n",
    "    # Add algorithm labels\n",
    "    for _, row in efficiency_data.iterrows():\n",
    "        axes[0, 1].annotate(row['Algorithm'], (row['Training_Time'], row['Accuracy']), \n",
    "                           xytext=(3, 3), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Average Training Time (seconds)')\n",
    "    axes[0, 1].set_ylabel('Average Accuracy')\n",
    "    axes[0, 1].set_title('Algorithm Efficiency Analysis\\n(Top-left is optimal: High accuracy, Low time)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Performance by dataset source\n",
    "if not mega_df.empty:\n",
    "    source_perf = mega_df.groupby(['Source', 'Algorithm'])['Accuracy'].mean().reset_index()\n",
    "    \n",
    "    pivot_source = source_perf.pivot(index='Algorithm', columns='Source', values='Accuracy')\n",
    "    if not pivot_source.empty:\n",
    "        pivot_source.plot(kind='bar', ax=axes[1, 0], width=0.8)\n",
    "        axes[1, 0].set_title('Performance by Dataset Source\\nComplexity Hierarchy vs Special Purpose')\n",
    "        axes[1, 0].set_ylabel('Average Accuracy')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].legend(title='Dataset Source')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Algorithm vs Optimal Algorithm comparison\n",
    "if not mega_df.empty:\n",
    "    # Compare actual performance vs optimal algorithms\n",
    "    optimal_performance = []\n",
    "    \n",
    "    for dataset in mega_df['Dataset'].unique():\n",
    "        dataset_data = mega_df[mega_df['Dataset'] == dataset]\n",
    "        optimal_alg = dataset_data['Optimal_Algorithm'].iloc[0]\n",
    "        \n",
    "        best_actual_performance = dataset_data['Accuracy'].max()\n",
    "        best_actual_alg = dataset_data.loc[dataset_data['Accuracy'].idxmax(), 'Algorithm']\n",
    "        \n",
    "        # Check if optimal algorithm was tested\n",
    "        if optimal_alg in dataset_data['Algorithm'].values:\n",
    "            optimal_performance_score = dataset_data[dataset_data['Algorithm'] == optimal_alg]['Accuracy'].iloc[0]\n",
    "        else:\n",
    "            optimal_performance_score = None\n",
    "        \n",
    "        optimal_performance.append({\n",
    "            'Dataset': dataset,\n",
    "            'Optimal_Algorithm': optimal_alg,\n",
    "            'Best_Actual_Algorithm': best_actual_alg,\n",
    "            'Best_Actual_Performance': best_actual_performance,\n",
    "            'Optimal_Performance': optimal_performance_score,\n",
    "            'Match': optimal_alg == best_actual_alg\n",
    "        })\n",
    "    \n",
    "    optimal_df = pd.DataFrame(optimal_performance)\n",
    "    \n",
    "    # Show match percentage\n",
    "    match_rate = optimal_df['Match'].mean()\n",
    "    \n",
    "    # Create bar chart of optimal vs actual\n",
    "    datasets_subset = optimal_df.head(8)  # Show first 8 for readability\n",
    "    x_pos = np.arange(len(datasets_subset))\n",
    "    \n",
    "    bars1 = axes[1, 1].bar(x_pos - 0.2, datasets_subset['Best_Actual_Performance'], \n",
    "                          0.4, label='Best Actual', alpha=0.7)\n",
    "    \n",
    "    optimal_scores = [score if score is not None else 0 for score in datasets_subset['Optimal_Performance']]\n",
    "    bars2 = axes[1, 1].bar(x_pos + 0.2, optimal_scores, \n",
    "                          0.4, label='Predicted Optimal', alpha=0.7)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Dataset')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].set_title(f'Predicted vs Actual Best Performance\\nMatch Rate: {match_rate:.1%}')\n",
    "    axes[1, 1].set_xticks(x_pos)\n",
    "    axes[1, 1].set_xticklabels([d[:10] + '...' if len(d) > 10 else d for d in datasets_subset['Dataset']], \n",
    "                              rotation=45, ha='right')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save ultimate analysis visualization\n",
    "save_figure(fig, 'ultimate_algorithm_analysis',\n",
    "           'Ultimate algorithm performance analysis across all datasets', 'analysis')\n",
    "plt.show()\n",
    "\n",
    "# Performance insights\n",
    "if not mega_df.empty:\n",
    "    print(\"\\nüéØ Key Performance Insights:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Best overall algorithm\n",
    "    best_overall = mega_df.groupby('Algorithm')['Accuracy'].mean().idxmax()\n",
    "    best_score = mega_df.groupby('Algorithm')['Accuracy'].mean().max()\n",
    "    print(f\"üèÜ Best Overall Algorithm: {best_overall} ({best_score:.4f})\")\n",
    "    \n",
    "    # Most consistent algorithm\n",
    "    algo_consistency = mega_df.groupby('Algorithm')['Accuracy'].std()\n",
    "    most_consistent = algo_consistency.idxmin()\n",
    "    print(f\"üìä Most Consistent: {most_consistent} (std: {algo_consistency.min():.4f})\")\n",
    "    \n",
    "    # Fastest algorithm\n",
    "    fastest = mega_df.groupby('Algorithm')['Training_Time'].mean().idxmin()\n",
    "    fastest_time = mega_df.groupby('Algorithm')['Training_Time'].mean().min()\n",
    "    print(f\"‚ö° Fastest Training: {fastest} ({fastest_time:.4f}s)\")\n",
    "    \n",
    "    # Algorithm-dataset matching analysis\n",
    "    if 'optimal_df' in locals():\n",
    "        print(f\"üîç Optimal Algorithm Prediction Accuracy: {match_rate:.1%}\")\n",
    "        \n",
    "        # Most frequently optimal algorithms\n",
    "        optimal_counts = optimal_df['Optimal_Algorithm'].value_counts()\n",
    "        print(\"üéØ Most Frequently Optimal Algorithms:\")\n",
    "        for alg, count in optimal_counts.head(3).items():\n",
    "            print(f\"   {alg}: {count} datasets\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Save comprehensive analysis results\n",
    "comprehensive_analysis_results = {\n",
    "    'total_datasets_analyzed': len(all_classification_data),\n",
    "    'total_algorithm_evaluations': len(mega_df) if not mega_df.empty else 0,\n",
    "    'best_overall_algorithm': best_overall if not mega_df.empty else 'N/A',\n",
    "    'best_overall_score': float(best_score) if not mega_df.empty else 0,\n",
    "    'algorithm_rankings': mega_df.groupby('Algorithm')['Accuracy'].mean().sort_values(ascending=False).to_dict() if not mega_df.empty else {},\n",
    "    'dataset_sources': list(all_classification_data.keys()) if all_classification_data else [],\n",
    "    'optimal_algorithm_prediction_accuracy': float(match_rate) if 'optimal_df' in locals() else 0\n",
    "}\n",
    "\n",
    "save_experiment_results('comprehensive_analysis_results', comprehensive_analysis_results,\n",
    "                       'Complete analysis results across all datasets and algorithms', 'analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8334b7",
   "metadata": {},
   "source": [
    "## 8. Interactive Dataset Explorer {#explorer}\n",
    "\n",
    "Let's create an interactive exploration tool for our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b958ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive dataset explorer and recommendation system\n",
    "print(\"üîç Creating Interactive Dataset Explorer and Recommendation System...\")\n",
    "\n",
    "class DatasetExplorer:\n",
    "    \"\"\"Interactive dataset exploration and algorithm recommendation system.\"\"\"\n",
    "    \n",
    "    def __init__(self, datasets_dict, algorithm_results):\n",
    "        self.datasets = datasets_dict\n",
    "        self.results = algorithm_results\n",
    "        self.recommendations = {}\n",
    "        self._build_recommendation_system()\n",
    "    \n",
    "    def _build_recommendation_system(self):\n",
    "        \"\"\"Build algorithm recommendation system based on performance results.\"\"\"\n",
    "        for dataset_name, algorithms in self.results.items():\n",
    "            valid_results = {alg: metrics for alg, metrics in algorithms.items() if 'error' not in metrics}\n",
    "            \n",
    "            if valid_results:\n",
    "                # Rank algorithms by accuracy\n",
    "                acc_sorted = sorted(valid_results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "                \n",
    "                self.recommendations[dataset_name] = {\n",
    "                    'best_overall': acc_sorted[0][0],\n",
    "                    'best_score': acc_sorted[0][1]['accuracy'],\n",
    "                    'all_results': valid_results,\n",
    "                    'top_3': acc_sorted[:3]\n",
    "                }\n",
    "    \n",
    "    def explore_dataset(self, dataset_name):\n",
    "        \"\"\"Explore a specific dataset with detailed analysis.\"\"\"\n",
    "        if dataset_name not in self.datasets:\n",
    "            print(f\"‚ùå Dataset '{dataset_name}' not found!\")\n",
    "            available = list(self.datasets.keys())[:5]\n",
    "            print(f\"Available datasets: {', '.join(available)}{'...' if len(self.datasets) > 5 else ''}\")\n",
    "            return\n",
    "        \n",
    "        dataset_info = self.datasets[dataset_name]\n",
    "        X, y = dataset_info['X'], dataset_info['y']\n",
    "        \n",
    "        print(f\"\\nüîç DATASET EXPLORATION: {dataset_name}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(f\"üìä Basic Statistics:\")\n",
    "        print(f\"  ‚Ä¢ Shape: {X.shape}\")\n",
    "        print(f\"  ‚Ä¢ Classes: {len(np.unique(y))} {np.unique(y)}\")\n",
    "        print(f\"  ‚Ä¢ Class distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "        print(f\"  ‚Ä¢ Features: {X.shape[1]}\")\n",
    "        \n",
    "        # Feature statistics\n",
    "        print(f\"\\nüìà Feature Analysis:\")\n",
    "        print(f\"  ‚Ä¢ Feature means range: [{np.mean(X, axis=0).min():.3f}, {np.mean(X, axis=0).max():.3f}]\")\n",
    "        print(f\"  ‚Ä¢ Feature std range: [{np.std(X, axis=0).min():.3f}, {np.std(X, axis=0).max():.3f}]\")\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        class_balance = np.min(np.bincount(y)) / np.max(np.bincount(y))\n",
    "        feature_correlation = np.abs(np.corrcoef(X.T)).mean() if X.shape[1] > 1 else 0\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Class balance ratio: {class_balance:.3f}\")\n",
    "        print(f\"  ‚Ä¢ Average feature correlation: {feature_correlation:.3f}\")\n",
    "        \n",
    "        # Dataset characteristics\n",
    "        if 'description' in dataset_info:\n",
    "            print(f\"\\nüìù Description: {dataset_info['description']}\")\n",
    "        \n",
    "        if 'challenge' in dataset_info:\n",
    "            print(f\"üéØ Challenge: {dataset_info['challenge']}\")\n",
    "        \n",
    "        if 'optimal_algorithm' in dataset_info:\n",
    "            print(f\"üèÜ Predicted Optimal Algorithm: {dataset_info['optimal_algorithm']}\")\n",
    "        \n",
    "        # Algorithm recommendations\n",
    "        if dataset_name in self.recommendations:\n",
    "            recs = self.recommendations[dataset_name]\n",
    "            print(f\"\\nü§ñ Algorithm Performance Results:\")\n",
    "            print(f\"  üèÜ Best Performing: {recs['best_overall']} (Accuracy: {recs['best_score']:.4f})\")\n",
    "            \n",
    "            # Top 3 by accuracy\n",
    "            print(f\"\\n  üìã Top 3 Performers:\")\n",
    "            for i, (alg, metrics) in enumerate(recs['top_3'], 1):\n",
    "                cv_info = f\" (CV: {metrics['cv_mean']:.3f})\" if metrics.get('cv_mean', 0) > 0 else \"\"\n",
    "                time_info = f\", Time: {metrics['training_time']:.3f}s\" if 'training_time' in metrics else \"\"\n",
    "                print(f\"    {i}. {alg}: {metrics['accuracy']:.4f}{cv_info}{time_info}\")\n",
    "        \n",
    "        # Data quality assessment\n",
    "        print(f\"\\nüîç Data Quality Assessment:\")\n",
    "        \n",
    "        # Assess difficulty based on various factors\n",
    "        difficulty_factors = []\n",
    "        if class_balance < 0.1:\n",
    "            difficulty_factors.append('Severe class imbalance')\n",
    "        elif class_balance < 0.5:\n",
    "            difficulty_factors.append('Moderate class imbalance')\n",
    "        \n",
    "        if X.shape[1] > 100:\n",
    "            difficulty_factors.append('High dimensionality')\n",
    "        \n",
    "        if feature_correlation > 0.8:\n",
    "            difficulty_factors.append('High feature correlation')\n",
    "        \n",
    "        if len(difficulty_factors) > 0:\n",
    "            print(f\"  ‚ö†Ô∏è  Challenges: {', '.join(difficulty_factors)}\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ Standard difficulty dataset\")\n",
    "        \n",
    "        # Recommendations based on characteristics\n",
    "        print(f\"\\nüí° Recommendations:\")\n",
    "        if class_balance < 0.1:\n",
    "            print(f\"  ‚Ä¢ Consider resampling techniques (SMOTE, ADASYN)\")\n",
    "            print(f\"  ‚Ä¢ Use metrics like F1-score, AUC-ROC instead of accuracy\")\n",
    "        \n",
    "        if X.shape[1] > 50:\n",
    "            print(f\"  ‚Ä¢ Consider dimensionality reduction (PCA, feature selection)\")\n",
    "            print(f\"  ‚Ä¢ Linear models may perform well in high dimensions\")\n",
    "        \n",
    "        if feature_correlation > 0.7:\n",
    "            print(f\"  ‚Ä¢ Consider regularized models (Ridge, Lasso)\")\n",
    "            print(f\"  ‚Ä¢ Principal Component Analysis may help\")\n",
    "    \n",
    "    def find_best_for_criteria(self, criteria='accuracy', top_k=5):\n",
    "        \"\"\"Find best algorithms for specific criteria across all datasets.\"\"\"\n",
    "        print(f\"\\nüèÜ TOP {top_k} ALGORITHMS BY {criteria.upper()}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        algorithm_scores = {}\n",
    "        \n",
    "        for dataset_name, algorithms in self.results.items():\n",
    "            for alg, results in algorithms.items():\n",
    "                if 'error' not in results:\n",
    "                    if alg not in algorithm_scores:\n",
    "                        algorithm_scores[alg] = []\n",
    "                    \n",
    "                    if criteria == 'accuracy':\n",
    "                        algorithm_scores[alg].append(results['accuracy'])\n",
    "                    elif criteria == 'speed':\n",
    "                        algorithm_scores[alg].append(1.0 / (results.get('total_time', results.get('training_time', 1)) + 1e-6))\n",
    "                    elif criteria == 'consistency':\n",
    "                        gap = abs(results.get('generalization_gap', 0))\n",
    "                        algorithm_scores[alg].append(1.0 / (gap + 1e-6))\n",
    "                    elif criteria == 'confidence':\n",
    "                        algorithm_scores[alg].append(results.get('confidence', 0.5))\n",
    "        \n",
    "        # Calculate average scores\n",
    "        avg_scores = {alg: np.mean(scores) for alg, scores in algorithm_scores.items()}\n",
    "        \n",
    "        # Sort and display top k\n",
    "        sorted_algorithms = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for i, (alg, score) in enumerate(sorted_algorithms[:top_k], 1):\n",
    "            std_score = np.std(algorithm_scores[alg])\n",
    "            n_datasets = len(algorithm_scores[alg])\n",
    "            print(f\"  {i}. {alg:<20}: {score:.4f} ¬± {std_score:.4f} ({n_datasets} datasets)\")\n",
    "        \n",
    "        return sorted_algorithms[:top_k]\n",
    "    \n",
    "    def dataset_similarity_analysis(self, target_dataset):\n",
    "        \"\"\"Find datasets similar to the target dataset.\"\"\"\n",
    "        if target_dataset not in self.datasets:\n",
    "            print(f\"‚ùå Dataset '{target_dataset}' not found!\")\n",
    "            return\n",
    "        \n",
    "        target_info = self.datasets[target_dataset]\n",
    "        target_X, target_y = target_info['X'], target_info['y']\n",
    "        \n",
    "        similarities = []\n",
    "        \n",
    "        for dataset_name, dataset_info in self.datasets.items():\n",
    "            if dataset_name == target_dataset:\n",
    "                continue\n",
    "            \n",
    "            X, y = dataset_info['X'], dataset_info['y']\n",
    "            \n",
    "            # Calculate similarity based on multiple factors\n",
    "            similarity_score = 0\n",
    "            factors = 0\n",
    "            \n",
    "            # Shape similarity\n",
    "            shape_sim = 1 - abs(X.shape[0] - target_X.shape[0]) / max(X.shape[0], target_X.shape[0])\n",
    "            feature_sim = 1 - abs(X.shape[1] - target_X.shape[1]) / max(X.shape[1], target_X.shape[1])\n",
    "            class_sim = 1 - abs(len(np.unique(y)) - len(np.unique(target_y))) / max(len(np.unique(y)), len(np.unique(target_y)))\n",
    "            \n",
    "            similarity_score += (shape_sim + feature_sim + class_sim)\n",
    "            factors += 3\n",
    "            \n",
    "            # Challenge similarity\n",
    "            if 'challenge' in target_info and 'challenge' in dataset_info:\n",
    "                if target_info['challenge'] == dataset_info['challenge']:\n",
    "                    similarity_score += 1\n",
    "                factors += 1\n",
    "            \n",
    "            # Level similarity\n",
    "            if 'level' in target_info and 'level' in dataset_info:\n",
    "                level_sim = 1 - abs(target_info['level'] - dataset_info['level']) / 4  # Max level is 4\n",
    "                similarity_score += level_sim\n",
    "                factors += 1\n",
    "            \n",
    "            avg_similarity = similarity_score / factors if factors > 0 else 0\n",
    "            similarities.append((dataset_name, avg_similarity, dataset_info.get('description', 'No description')))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nüîç Datasets Most Similar to {target_dataset}:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i, (name, sim_score, description) in enumerate(similarities[:5], 1):\n",
    "            print(f\"{i}. {name} (Similarity: {sim_score:.3f})\")\n",
    "            print(f\"   {description}\")\n",
    "            print()\n",
    "        \n",
    "        return similarities[:5]\n",
    "    \n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"Generate comprehensive dataset analysis report.\"\"\"\n",
    "        report = \"\\n\" + \"=\"*80 + \"\\n\"\n",
    "        report += \"COMPREHENSIVE DATASET ANALYSIS REPORT\\n\"\n",
    "        report += \"=\"*80 + \"\\n\\n\"\n",
    "        \n",
    "        # Dataset portfolio summary\n",
    "        total_datasets = len(self.datasets)\n",
    "        total_samples = sum(info['X'].shape[0] for info in self.datasets.values())\n",
    "        avg_features = np.mean([info['X'].shape[1] for info in self.datasets.values()])\n",
    "        \n",
    "        report += f\"üìä Dataset Portfolio Summary:\\n\"\n",
    "        report += f\"  ‚Ä¢ Total datasets: {total_datasets}\\n\"\n",
    "        report += f\"  ‚Ä¢ Total samples: {total_samples:,}\\n\"\n",
    "        report += f\"  ‚Ä¢ Average features: {avg_features:.1f}\\n\\n\"\n",
    "        \n",
    "        # Algorithm performance summary\n",
    "        if self.recommendations:\n",
    "            all_algorithms = set()\n",
    "            for recs in self.recommendations.values():\n",
    "                all_algorithms.update(recs['all_results'].keys())\n",
    "            \n",
    "            algorithm_wins = {alg: 0 for alg in all_algorithms}\n",
    "            algorithm_scores = {alg: [] for alg in all_algorithms}\n",
    "            \n",
    "            for dataset_name, recs in self.recommendations.items():\n",
    "                best_alg = recs['best_overall']\n",
    "                algorithm_wins[best_alg] += 1\n",
    "                \n",
    "                for alg, metrics in recs['all_results'].items():\n",
    "                    algorithm_scores[alg].append(metrics['accuracy'])\n",
    "            \n",
    "            report += f\"üèÜ Algorithm Championship:\\n\"\n",
    "            sorted_wins = sorted(algorithm_wins.items(), key=lambda x: x[1], reverse=True)\n",
    "            for i, (alg, wins) in enumerate(sorted_wins[:5], 1):\n",
    "                win_rate = wins / len(self.recommendations) * 100\n",
    "                avg_score = np.mean(algorithm_scores[alg]) if algorithm_scores[alg] else 0\n",
    "                report += f\"  {i}. {alg:<20}: {wins} wins ({win_rate:.1f}%), Avg Score: {avg_score:.3f}\\n\"\n",
    "            \n",
    "            report += \"\\n\"\n",
    "        \n",
    "        # Dataset complexity distribution\n",
    "        complexity_dist = {}\n",
    "        for info in self.datasets.values():\n",
    "            level = info.get('level', 'special')\n",
    "            complexity_dist[level] = complexity_dist.get(level, 0) + 1\n",
    "        \n",
    "        report += f\"üìà Dataset Complexity Distribution:\\n\"\n",
    "        for level, count in sorted(complexity_dist.items()):\n",
    "            percentage = (count / total_datasets) * 100\n",
    "            report += f\"  Level {level}: {count} datasets ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        report += \"\\n\" + \"=\"*80 + \"\\n\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def list_available_datasets(self):\n",
    "        \"\"\"List all available datasets with brief descriptions.\"\"\"\n",
    "        print(\"\\nüìö Available Datasets:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Group by source\n",
    "        regression_datasets = []\n",
    "        classification_datasets = []\n",
    "        clustering_datasets = []\n",
    "        special_datasets = []\n",
    "        \n",
    "        for name, info in self.datasets.items():\n",
    "            dataset_type = info.get('type', 'classification')\n",
    "            description = info.get('description', 'No description')\n",
    "            shape = info['X'].shape\n",
    "            \n",
    "            entry = f\"{name:<25} {str(shape):<15} {description[:40]}\"\n",
    "            \n",
    "            if 'regression' in name.lower() or dataset_type == 'regression':\n",
    "                regression_datasets.append(entry)\n",
    "            elif 'clustering' in name.lower() or dataset_type == 'clustering':\n",
    "                clustering_datasets.append(entry)\n",
    "            elif info.get('source') == 'special_purpose':\n",
    "                special_datasets.append(entry)\n",
    "            else:\n",
    "                classification_datasets.append(entry)\n",
    "        \n",
    "        if classification_datasets:\n",
    "            print(\"\\nüéØ Classification Datasets:\")\n",
    "            for entry in classification_datasets:\n",
    "                print(f\"  {entry}\")\n",
    "        \n",
    "        if regression_datasets:\n",
    "            print(\"\\nüìà Regression Datasets:\")\n",
    "            for entry in regression_datasets:\n",
    "                print(f\"  {entry}\")\n",
    "        \n",
    "        if clustering_datasets:\n",
    "            print(\"\\nüîó Clustering Datasets:\")\n",
    "            for entry in clustering_datasets:\n",
    "                print(f\"  {entry}\")\n",
    "        \n",
    "        if special_datasets:\n",
    "            print(\"\\n‚ö° Special Purpose Datasets:\")\n",
    "            for entry in special_datasets:\n",
    "                print(f\"  {entry}\")\n",
    "        \n",
    "        print(\"\\nüí° Use explorer.explore_dataset('dataset_name') for detailed analysis\")\n",
    "\n",
    "# Initialize the explorer\n",
    "print(\"üîß Initializing Dataset Explorer...\")\n",
    "\n",
    "all_dataset_info = {}\n",
    "all_dataset_info.update({name: info for name, info in classification_datasets.items()})\n",
    "all_dataset_info.update({name: info for name, info in special_datasets.items() if info['type'] == 'classification'})\n",
    "all_dataset_info.update({name: info for name, info in regression_datasets.items()})\n",
    "all_dataset_info.update({name: info for name, info in clustering_datasets.items()})\n",
    "\n",
    "explorer = DatasetExplorer(all_dataset_info, algorithm_benchmark_results)\n",
    "\n",
    "print(\"‚úÖ Dataset Explorer initialized!\")\n",
    "print(f\"üìä Total datasets available: {len(all_dataset_info)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af00888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the explorer functionality\n",
    "print(\"\\nüé¨ DATASET EXPLORER DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. List available datasets\n",
    "explorer.list_available_datasets()\n",
    "\n",
    "# 2. Explore specific datasets\n",
    "example_datasets = ['Easy_Linear', 'Moons_Pattern', 'Extreme_Imbalance']\n",
    "\n",
    "for dataset_name in example_datasets:\n",
    "    if dataset_name in all_dataset_info:\n",
    "        explorer.explore_dataset(dataset_name)\n",
    "        \n",
    "        # Show similarity analysis for first dataset\n",
    "        if dataset_name == example_datasets[0]:\n",
    "            explorer.dataset_similarity_analysis(dataset_name)\n",
    "\n",
    "# 3. Find best algorithms by different criteria\n",
    "criteria_list = ['accuracy', 'speed', 'consistency']\n",
    "for criteria in criteria_list:\n",
    "    top_algorithms = explorer.find_best_for_criteria(criteria, top_k=3)\n",
    "\n",
    "# 4. Generate comprehensive report\n",
    "comprehensive_report = explorer.generate_comprehensive_report()\n",
    "print(comprehensive_report)\n",
    "\n",
    "# Save the comprehensive report\n",
    "save_report(comprehensive_report, 'dataset_explorer_report',\n",
    "           'Comprehensive dataset analysis and exploration report', 'explorer')\n",
    "\n",
    "print(\"\\n‚ú® Dataset Explorer demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f5a914",
   "metadata": {},
   "source": [
    "## 9. Performance Benchmarking {#benchmarking}\n",
    "\n",
    "Comprehensive benchmarking system for evaluating model performance across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2804f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced benchmarking system\n",
    "print(\"‚ö° Advanced Performance Benchmarking System...\")\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"Comprehensive performance benchmarking system.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.benchmark_results = {}\n",
    "        self.timing_results = {}\n",
    "        self.memory_results = {}\n",
    "    \n",
    "    def benchmark_algorithm(self, algorithm, X_train, X_test, y_train, y_test, \n",
    "                          algorithm_name, dataset_name):\n",
    "        \"\"\"Comprehensive benchmarking of a single algorithm.\"\"\"\n",
    "        import psutil\n",
    "        import gc\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Memory before training\n",
    "        process = psutil.Process()\n",
    "        memory_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        try:\n",
    "            # Training phase\n",
    "            gc.collect()  # Clean memory\n",
    "            start_time = time.time()\n",
    "            algorithm.fit(X_train, y_train)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Memory after training\n",
    "            memory_after_training = process.memory_info().rss / 1024 / 1024  # MB\n",
    "            \n",
    "            # Prediction phase\n",
    "            start_time = time.time()\n",
    "            predictions = algorithm.predict(X_test)\n",
    "            prediction_time = time.time() - start_time\n",
    "            \n",
    "            # Memory after prediction\n",
    "            memory_after_prediction = process.memory_info().rss / 1024 / 1024  # MB\n",
    "            \n",
    "            # Performance metrics\n",
    "            if hasattr(algorithm, 'predict_proba'):\n",
    "                probabilities = algorithm.predict_proba(X_test)\n",
    "                confidence = np.mean(np.max(probabilities, axis=1))\n",
    "            else:\n",
    "                confidence = None\n",
    "            \n",
    "            # Classification metrics\n",
    "            if len(np.unique(y_test)) > 1:\n",
    "                accuracy = accuracy_score(y_test, predictions)\n",
    "                \n",
    "                # Additional metrics for binary classification\n",
    "                if len(np.unique(y_test)) == 2:\n",
    "                    from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "                    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "                    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "                    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "                    \n",
    "                    if confidence is not None:\n",
    "                        try:\n",
    "                            auc = roc_auc_score(y_test, probabilities[:, 1])\n",
    "                        except:\n",
    "                            auc = None\n",
    "                    else:\n",
    "                        auc = None\n",
    "                else:\n",
    "                    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "                    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "                    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "                    auc = None\n",
    "            else:\n",
    "                accuracy = precision = recall = f1 = auc = 0.0\n",
    "            \n",
    "            # Cross-validation\n",
    "            try:\n",
    "                cv_scores = cross_val_score(algorithm, X_train, y_train, cv=3, scoring='accuracy')\n",
    "                cv_mean = cv_scores.mean()\n",
    "                cv_std = cv_scores.std()\n",
    "            except:\n",
    "                cv_mean = cv_std = 0.0\n",
    "            \n",
    "            # Compile results\n",
    "            results = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'auc_score': auc,\n",
    "                'confidence': confidence,\n",
    "                'cv_mean': cv_mean,\n",
    "                'cv_std': cv_std,\n",
    "                'training_time': training_time,\n",
    "                'prediction_time': prediction_time,\n",
    "                'total_time': training_time + prediction_time,\n",
    "                'memory_usage_mb': memory_after_training - memory_before,\n",
    "                'memory_peak_mb': memory_after_prediction - memory_before,\n",
    "                'samples_per_second': len(X_test) / prediction_time if prediction_time > 0 else 0,\n",
    "                'model_size_params': self._estimate_model_size(algorithm)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            results = {'error': str(e)}\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _estimate_model_size(self, model):\n",
    "        \"\"\"Estimate model complexity/size.\"\"\"\n",
    "        if hasattr(model, 'coef_'):\n",
    "            return np.size(model.coef_)\n",
    "        elif hasattr(model, 'n_estimators'):\n",
    "            return model.n_estimators\n",
    "        elif hasattr(model, 'support_vectors_'):\n",
    "            return len(model.support_vectors_)\n",
    "        elif hasattr(model, 'hidden_layer_sizes'):\n",
    "            total_params = sum(model.hidden_layer_sizes)\n",
    "            return total_params\n",
    "        else:\n",
    "            return 1  # Simple model\n",
    "    \n",
    "    def run_comprehensive_benchmark(self, datasets, algorithms, test_size=0.3):\n",
    "        \"\"\"Run comprehensive benchmark across all datasets and algorithms.\"\"\"\n",
    "        print(\"üöÄ Running Comprehensive Performance Benchmark...\")\n",
    "        \n",
    "        for dataset_name, dataset_info in datasets.items():\n",
    "            print(f\"\\n--- Benchmarking on {dataset_name} ---\")\n",
    "            \n",
    "            X, y = dataset_info['X'], dataset_info['y']\n",
    "            \n",
    "            # Skip if dataset is too large for benchmarking\n",
    "            if X.shape[0] > 5000:\n",
    "                print(f\"  Skipping {dataset_name} - too large for detailed benchmarking\")\n",
    "                continue\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=test_size, random_state=42, \n",
    "                stratify=y if len(np.unique(y)) > 1 else None\n",
    "            )\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            dataset_results = {}\n",
    "            \n",
    "            for alg_name, alg_info in algorithms.items():\n",
    "                print(f\"  Testing {alg_name}...\")\n",
    "                \n",
    "                # Create fresh instance\n",
    "                algorithm = alg_info['model'].__class__(**alg_info['model'].get_params())\n",
    "                \n",
    "                # Benchmark\n",
    "                results = self.benchmark_algorithm(\n",
    "                    algorithm, X_train_scaled, X_test_scaled, y_train, y_test,\n",
    "                    alg_name, dataset_name\n",
    "                )\n",
    "                \n",
    "                dataset_results[alg_name] = results\n",
    "                \n",
    "                if 'error' not in results:\n",
    "                    print(f\"    Accuracy: {results['accuracy']:.4f}, \"\n",
    "                          f\"Time: {results['total_time']:.3f}s, \"\n",
    "                          f\"Memory: {results['memory_usage_mb']:.1f}MB\")\n",
    "                else:\n",
    "                    print(f\"    ‚ùå Failed: {results['error']}\")\n",
    "            \n",
    "            self.benchmark_results[dataset_name] = dataset_results\n",
    "        \n",
    "        print(\"\\n‚ú® Comprehensive benchmark complete!\")\n",
    "        return self.benchmark_results\n",
    "    \n",
    "    def generate_benchmark_report(self):\n",
    "        \"\"\"Generate comprehensive benchmark report.\"\"\"\n",
    "        if not self.benchmark_results:\n",
    "            return \"No benchmark results available.\"\n",
    "        \n",
    "        report = \"\\n\" + \"=\"*100 + \"\\n\"\n",
    "        report += \"COMPREHENSIVE PERFORMANCE BENCHMARK REPORT\\n\"\n",
    "        report += \"=\"*100 + \"\\n\\n\"\n",
    "        \n",
    "        # Overall statistics\n",
    "        total_tests = sum(len(algorithms) for algorithms in self.benchmark_results.values())\n",
    "        successful_tests = sum(\n",
    "            len([alg for alg, results in algorithms.items() if 'error' not in results])\n",
    "            for algorithms in self.benchmark_results.values()\n",
    "        )\n",
    "        \n",
    "        report += f\"üìä Benchmark Overview:\\n\"\n",
    "        report += f\"  ‚Ä¢ Total tests: {total_tests}\\n\"\n",
    "        report += f\"  ‚Ä¢ Successful tests: {successful_tests}\\n\"\n",
    "        report += f\"  ‚Ä¢ Success rate: {successful_tests/total_tests*100:.1f}%\\n\\n\"\n",
    "        \n",
    "        # Performance rankings\n",
    "        all_results = []\n",
    "        for dataset_name, algorithms in self.benchmark_results.items():\n",
    "            for alg_name, results in algorithms.items():\n",
    "                if 'error' not in results:\n",
    "                    all_results.append({\n",
    "                        'dataset': dataset_name,\n",
    "                        'algorithm': alg_name,\n",
    "                        **results\n",
    "                    })\n",
    "        \n",
    "        if all_results:\n",
    "            df = pd.DataFrame(all_results)\n",
    "            \n",
    "            # Top performers by accuracy\n",
    "            report += f\"üèÜ Top 10 Performers by Accuracy:\\n\"\n",
    "            top_accuracy = df.nlargest(10, 'accuracy')\n",
    "            for i, (_, row) in enumerate(top_accuracy.iterrows(), 1):\n",
    "                report += f\"  {i:2d}. {row['algorithm']} on {row['dataset']}: {row['accuracy']:.4f}\\n\"\n",
    "            \n",
    "            # Fastest algorithms\n",
    "            report += f\"\\n‚ö° Fastest Algorithms (by total time):\\n\"\n",
    "            fastest = df.nsmallest(10, 'total_time')\n",
    "            for i, (_, row) in enumerate(fastest.iterrows(), 1):\n",
    "                report += f\"  {i:2d}. {row['algorithm']} on {row['dataset']}: {row['total_time']:.4f}s\\n\"\n",
    "            \n",
    "            # Most memory efficient\n",
    "            report += f\"\\nüíæ Most Memory Efficient:\\n\"\n",
    "            memory_efficient = df.nsmallest(10, 'memory_usage_mb')\n",
    "            for i, (_, row) in enumerate(memory_efficient.iterrows(), 1):\n",
    "                report += f\"  {i:2d}. {row['algorithm']} on {row['dataset']}: {row['memory_usage_mb']:.1f}MB\\n\"\n",
    "            \n",
    "            # Algorithm summary\n",
    "            report += f\"\\nüìà Algorithm Performance Summary:\\n\"\n",
    "            algo_summary = df.groupby('algorithm').agg({\n",
    "                'accuracy': ['mean', 'std'],\n",
    "                'total_time': ['mean', 'std'],\n",
    "                'memory_usage_mb': ['mean', 'std']\n",
    "            }).round(4)\n",
    "            \n",
    "            for alg in algo_summary.index:\n",
    "                acc_mean = algo_summary.loc[alg, ('accuracy', 'mean')]\n",
    "                acc_std = algo_summary.loc[alg, ('accuracy', 'std')]\n",
    "                time_mean = algo_summary.loc[alg, ('total_time', 'mean')]\n",
    "                report += f\"  {alg}: Acc {acc_mean:.4f}¬±{acc_std:.4f}, Time {time_mean:.4f}s\\n\"\n",
    "        \n",
    "        report += \"\\n\" + \"=\"*100 + \"\\n\"\n",
    "        return report\n",
    "\n",
    "# Run comprehensive benchmark\n",
    "print(\"\\n--- Running Performance Benchmark ---\")\n",
    "\n",
    "# Use a subset of algorithms for detailed benchmarking\n",
    "benchmark_algorithms = {\n",
    "    'Logistic Regression': classification_algorithms['Logistic Regression'],\n",
    "    'Random Forest': classification_algorithms['Random Forest'],\n",
    "    'SVM (RBF)': classification_algorithms['SVM (RBF)'],\n",
    "    'Gradient Boosting': classification_algorithms['Gradient Boosting']\n",
    "}\n",
    "\n",
    "# Use a subset of datasets for benchmarking\n",
    "benchmark_datasets = {name: info for name, info in classification_datasets.items() \n",
    "                     if info['X'].shape[0] <= 2000}  # Limit to smaller datasets\n",
    "\n",
    "# Initialize and run benchmark\n",
    "benchmark = PerformanceBenchmark()\n",
    "benchmark_results = benchmark.run_comprehensive_benchmark(benchmark_datasets, benchmark_algorithms)\n",
    "\n",
    "# Generate and display report\n",
    "benchmark_report = benchmark.generate_benchmark_report()\n",
    "print(benchmark_report)\n",
    "\n",
    "# Save benchmark results\n",
    "save_experiment_results('performance_benchmark', benchmark_results,\n",
    "                       'Comprehensive performance benchmark results', 'benchmarking')\n",
    "\n",
    "save_report(benchmark_report, 'performance_benchmark_report',\n",
    "           'Detailed performance benchmark analysis', 'benchmarking')\n",
    "\n",
    "print(\"\\n‚ú® Performance benchmarking complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d88001",
   "metadata": {},
   "source": [
    "## 10. Comprehensive Results Saving {#saving}\n",
    "\n",
    "Save all generated models, datasets, and comprehensive analysis results with detailed metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23daad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive results saving with enhanced metadata\n",
    "print(\"üíæ COMPREHENSIVE RESULTS SAVING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def save_all_trained_models():\n",
    "    \"\"\"Save all trained models from the analysis.\"\"\"\n",
    "    print(\"ü§ñ Saving all trained models...\")\n",
    "    \n",
    "    models_saved = 0\n",
    "    \n",
    "    # Save regression models\n",
    "    if 'algorithm_performance' in globals():\n",
    "        for dataset_name, algorithms in algorithm_performance.items():\n",
    "            for alg_name, metrics in algorithms.items():\n",
    "                if 'model' in metrics and 'error' not in metrics:\n",
    "                    model_metadata = {\n",
    "                        'dataset': dataset_name,\n",
    "                        'algorithm': alg_name,\n",
    "                        'test_r2': metrics.get('test_r2', 'N/A'),\n",
    "                        'cv_mean': metrics.get('cv_mean', 'N/A'),\n",
    "                        'training_time': metrics.get('training_time', 'N/A'),\n",
    "                        'model_type': 'regression'\n",
    "                    }\n",
    "                    save_model(metrics['model'], \n",
    "                             f\"{dataset_name}_{alg_name}_regression\",\n",
    "                             f\"Regression model: {alg_name} on {dataset_name}\",\n",
    "                             'regression', model_metadata)\n",
    "                    models_saved += 1\n",
    "    \n",
    "    # Save classification models\n",
    "    if 'algorithm_benchmark_results' in globals():\n",
    "        for dataset_name, algorithms in algorithm_benchmark_results.items():\n",
    "            for alg_name, metrics in algorithms.items():\n",
    "                if 'model' in metrics and 'error' not in metrics:\n",
    "                    model_metadata = {\n",
    "                        'dataset': dataset_name,\n",
    "                        'algorithm': alg_name,\n",
    "                        'accuracy': metrics.get('accuracy', 'N/A'),\n",
    "                        'cv_mean': metrics.get('cv_mean', 'N/A'),\n",
    "                        'training_time': metrics.get('training_time', 'N/A'),\n",
    "                        'model_type': 'classification'\n",
    "                    }\n",
    "                    save_model(metrics['model'], \n",
    "                             f\"{dataset_name}_{alg_name}_classification\",\n",
    "                             f\"Classification model: {alg_name} on {dataset_name}\",\n",
    "                             'classification', model_metadata)\n",
    "                    models_saved += 1\n",
    "    \n",
    "    # Save clustering models\n",
    "    if 'clustering_results' in globals():\n",
    "        for dataset_name, algorithms in clustering_results.items():\n",
    "            for alg_name, metrics in algorithms.items():\n",
    "                if 'model' in metrics and 'error' not in metrics:\n",
    "                    model_metadata = {\n",
    "                        'dataset': dataset_name,\n",
    "                        'algorithm': alg_name,\n",
    "                        'silhouette_score': metrics.get('silhouette_score', 'N/A'),\n",
    "                        'ari_score': metrics.get('ari_score', 'N/A'),\n",
    "                        'clustering_time': metrics.get('clustering_time', 'N/A'),\n",
    "                        'model_type': 'clustering'\n",
    "                    }\n",
    "                    save_model(metrics['model'], \n",
    "                             f\"{dataset_name}_{alg_name}_clustering\",\n",
    "                             f\"Clustering model: {alg_name} on {dataset_name}\",\n",
    "                             'clustering', model_metadata)\n",
    "                    models_saved += 1\n",
    "    \n",
    "    print(f\"‚úÖ Saved {models_saved} trained models\")\n",
    "    return models_saved\n",
    "\n",
    "def save_all_datasets():\n",
    "    \"\"\"Save all generated datasets.\"\"\"\n",
    "    print(\"üìä Saving all generated datasets...\")\n",
    "    \n",
    "    datasets_saved = 0\n",
    "    \n",
    "    # Save regression datasets\n",
    "    if 'regression_datasets' in globals():\n",
    "        for name, info in regression_datasets.items():\n",
    "            dataset_package = {\n",
    "                'X': info['X'],\n",
    "                'y': info['y'],\n",
    "                'true_coef': info.get('true_coef'),\n",
    "                'metadata': {\n",
    "                    'description': info['description'],\n",
    "                    'optimal_algorithm': info.get('optimal_algorithm'),\n",
    "                    'challenge': info.get('challenge'),\n",
    "                    'shape': info['X'].shape,\n",
    "                    'type': 'regression'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            filepath = results_dir / 'data_generation' / f\"{name}_regression_dataset.joblib\"\n",
    "            joblib.dump(dataset_package, filepath, compress=3)\n",
    "            datasets_saved += 1\n",
    "    \n",
    "    # Save classification datasets\n",
    "    if 'classification_datasets' in globals():\n",
    "        for name, info in classification_datasets.items():\n",
    "            dataset_package = {\n",
    "                'X': info['X'],\n",
    "                'y': info['y'],\n",
    "                'metadata': {\n",
    "                    'description': info['description'],\n",
    "                    'level': info.get('level'),\n",
    "                    'optimal_algorithm': info.get('optimal_algorithm'),\n",
    "                    'complexity_factors': info.get('complexity_factors'),\n",
    "                    'shape': info['X'].shape,\n",
    "                    'type': 'classification'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            filepath = results_dir / 'data_generation' / f\"{name}_classification_dataset.joblib\"\n",
    "            joblib.dump(dataset_package, filepath, compress=3)\n",
    "            datasets_saved += 1\n",
    "    \n",
    "    # Save clustering datasets\n",
    "    if 'clustering_datasets' in globals():\n",
    "        for name, info in clustering_datasets.items():\n",
    "            dataset_package = {\n",
    "                'X': info['X'],\n",
    "                'y': info['y'],\n",
    "                'metadata': {\n",
    "                    'description': info['description'],\n",
    "                    'optimal_algorithm': info.get('optimal_algorithm'),\n",
    "                    'challenge': info.get('challenge'),\n",
    "                    'true_k': info.get('true_k'),\n",
    "                    'shape': info['X'].shape,\n",
    "                    'type': 'clustering'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            filepath = results_dir / 'data_generation' / f\"{name}_clustering_dataset.joblib\"\n",
    "            joblib.dump(dataset_package, filepath, compress=3)\n",
    "            datasets_saved += 1\n",
    "    \n",
    "    # Save special datasets\n",
    "    if 'special_datasets' in globals():\n",
    "        for name, info in special_datasets.items():\n",
    "            dataset_package = {\n",
    "                'X': info['X'],\n",
    "                'y': info['y'],\n",
    "                'metadata': {\n",
    "                    'description': info['description'],\n",
    "                    'type': info['type'],\n",
    "                    'challenge': info['challenge'],\n",
    "                    'optimal_algorithm': info.get('optimal_algorithm'),\n",
    "                    'shape': info['X'].shape\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            filepath = results_dir / 'data_generation' / f\"{name}_special_dataset.joblib\"\n",
    "            joblib.dump(dataset_package, filepath, compress=3)\n",
    "            datasets_saved += 1\n",
    "    \n",
    "    print(f\"‚úÖ Saved {datasets_saved} datasets\")\n",
    "    return datasets_saved\n",
    "\n",
    "def generate_final_summary():\n",
    "    \"\"\"Generate final comprehensive summary.\"\"\"\n",
    "    summary = {\n",
    "        'notebook_execution': {\n",
    "            'completion_time': get_timestamp(),\n",
    "            'notebook_name': '01_data_generation_showcase',\n",
    "            'status': 'completed'\n",
    "        },\n",
    "        'datasets_generated': {\n",
    "            'regression': len(regression_datasets) if 'regression_datasets' in globals() else 0,\n",
    "            'classification': len(classification_datasets) if 'classification_datasets' in globals() else 0,\n",
    "            'clustering': len(clustering_datasets) if 'clustering_datasets' in globals() else 0,\n",
    "            'special': len(special_datasets) if 'special_datasets' in globals() else 0,\n",
    "            'total': (len(regression_datasets) if 'regression_datasets' in globals() else 0) + \n",
    "                    (len(classification_datasets) if 'classification_datasets' in globals() else 0) + \n",
    "                    (len(clustering_datasets) if 'clustering_datasets' in globals() else 0) + \n",
    "                    (len(special_datasets) if 'special_datasets' in globals() else 0)\n",
    "        },\n",
    "        'algorithms_tested': {\n",
    "            'regression': len(regression_algorithms) if 'regression_algorithms' in globals() else 0,\n",
    "            'classification': len(classification_algorithms) if 'classification_algorithms' in globals() else 0,\n",
    "            'clustering': len(clustering_algorithms) if 'clustering_algorithms' in globals() else 0\n",
    "        },\n",
    "        'performance_insights': {\n",
    "            'best_overall_classifier': best_overall if 'best_overall' in globals() else 'N/A',\n",
    "            'best_classifier_score': float(best_score) if 'best_score' in globals() else 0,\n",
    "            'total_experiments': len(mega_df) if 'mega_df' in globals() and not mega_df.empty else 0,\n",
    "            'optimal_prediction_accuracy': float(match_rate) if 'match_rate' in globals() else 0\n",
    "        },\n",
    "        'resources_saved': {\n",
    "            'figures': len(list((results_dir / 'figures').glob('*data_generation*.png'))),\n",
    "            'models': 0,  # Will be updated\n",
    "            'datasets': 0,  # Will be updated\n",
    "            'reports': len(list((results_dir / 'reports').glob('*data_generation*.txt')))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Execute comprehensive saving\n",
    "print(\"\\nüîÑ Executing comprehensive results saving...\")\n",
    "\n",
    "# Save all models\n",
    "models_count = save_all_trained_models()\n",
    "\n",
    "# Save all datasets\n",
    "datasets_count = save_all_datasets()\n",
    "\n",
    "# Generate final summary\n",
    "final_summary = generate_final_summary()\n",
    "final_summary['resources_saved']['models'] = models_count\n",
    "final_summary['resources_saved']['datasets'] = datasets_count\n",
    "\n",
    "# Save final summary\n",
    "save_experiment_results('final_data_generation_summary', final_summary,\n",
    "                       'Comprehensive summary of all data generation activities', 'summary')\n",
    "\n",
    "# Generate master report\n",
    "master_report = f\"\"\"\n",
    "{'='*100}\n",
    "MASTER DATA GENERATION SHOWCASE REPORT\n",
    "{'='*100}\n",
    "\n",
    "üéØ EXECUTIVE SUMMARY\n",
    "{'-'*50}\n",
    "This comprehensive data generation showcase successfully demonstrated the creation and \n",
    "analysis of sophisticated synthetic datasets across multiple machine learning domains.\n",
    "\n",
    "üìä DATASETS GENERATED\n",
    "{'-'*25}\n",
    "‚Ä¢ Regression Datasets: {final_summary['datasets_generated']['regression']}\n",
    "‚Ä¢ Classification Datasets: {final_summary['datasets_generated']['classification']}\n",
    "‚Ä¢ Clustering Datasets: {final_summary['datasets_generated']['clustering']}\n",
    "‚Ä¢ Special Purpose Datasets: {final_summary['datasets_generated']['special']}\n",
    "‚Ä¢ Total Datasets: {final_summary['datasets_generated']['total']}\n",
    "\n",
    "ü§ñ ALGORITHMS EVALUATED\n",
    "{'-'*30}\n",
    "‚Ä¢ Regression Algorithms: {final_summary['algorithms_tested']['regression']}\n",
    "‚Ä¢ Classification Algorithms: {final_summary['algorithms_tested']['classification']}\n",
    "‚Ä¢ Clustering Algorithms: {final_summary['algorithms_tested']['clustering']}\n",
    "\n",
    "üèÜ KEY PERFORMANCE INSIGHTS\n",
    "{'-'*35}\n",
    "‚Ä¢ Best Overall Classifier: {final_summary['performance_insights']['best_overall_classifier']}\n",
    "‚Ä¢ Best Classification Score: {final_summary['performance_insights']['best_classifier_score']:.4f}\n",
    "‚Ä¢ Total Experiments Conducted: {final_summary['performance_insights']['total_experiments']}\n",
    "‚Ä¢ Algorithm Prediction Accuracy: {final_summary['performance_insights']['optimal_prediction_accuracy']:.1%}\n",
    "\n",
    "üíæ RESOURCES SAVED\n",
    "{'-'*20}\n",
    "‚Ä¢ Visualization Figures: {final_summary['resources_saved']['figures']}\n",
    "‚Ä¢ Trained Models: {final_summary['resources_saved']['models']}\n",
    "‚Ä¢ Dataset Files: {final_summary['resources_saved']['datasets']}\n",
    "‚Ä¢ Analysis Reports: {final_summary['resources_saved']['reports']}\n",
    "\n",
    "üéì METHODOLOGICAL CONTRIBUTIONS\n",
    "{'-'*40}\n",
    "1. Progressive Complexity Hierarchy: Developed systematic approach to dataset complexity\n",
    "2. Comprehensive Algorithm Benchmarking: Standardized evaluation across diverse scenarios\n",
    "3. Interactive Exploration System: Created tools for dataset analysis and recommendation\n",
    "4. Special Purpose Edge Cases: Addressed real-world ML challenges and limitations\n",
    "5. Performance Optimization: Demonstrated efficiency considerations in algorithm selection\n",
    "\n",
    "üîÆ PRACTICAL APPLICATIONS\n",
    "{'-'*30}\n",
    "‚Ä¢ Algorithm Selection Guidance: Data-driven recommendations for optimal algorithms\n",
    "‚Ä¢ Educational Resource: Comprehensive examples for ML education and training\n",
    "‚Ä¢ Benchmark Standard: Reproducible evaluation framework for new algorithms\n",
    "‚Ä¢ Research Foundation: Baseline datasets for ML research and development\n",
    "\n",
    "‚ú® CONCLUSION\n",
    "{'-'*15}\n",
    "The data generation showcase successfully created a comprehensive ecosystem of synthetic\n",
    "datasets that effectively demonstrate the strengths, weaknesses, and optimal use cases\n",
    "of various machine learning algorithms. The systematic approach to complexity progression\n",
    "and comprehensive evaluation provides valuable insights for both practitioners and\n",
    "researchers in the field of machine learning.\n",
    "\n",
    "All results, models, and datasets have been systematically saved with detailed metadata\n",
    "for future reference and reproducibility.\n",
    "\n",
    "{'='*100}\n",
    "Report Generated: {final_summary['notebook_execution']['completion_time']}\n",
    "Status: {final_summary['notebook_execution']['status'].upper()}\n",
    "{'='*100}\n",
    "\"\"\"\n",
    "\n",
    "# Save master report\n",
    "save_report(master_report, 'master_data_generation_report',\n",
    "           'Master report summarizing all data generation activities', 'summary')\n",
    "\n",
    "print(master_report)\n",
    "\n",
    "print(\"\\nüéâ DATA GENERATION SHOWCASE COMPLETE!\")\n",
    "print(\"\\nüî¨ Key Achievements:\")\n",
    "print(\"   ‚Ä¢ Generated comprehensive dataset portfolio across ML domains\")\n",
    "print(\"   ‚Ä¢ Demonstrated systematic algorithm evaluation methodology\")\n",
    "print(\"   ‚Ä¢ Created interactive exploration and recommendation tools\")\n",
    "print(\"   ‚Ä¢ Established performance benchmarking standards\")\n",
    "print(\"   ‚Ä¢ Produced reproducible research-quality results\")\n",
    "print(\"\\nüíæ All results systematically saved with detailed metadata\")\n",
    "print(\"üìä Comprehensive visualizations and analysis completed\")\n",
    "print(\"üìã Master documentation generated for future reference\")\n",
    "\n",
    "print(f\"\\nüìÅ Results location: {results_dir}\")\n",
    "print(\"‚ú® Ready for integration with advanced ML techniques! ‚ú®\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
