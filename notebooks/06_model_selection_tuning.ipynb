{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08992bc",
   "metadata": {},
   "source": [
    "# Advanced Model Selection and Hyperparameter Tuning\n",
    "\n",
    "This notebook demonstrates sophisticated model selection techniques and hyperparameter optimization strategies available in the sklearn-mastery project, including automated model selection, advanced optimization algorithms, and multi-objective optimization.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Automated Model Selection](#automated)\n",
    "3. [Advanced Hyperparameter Optimization](#optimization)\n",
    "4. [Multi-Objective Optimization](#multi-objective)\n",
    "5. [Bayesian Optimization](#bayesian)\n",
    "6. [Population-Based Training](#population)\n",
    "7. [Cross-Validation Strategies](#cross-validation)\n",
    "8. [Model Selection Pipelines](#pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f761063",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e31df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score,\n",
    "    StratifiedKFold, GroupKFold, TimeSeriesSplit, validation_curve, learning_curve\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import uniform, randint\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced ML imports\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import joblib\n",
    "\n",
    "# Results saving imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae90e3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Project imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.generators import SyntheticDataGenerator\n",
    "from pipelines.model_selection import AdvancedModelSelector\n",
    "from evaluation.metrics import ModelEvaluator\n",
    "from evaluation.visualization import ModelVisualizationSuite\n",
    "from utils.helpers import performance_timer\n",
    "from utils.decorators import memory_profiler\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d934691",
   "metadata": {},
   "source": [
    "### Results Management Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cecfb4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Results saving setup for model selection\n",
    "def setup_results_directories():\n",
    "    \"\"\"Create results directory structure if it doesn't exist.\"\"\"\n",
    "    base_dir = Path('../results')\n",
    "    directories = [\n",
    "        base_dir / 'figures',\n",
    "        base_dir / 'models',\n",
    "        base_dir / 'optimized_models',  # Specific for hyperparameter optimized models\n",
    "        base_dir / 'pipelines',\n",
    "        base_dir / 'experiments',\n",
    "        base_dir / 'reports'\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"ðŸ“ Created/verified directory: {directory}\")\n",
    "    \n",
    "    return base_dir\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\"Get current timestamp for file naming.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def save_figure(fig, name, description=\"\", category=\"general\", dpi=300):\n",
    "    \"\"\"Save figure with proper naming and metadata.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_model_selection_{category}_{name}.png\"\n",
    "    filepath = results_dir / 'figures' / filename\n",
    "    \n",
    "    # Save figure\n",
    "    fig.savefig(filepath, dpi=dpi, bbox_inches='tight', facecolor='white')\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'filename': filename,\n",
    "        'description': description,\n",
    "        'category': category,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '06_model_selection_tuning',\n",
    "        'dpi': dpi\n",
    "    }\n",
    "    \n",
    "    metadata_file = filepath.with_suffix('.json')\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"ðŸ’¾ Saved figure: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_optimized_model(model, name, description=\"\", optimization_results=None):\n",
    "    \"\"\"Save optimized model with comprehensive metadata.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_optimized_{name}.joblib\"\n",
    "    filepath = results_dir / 'optimized_models' / filename\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, filepath, compress=3)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'filename': filename,\n",
    "        'model_name': name,\n",
    "        'description': description,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '06_model_selection_tuning',\n",
    "        'model_type': type(model).__name__,\n",
    "        'optimization_results': optimization_results or {},\n",
    "        'file_size_mb': filepath.stat().st_size / (1024*1024) if filepath.exists() else 0\n",
    "    }\n",
    "    \n",
    "    # Add model-specific metadata\n",
    "    if hasattr(model, 'best_params_'):\n",
    "        metadata['best_params'] = model.best_params_\n",
    "        metadata['best_score'] = model.best_score_\n",
    "    \n",
    "    if hasattr(model, 'cv_results_'):\n",
    "        metadata['n_iterations'] = len(model.cv_results_['params'])\n",
    "    \n",
    "    metadata_file = filepath.with_suffix('.json')\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"ðŸ’¾ Saved optimized model: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_optimization_experiment(experiment_name, results, description=\"\"):\n",
    "    \"\"\"Save optimization experiment results with detailed configuration.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_optimization_{experiment_name}.json\"\n",
    "    filepath = results_dir / 'experiments' / filename\n",
    "    \n",
    "    experiment_data = {\n",
    "        'experiment_name': experiment_name,\n",
    "        'description': description,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '06_model_selection_tuning',\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(experiment_data, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"ðŸ’¾ Saved optimization experiment: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_model_selection_report(content, report_name, format='txt'):\n",
    "    \"\"\"Save comprehensive model selection report.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_model_selection_report_{report_name}.{format}\"\n",
    "    filepath = results_dir / 'reports' / filename\n",
    "    \n",
    "    if format == 'txt':\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(content)\n",
    "    elif format == 'json':\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(content, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"ðŸ’¾ Saved report: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_pipeline(pipeline, name, description=\"\", performance_metrics=None):\n",
    "    \"\"\"Save model selection pipeline with metadata.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_pipeline_{name}.joblib\"\n",
    "    filepath = results_dir / 'pipelines' / filename\n",
    "    \n",
    "    # Save pipeline\n",
    "    joblib.dump(pipeline, filepath, compress=3)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'filename': filename,\n",
    "        'pipeline_name': name,\n",
    "        'description': description,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '06_model_selection_tuning',\n",
    "        'pipeline_type': type(pipeline).__name__,\n",
    "        'performance_metrics': performance_metrics or {},\n",
    "        'file_size_mb': filepath.stat().st_size / (1024*1024) if filepath.exists() else 0\n",
    "    }\n",
    "    \n",
    "    metadata_file = filepath.with_suffix('.json')\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"ðŸ’¾ Saved pipeline: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "# Initialize results directories\n",
    "results_dir = setup_results_directories()\n",
    "print(f\"ðŸ“Š Results will be saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0854d4",
   "metadata": {},
   "source": [
    "## 2. Automated Model Selection {#automated}\n",
    "\n",
    "Let's explore automated model selection techniques that can find the best algorithm for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87999c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate datasets for model selection\n",
    "print(\"ðŸŽ¯ Generating Datasets for Model Selection...\")\n",
    "\n",
    "generator = SyntheticDataGenerator(random_state=42)\n",
    "\n",
    "# Dataset 1: Complex classification\n",
    "X_complex, y_complex = generator.classification_dataset(\n",
    "    n_samples=2000,\n",
    "    n_features=25,\n",
    "    n_informative=20,\n",
    "    n_redundant=3,\n",
    "    n_classes=3,\n",
    "    n_clusters_per_class=2,\n",
    "    class_sep=0.8\n",
    ")\n",
    "\n",
    "print(f\"Complex classification dataset: {X_complex.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_complex)}\")\n",
    "\n",
    "# Dataset 2: Imbalanced classification\n",
    "X_imbal, y_imbal = generator.imbalanced_classification(\n",
    "    n_samples=1500,\n",
    "    n_features=20,\n",
    "    imbalance_ratio=0.1\n",
    ")\n",
    "\n",
    "print(f\"Imbalanced classification dataset: {X_imbal.shape}\")\n",
    "print(f\"Imbalanced class distribution: {np.bincount(y_imbal)}\")\n",
    "\n",
    "# Dataset 3: High-dimensional\n",
    "X_highdim, y_highdim = generator.classification_dataset(\n",
    "    n_samples=1000,\n",
    "    n_features=100,\n",
    "    n_informative=15,\n",
    "    n_redundant=10,\n",
    "    n_classes=2,\n",
    "    class_sep=0.6\n",
    ")\n",
    "\n",
    "print(f\"High-dimensional dataset: {X_highdim.shape}\")\n",
    "print(f\"High-dim class distribution: {np.bincount(y_highdim)}\")\n",
    "\n",
    "# Split datasets\n",
    "datasets = {\n",
    "    'Complex': train_test_split(X_complex, y_complex, test_size=0.3, random_state=42, stratify=y_complex),\n",
    "    'Imbalanced': train_test_split(X_imbal, y_imbal, test_size=0.3, random_state=42, stratify=y_imbal),\n",
    "    'High-Dimensional': train_test_split(X_highdim, y_highdim, test_size=0.3, random_state=42, stratify=y_highdim)\n",
    "}\n",
    "\n",
    "print(\"\\nâœ¨ Datasets prepared for model selection!\")\n",
    "\n",
    "# Save dataset characteristics\n",
    "dataset_info = {\n",
    "    'complex': {\n",
    "        'shape': X_complex.shape,\n",
    "        'class_distribution': np.bincount(y_complex).tolist(),\n",
    "        'n_features': X_complex.shape[1],\n",
    "        'n_classes': len(np.unique(y_complex))\n",
    "    },\n",
    "    'imbalanced': {\n",
    "        'shape': X_imbal.shape,\n",
    "        'class_distribution': np.bincount(y_imbal).tolist(),\n",
    "        'imbalance_ratio': min(np.bincount(y_imbal)) / max(np.bincount(y_imbal))\n",
    "    },\n",
    "    'high_dimensional': {\n",
    "        'shape': X_highdim.shape,\n",
    "        'class_distribution': np.bincount(y_highdim).tolist(),\n",
    "        'dimensionality_ratio': X_highdim.shape[1] / X_highdim.shape[0]\n",
    "    }\n",
    "}\n",
    "\n",
    "save_optimization_experiment('dataset_generation', dataset_info, \n",
    "                           'Generated datasets for model selection experiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd2c1c",
   "metadata": {},
   "source": [
    "### Automated Model Selection Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b03d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Custom automated model selection implementation\n",
    "print(\"ðŸ¤– Implementing Automated Model Selection...\")\n",
    "\n",
    "class AdvancedModelSelector:\n",
    "    \"\"\"Advanced automated model selection with comprehensive evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, cv_folds=5, scoring='accuracy', n_jobs=-1, verbose=True):\n",
    "        self.cv_folds = cv_folds\n",
    "        self.scoring = scoring\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Define candidate models\n",
    "        self.candidate_models = {\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'Random Forest': RandomForestClassifier(random_state=42),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "            'SVM': SVC(random_state=42),\n",
    "            'Neural Network': MLPClassifier(random_state=42, max_iter=500),\n",
    "            'Gaussian Process': GaussianProcessClassifier(random_state=42)\n",
    "        }\n",
    "    \n",
    "    def compare_models(self, X, y, task_type='classification'):\n",
    "        \"\"\"Compare all candidate models using cross-validation.\"\"\"\n",
    "        comparison_results = {}\n",
    "        \n",
    "        for name, model in self.candidate_models.items():\n",
    "            if self.verbose:\n",
    "                print(f\"  Evaluating {name}...\")\n",
    "            \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(model, X, y, cv=self.cv_folds, \n",
    "                                          scoring=self.scoring, n_jobs=self.n_jobs)\n",
    "                \n",
    "                evaluation_time = time.time() - start_time\n",
    "                \n",
    "                comparison_results[name] = {\n",
    "                    'mean_score': cv_scores.mean(),\n",
    "                    'std_score': cv_scores.std(),\n",
    "                    'scores': cv_scores.tolist(),\n",
    "                    'evaluation_time': evaluation_time\n",
    "                }\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"    Score: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"    Failed: {str(e)}\")\n",
    "                comparison_results[name] = {'error': str(e)}\n",
    "        \n",
    "        # Find best model\n",
    "        valid_results = {k: v for k, v in comparison_results.items() if 'error' not in v}\n",
    "        if valid_results:\n",
    "            best_model_name = max(valid_results, key=lambda x: valid_results[x]['mean_score'])\n",
    "            best_model = self.candidate_models[best_model_name]\n",
    "            best_model.fit(X, y)  # Fit on full data\n",
    "            \n",
    "            return best_model, comparison_results\n",
    "        else:\n",
    "            return None, comparison_results\n",
    "\n",
    "# Test automated model selection\n",
    "model_selector = AdvancedModelSelector(\n",
    "    cv_folds=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "selection_results = {}\n",
    "\n",
    "for dataset_name, (X_train, X_test, y_train, y_test) in datasets.items():\n",
    "    print(f\"\\n--- Automated Selection for {dataset_name} Dataset ---\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run automated model selection\n",
    "        best_model, comparison_results = model_selector.compare_models(\n",
    "            X_train, y_train, task_type='classification'\n",
    "        )\n",
    "        \n",
    "        selection_time = time.time() - start_time\n",
    "        \n",
    "        if best_model:\n",
    "            # Test best model\n",
    "            test_score = best_model.score(X_test, y_test)\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            \n",
    "            selection_results[dataset_name] = {\n",
    "                'best_model': best_model.__class__.__name__,\n",
    "                'best_cv_score': max([result['mean_score'] for result in comparison_results.values() \n",
    "                                    if 'error' not in result]),\n",
    "                'test_score': test_score,\n",
    "                'selection_time': selection_time,\n",
    "                'all_results': comparison_results,\n",
    "                'y_pred': y_pred,\n",
    "                'y_test': y_test\n",
    "            }\n",
    "            \n",
    "            print(f\"Best Model: {best_model.__class__.__name__}\")\n",
    "            print(f\"Best CV Score: {selection_results[dataset_name]['best_cv_score']:.4f}\")\n",
    "            print(f\"Test Score: {test_score:.4f}\")\n",
    "            print(f\"Selection Time: {selection_time:.2f}s\")\n",
    "            \n",
    "            # Save the best model for each dataset\n",
    "            optimization_results = {\n",
    "                'best_cv_score': selection_results[dataset_name]['best_cv_score'],\n",
    "                'test_score': test_score,\n",
    "                'selection_time': selection_time,\n",
    "                'dataset': dataset_name,\n",
    "                'all_model_scores': {name: result.get('mean_score', 0) \n",
    "                                   for name, result in comparison_results.items() \n",
    "                                   if 'error' not in result}\n",
    "            }\n",
    "            \n",
    "            save_optimized_model(best_model, f\"autoselected_{dataset_name.lower()}_best_{best_model.__class__.__name__.lower()}\",\n",
    "                               f\"Best model selected automatically for {dataset_name} dataset\",\n",
    "                               optimization_results)\n",
    "        else:\n",
    "            print(\"âŒ No valid models found\")\n",
    "            selection_results[dataset_name] = {'error': 'No valid models'}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {str(e)}\")\n",
    "        selection_results[dataset_name] = {'error': str(e)}\n",
    "\n",
    "print(\"\\nâœ¨ Automated model selection complete!\")\n",
    "\n",
    "# Save comprehensive model selection results\n",
    "model_selection_summary = {dataset: {\n",
    "    'best_model': results.get('best_model', 'Failed'),\n",
    "    'best_cv_score': results.get('best_cv_score', 0),\n",
    "    'test_score': results.get('test_score', 0),\n",
    "    'selection_time': results.get('selection_time', 0)\n",
    "} for dataset, results in selection_results.items() if 'error' not in results}\n",
    "\n",
    "save_optimization_experiment('automated_model_selection', model_selection_summary,\n",
    "                           'Results from automated model selection across different datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee2993",
   "metadata": {},
   "source": [
    "### Model Selection Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30491f4e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize automated model selection results\n",
    "print(\"ðŸ“Š Visualizing Automated Model Selection Results...\")\n",
    "\n",
    "if selection_results:\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Best models by dataset\n",
    "    dataset_names = []\n",
    "    best_models = []\n",
    "    test_scores = []\n",
    "    \n",
    "    for dataset, result in selection_results.items():\n",
    "        if 'error' not in result:\n",
    "            dataset_names.append(dataset)\n",
    "            best_models.append(result['best_model'])\n",
    "            test_scores.append(result['test_score'])\n",
    "    \n",
    "    if dataset_names:\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(set(best_models))))\n",
    "        model_colors = {model: colors[i] for i, model in enumerate(set(best_models))}\n",
    "        bar_colors = [model_colors[model] for model in best_models]\n",
    "        \n",
    "        bars = axes[0, 0].bar(dataset_names, test_scores, color=bar_colors, alpha=0.7)\n",
    "        axes[0, 0].set_title('Best Models by Dataset')\n",
    "        axes[0, 0].set_ylabel('Test Accuracy')\n",
    "        axes[0, 0].set_ylim(0, 1)\n",
    "        \n",
    "        # Add model names on bars\n",
    "        for bar, model, score in zip(bars, best_models, test_scores):\n",
    "            axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                           model, ha='center', va='bottom', rotation=45, fontsize=8)\n",
    "            axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height()/2, \n",
    "                           f'{score:.3f}', ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        # 2. Selection time comparison\n",
    "        selection_times = [selection_results[name]['selection_time'] for name in dataset_names\n",
    "                          if 'selection_time' in selection_results[name]]\n",
    "        \n",
    "        if selection_times:\n",
    "            axes[0, 1].bar(dataset_names, selection_times, color='lightcoral', alpha=0.7)\n",
    "            axes[0, 1].set_title('Model Selection Time')\n",
    "            axes[0, 1].set_ylabel('Time (seconds)')\n",
    "            \n",
    "            for i, (name, time_val) in enumerate(zip(dataset_names, selection_times)):\n",
    "                axes[0, 1].text(i, time_val + max(selection_times) * 0.02, \n",
    "                               f'{time_val:.1f}s', ha='center', va='bottom')\n",
    "        \n",
    "        # 3. Model performance comparison for first dataset\n",
    "        if dataset_names and 'all_results' in selection_results[dataset_names[0]]:\n",
    "            first_dataset = dataset_names[0]\n",
    "            all_results = selection_results[first_dataset]['all_results']\n",
    "            \n",
    "            model_names = [name for name, result in all_results.items() if 'error' not in result]\n",
    "            cv_scores = [all_results[model]['mean_score'] for model in model_names]\n",
    "            cv_stds = [all_results[model]['std_score'] for model in model_names]\n",
    "            \n",
    "            bars = axes[1, 0].bar(range(len(model_names)), cv_scores, \n",
    "                                 yerr=cv_stds, capsize=5, color='lightgreen', alpha=0.7)\n",
    "            axes[1, 0].set_title(f'Model Comparison - {first_dataset} Dataset')\n",
    "            axes[1, 0].set_ylabel('CV Accuracy')\n",
    "            axes[1, 0].set_xticks(range(len(model_names)))\n",
    "            axes[1, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "            \n",
    "            # Highlight best model\n",
    "            best_idx = np.argmax(cv_scores)\n",
    "            bars[best_idx].set_color('gold')\n",
    "            bars[best_idx].set_alpha(0.9)\n",
    "        \n",
    "        # 4. Performance vs dataset characteristics\n",
    "        dataset_characteristics = {\n",
    "            'Complex': {'n_features': 25, 'n_classes': 3, 'imbalance': 1.0},\n",
    "            'Imbalanced': {'n_features': 20, 'n_classes': 2, 'imbalance': 0.1},\n",
    "            'High-Dimensional': {'n_features': 100, 'n_classes': 2, 'imbalance': 1.0}\n",
    "        }\n",
    "        \n",
    "        if len(dataset_names) >= 2:\n",
    "            x_vals = [dataset_characteristics[name]['n_features'] for name in dataset_names]\n",
    "            y_vals = test_scores\n",
    "            \n",
    "            scatter = axes[1, 1].scatter(x_vals, y_vals, c=range(len(dataset_names)), \n",
    "                                       cmap='viridis', s=100, alpha=0.7)\n",
    "            \n",
    "            for i, (x, y, name) in enumerate(zip(x_vals, y_vals, dataset_names)):\n",
    "                axes[1, 1].annotate(name, (x, y), xytext=(5, 5), \n",
    "                                  textcoords='offset points', fontsize=9)\n",
    "            \n",
    "            axes[1, 1].set_xlabel('Number of Features')\n",
    "            axes[1, 1].set_ylabel('Test Accuracy')\n",
    "            axes[1, 1].set_title('Performance vs Dataset Complexity')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save automated selection visualization\n",
    "    save_figure(fig, 'automated_model_selection_results',\n",
    "               'Comprehensive results from automated model selection across datasets', 'selection')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\nðŸ“Š Automated Model Selection Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Dataset':<15} {'Best Model':<20} {'CV Score':<10} {'Test Score':<10} {'Time(s)':<10}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for dataset in dataset_names:\n",
    "        result = selection_results[dataset]\n",
    "        print(f\"{dataset:<15} {result['best_model']:<20} {result['best_cv_score']:<10.4f} \"\n",
    "              f\"{result['test_score']:<10.4f} {result.get('selection_time', 0):<10.2f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ¨ Automated model selection visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95f1a3",
   "metadata": {},
   "source": [
    "## 3. Advanced Hyperparameter Optimization {#optimization}\n",
    "\n",
    "Let's explore sophisticated hyperparameter optimization techniques beyond grid and random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98ca24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Advanced hyperparameter optimization\n",
    "print(\"âš™ï¸ Advanced Hyperparameter Optimization...\")\n",
    "\n",
    "# Use the complex dataset for optimization\n",
    "X_train, X_test, y_train, y_test = datasets['Complex']\n",
    "\n",
    "# Define models and parameter spaces\n",
    "optimization_tasks = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'param_space': {\n",
    "            'n_estimators': randint(50, 200),\n",
    "            'max_depth': randint(3, 20),\n",
    "            'min_samples_split': randint(2, 20),\n",
    "            'min_samples_leaf': randint(1, 10),\n",
    "            'max_features': ['sqrt', 'log2', 0.5, 0.8]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'param_space': {\n",
    "            'n_estimators': randint(50, 150),\n",
    "            'learning_rate': uniform(0.01, 0.2),\n",
    "            'max_depth': randint(3, 10),\n",
    "            'subsample': uniform(0.6, 0.4),\n",
    "            'min_samples_split': randint(2, 15)\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(random_state=42),\n",
    "        'param_space': {\n",
    "            'C': uniform(0.1, 10),\n",
    "            'gamma': ['scale', 'auto'] + list(uniform(0.001, 1).rvs(3)),\n",
    "            'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Compare different optimization strategies\n",
    "optimization_strategies = {\n",
    "    'Random Search': 'random',\n",
    "    'Grid Search': 'grid'\n",
    "}\n",
    "\n",
    "optimization_results = {}\n",
    "\n",
    "for model_name, task in optimization_tasks.items():\n",
    "    print(f\"\\n--- Optimizing {model_name} ---\")\n",
    "    \n",
    "    optimization_results[model_name] = {}\n",
    "    \n",
    "    for strategy_name, strategy_type in optimization_strategies.items():\n",
    "        print(f\"\\n  {strategy_name}:\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if strategy_type == 'random':\n",
    "                # Random search\n",
    "                search = RandomizedSearchCV(\n",
    "                    estimator=task['model'],\n",
    "                    param_distributions=task['param_space'],\n",
    "                    n_iter=50,\n",
    "                    cv=5,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42\n",
    "                )\n",
    "                search.fit(X_train, y_train)\n",
    "                \n",
    "            elif strategy_type == 'grid':\n",
    "                # Grid search (with reduced parameter space)\n",
    "                grid_params = {}\n",
    "                for param, values in task['param_space'].items():\n",
    "                    if hasattr(values, 'rvs'):  # Continuous distribution\n",
    "                        if param in ['learning_rate', 'subsample']:\n",
    "                            grid_params[param] = [0.01, 0.1, 0.2]\n",
    "                        elif param == 'C':\n",
    "                            grid_params[param] = [0.1, 1, 10]\n",
    "                        else:\n",
    "                            grid_params[param] = values.rvs(3, random_state=42)\n",
    "                    elif isinstance(values, list):\n",
    "                        grid_params[param] = values[:3]  # Limit to first 3\n",
    "                    else:\n",
    "                        grid_params[param] = [values.rvs(1, random_state=42)[0] for _ in range(3)]\n",
    "                \n",
    "                search = GridSearchCV(\n",
    "                    estimator=task['model'],\n",
    "                    param_grid=grid_params,\n",
    "                    cv=5,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                search.fit(X_train, y_train)\n",
    "            \n",
    "            optimization_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate best model\n",
    "            best_model = search.best_estimator_\n",
    "            test_score = best_model.score(X_test, y_test)\n",
    "            \n",
    "            optimization_results[model_name][strategy_name] = {\n",
    "                'best_params': search.best_params_,\n",
    "                'best_cv_score': search.best_score_,\n",
    "                'test_score': test_score,\n",
    "                'optimization_time': optimization_time,\n",
    "                'n_iterations': getattr(search, 'n_iter', len(search.cv_results_['params']))\n",
    "            }\n",
    "            \n",
    "            print(f\"    Best CV Score: {search.best_score_:.4f}\")\n",
    "            print(f\"    Test Score: {test_score:.4f}\")\n",
    "            print(f\"    Optimization Time: {optimization_time:.2f}s\")\n",
    "            print(f\"    Best Params: {search.best_params_}\")\n",
    "            \n",
    "            # Save optimized model\n",
    "            opt_results = {\n",
    "                'strategy': strategy_name,\n",
    "                'best_cv_score': search.best_score_,\n",
    "                'test_score': test_score,\n",
    "                'optimization_time': optimization_time,\n",
    "                'n_iterations': optimization_results[model_name][strategy_name]['n_iterations'],\n",
    "                'search_space_size': len(task['param_space'])\n",
    "            }\n",
    "            \n",
    "            save_optimized_model(search, f\"{strategy_type}_{model_name.lower().replace(' ', '_')}\",\n",
    "                               f\"{model_name} optimized using {strategy_name}\", opt_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    âŒ Failed: {str(e)}\")\n",
    "\n",
    "print(\"\\nâœ¨ Advanced hyperparameter optimization complete!\")\n",
    "\n",
    "# Save optimization comparison results\n",
    "optimization_summary = {}\n",
    "for model_name, strategies in optimization_results.items():\n",
    "    optimization_summary[model_name] = {}\n",
    "    for strategy_name, results in strategies.items():\n",
    "        optimization_summary[model_name][strategy_name] = {\n",
    "            'best_cv_score': results['best_cv_score'],\n",
    "            'test_score': results['test_score'],\n",
    "            'optimization_time': results['optimization_time'],\n",
    "            'n_iterations': results['n_iterations']\n",
    "        }\n",
    "\n",
    "save_optimization_experiment('hyperparameter_optimization_comparison', optimization_summary,\n",
    "                           'Comparison of different hyperparameter optimization strategies')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508748e",
   "metadata": {},
   "source": [
    "### Optimization Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb31d39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize optimization results\n",
    "print(\"ðŸ“Š Visualizing Optimization Results...\")\n",
    "\n",
    "if optimization_results:\n",
    "    # Prepare data for visualization\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, strategies in optimization_results.items():\n",
    "        for strategy_name, result in strategies.items():\n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'Strategy': strategy_name,\n",
    "                'CV_Score': result['best_cv_score'],\n",
    "                'Test_Score': result['test_score'],\n",
    "                'Time': result['optimization_time'],\n",
    "                'Iterations': result['n_iterations']\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Test score comparison\n",
    "    sns.barplot(data=comparison_df, x='Model', y='Test_Score', hue='Strategy', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Test Score by Optimization Strategy')\n",
    "    axes[0, 0].set_ylabel('Test Accuracy')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].legend(title='Strategy')\n",
    "    \n",
    "    # 2. Optimization time comparison\n",
    "    sns.barplot(data=comparison_df, x='Model', y='Time', hue='Strategy', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Optimization Time by Strategy')\n",
    "    axes[0, 1].set_ylabel('Time (seconds)')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].legend(title='Strategy')\n",
    "    \n",
    "    # 3. Efficiency analysis (Performance vs Time)\n",
    "    for strategy in comparison_df['Strategy'].unique():\n",
    "        strategy_data = comparison_df[comparison_df['Strategy'] == strategy]\n",
    "        axes[1, 0].scatter(strategy_data['Time'], strategy_data['Test_Score'], \n",
    "                          label=strategy, s=100, alpha=0.7)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Optimization Time (seconds)')\n",
    "    axes[1, 0].set_ylabel('Test Accuracy')\n",
    "    axes[1, 0].set_title('Optimization Efficiency (Performance vs Time)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Strategy ranking by model\n",
    "    strategy_rankings = []\n",
    "    for model in comparison_df['Model'].unique():\n",
    "        model_data = comparison_df[comparison_df['Model'] == model]\n",
    "        ranked = model_data.sort_values('Test_Score', ascending=False)\n",
    "        for i, (_, row) in enumerate(ranked.iterrows()):\n",
    "            strategy_rankings.append({\n",
    "                'Model': model,\n",
    "                'Strategy': row['Strategy'],\n",
    "                'Rank': i + 1\n",
    "            })\n",
    "    \n",
    "    ranking_df = pd.DataFrame(strategy_rankings)\n",
    "    ranking_pivot = ranking_df.pivot(index='Strategy', columns='Model', values='Rank')\n",
    "    \n",
    "    sns.heatmap(ranking_pivot, annot=True, cmap='RdYlGn_r', ax=axes[1, 1], \n",
    "                cbar_kws={'label': 'Rank (1=Best)'})\n",
    "    axes[1, 1].set_title('Strategy Rankings by Model')\n",
    "    axes[1, 1].set_xlabel('Model')\n",
    "    axes[1, 1].set_ylabel('Optimization Strategy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save optimization comparison figure\n",
    "    save_figure(fig, 'hyperparameter_optimization_comparison',\n",
    "               'Comprehensive comparison of hyperparameter optimization strategies', 'optimization')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\nðŸ“Š Hyperparameter Optimization Summary:\")\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"{'Model':<20} {'Strategy':<15} {'CV Score':<10} {'Test Score':<10} {'Time(s)':<10} {'Iters':<8}\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    for _, row in comparison_df.iterrows():\n",
    "        print(f\"{row['Model']:<20} {row['Strategy']:<15} {row['CV_Score']:<10.4f} \"\n",
    "              f\"{row['Test_Score']:<10.4f} {row['Time']:<10.2f} {row['Iterations']:<8}\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    # Best strategy analysis\n",
    "    print(\"\\nðŸ† Best Strategy Analysis:\")\n",
    "    for model in comparison_df['Model'].unique():\n",
    "        model_data = comparison_df[comparison_df['Model'] == model]\n",
    "        best_row = model_data.loc[model_data['Test_Score'].idxmax()]\n",
    "        print(f\"  {model}: {best_row['Strategy']} (Score: {best_row['Test_Score']:.4f}, Time: {best_row['Time']:.1f}s)\")\n",
    "\n",
    "print(\"\\nâœ¨ Optimization results visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3a005",
   "metadata": {},
   "source": [
    "## 4. Multi-Objective Optimization {#multi-objective}\n",
    "\n",
    "Exploring optimization techniques that balance multiple objectives like accuracy, speed, and model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54e129",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Multi-objective optimization\n",
    "print(\"ðŸŽ¯ Multi-Objective Optimization...\")\n",
    "\n",
    "class MultiObjectiveOptimizer:\n",
    "    \"\"\"Multi-objective hyperparameter optimization balancing performance and complexity.\"\"\"\n",
    "    \n",
    "    def __init__(self, weights={'accuracy': 0.6, 'speed': 0.2, 'simplicity': 0.2}):\n",
    "        self.weights = weights\n",
    "        self.results = []\n",
    "    \n",
    "    def evaluate_configuration(self, model, params, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Evaluate a model configuration on multiple objectives.\"\"\"\n",
    "        try:\n",
    "            # Set parameters\n",
    "            model.set_params(**params)\n",
    "            \n",
    "            # Measure training time\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train, y_train)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Measure prediction time\n",
    "            start_time = time.time()\n",
    "            predictions = model.predict(X_val)\n",
    "            prediction_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_val, predictions)\n",
    "            \n",
    "            # Speed score (inverse of total time, normalized)\n",
    "            total_time = training_time + prediction_time\n",
    "            speed_score = 1.0 / (1.0 + total_time)  # Bounded between 0 and 1\n",
    "            \n",
    "            # Simplicity score (based on model complexity)\n",
    "            simplicity_score = self._calculate_simplicity(model, params)\n",
    "            \n",
    "            # Combined score\n",
    "            combined_score = (\n",
    "                self.weights['accuracy'] * accuracy +\n",
    "                self.weights['speed'] * speed_score +\n",
    "                self.weights['simplicity'] * simplicity_score\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'params': params,\n",
    "                'accuracy': accuracy,\n",
    "                'speed_score': speed_score,\n",
    "                'simplicity_score': simplicity_score,\n",
    "                'combined_score': combined_score,\n",
    "                'training_time': training_time,\n",
    "                'prediction_time': prediction_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def _calculate_simplicity(self, model, params):\n",
    "        \"\"\"Calculate simplicity score based on model parameters.\"\"\"\n",
    "        if hasattr(model, 'n_estimators'):\n",
    "            # For ensemble methods, fewer estimators = more simple\n",
    "            n_est = params.get('n_estimators', getattr(model, 'n_estimators', 100))\n",
    "            return max(0, 1.0 - (n_est - 50) / 150)  # Normalize to 0-1\n",
    "        \n",
    "        elif hasattr(model, 'C'):\n",
    "            # For SVM, smaller C = more simple\n",
    "            C = params.get('C', getattr(model, 'C', 1.0))\n",
    "            return max(0, 1.0 - np.log10(C + 1) / 2)  # Normalize to 0-1\n",
    "        \n",
    "        elif hasattr(model, 'hidden_layer_sizes'):\n",
    "            # For neural networks, fewer/smaller layers = more simple\n",
    "            layers = params.get('hidden_layer_sizes', getattr(model, 'hidden_layer_sizes', (100,)))\n",
    "            if isinstance(layers, tuple):\n",
    "                total_neurons = sum(layers)\n",
    "                return max(0, 1.0 - (total_neurons - 50) / 500)  # Normalize to 0-1\n",
    "        \n",
    "        return 0.5  # Default moderate simplicity\n",
    "    \n",
    "    def optimize(self, model_class, param_distributions, X_train, y_train, X_val, y_val, n_trials=30):\n",
    "        \"\"\"Perform multi-objective optimization.\"\"\"\n",
    "        print(f\"  Running multi-objective optimization with {n_trials} trials...\")\n",
    "        \n",
    "        self.results = []\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            # Sample parameters\n",
    "            params = {}\n",
    "            for param_name, distribution in param_distributions.items():\n",
    "                if hasattr(distribution, 'rvs'):\n",
    "                    params[param_name] = distribution.rvs()\n",
    "                elif isinstance(distribution, list):\n",
    "                    params[param_name] = np.random.choice(distribution)\n",
    "                else:\n",
    "                    params[param_name] = distribution\n",
    "            \n",
    "            # Evaluate configuration\n",
    "            model = model_class(random_state=42)\n",
    "            result = self.evaluate_configuration(model, params, X_train, y_train, X_val, y_val)\n",
    "            \n",
    "            if result:\n",
    "                self.results.append(result)\n",
    "        \n",
    "        # Sort by combined score\n",
    "        self.results.sort(key=lambda x: x['combined_score'], reverse=True)\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "# Test multi-objective optimization\n",
    "X_train, X_test, y_train, y_test = datasets['Complex']\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "multi_obj_results = {}\n",
    "\n",
    "# Define optimization tasks\n",
    "multi_obj_tasks = {\n",
    "    'Random Forest': {\n",
    "        'model_class': RandomForestClassifier,\n",
    "        'param_distributions': {\n",
    "            'n_estimators': randint(10, 100),\n",
    "            'max_depth': randint(3, 15),\n",
    "            'min_samples_split': randint(2, 10)\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model_class': GradientBoostingClassifier,\n",
    "        'param_distributions': {\n",
    "            'n_estimators': randint(10, 80),\n",
    "            'learning_rate': uniform(0.05, 0.25),\n",
    "            'max_depth': randint(3, 8)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test different weight configurations\n",
    "weight_configs = {\n",
    "    'Balanced': {'accuracy': 0.5, 'speed': 0.25, 'simplicity': 0.25},\n",
    "    'Accuracy-Focused': {'accuracy': 0.8, 'speed': 0.1, 'simplicity': 0.1},\n",
    "    'Speed-Focused': {'accuracy': 0.4, 'speed': 0.5, 'simplicity': 0.1},\n",
    "    'Simplicity-Focused': {'accuracy': 0.4, 'speed': 0.1, 'simplicity': 0.5}\n",
    "}\n",
    "\n",
    "for model_name, task in multi_obj_tasks.items():\n",
    "    print(f\"\\n--- Multi-Objective Optimization: {model_name} ---\")\n",
    "    multi_obj_results[model_name] = {}\n",
    "    \n",
    "    for weight_name, weights in weight_configs.items():\n",
    "        print(f\"\\n  Weight Configuration: {weight_name}\")\n",
    "        \n",
    "        optimizer = MultiObjectiveOptimizer(weights=weights)\n",
    "        results = optimizer.optimize(\n",
    "            task['model_class'], \n",
    "            task['param_distributions'],\n",
    "            X_train_sub, y_train_sub, X_val, y_val,\n",
    "            n_trials=20\n",
    "        )\n",
    "        \n",
    "        if results:\n",
    "            best_result = results[0]\n",
    "            print(f\"    Best Combined Score: {best_result['combined_score']:.4f}\")\n",
    "            print(f\"    Accuracy: {best_result['accuracy']:.4f}\")\n",
    "            print(f\"    Speed Score: {best_result['speed_score']:.4f}\")\n",
    "            print(f\"    Simplicity Score: {best_result['simplicity_score']:.4f}\")\n",
    "            print(f\"    Best Params: {best_result['params']}\")\n",
    "            \n",
    "            multi_obj_results[model_name][weight_name] = {\n",
    "                'best_result': best_result,\n",
    "                'all_results': results[:5]  # Top 5 results\n",
    "            }\n",
    "\n",
    "print(\"\\nâœ¨ Multi-objective optimization complete!\")\n",
    "\n",
    "# Save multi-objective results\n",
    "multi_obj_summary = {}\n",
    "for model_name, weight_configs in multi_obj_results.items():\n",
    "    multi_obj_summary[model_name] = {}\n",
    "    for weight_name, results in weight_configs.items():\n",
    "        best = results['best_result']\n",
    "        multi_obj_summary[model_name][weight_name] = {\n",
    "            'combined_score': best['combined_score'],\n",
    "            'accuracy': best['accuracy'],\n",
    "            'speed_score': best['speed_score'],\n",
    "            'simplicity_score': best['simplicity_score'],\n",
    "            'training_time': best['training_time'],\n",
    "            'best_params': best['params']\n",
    "        }\n",
    "\n",
    "save_optimization_experiment('multi_objective_optimization', multi_obj_summary,\n",
    "                           'Results from multi-objective hyperparameter optimization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa66ea",
   "metadata": {},
   "source": [
    "### Multi-Objective Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f0e79e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize multi-objective optimization results\n",
    "print(\"ðŸ“Š Visualizing Multi-Objective Optimization Results...\")\n",
    "\n",
    "if multi_obj_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    plot_data = []\n",
    "    for model_name, weight_configs in multi_obj_results.items():\n",
    "        for weight_name, results in weight_configs.items():\n",
    "            best = results['best_result']\n",
    "            plot_data.append({\n",
    "                'Model': model_name,\n",
    "                'Weight_Config': weight_name,\n",
    "                'Combined_Score': best['combined_score'],\n",
    "                'Accuracy': best['accuracy'],\n",
    "                'Speed_Score': best['speed_score'],\n",
    "                'Simplicity_Score': best['simplicity_score']\n",
    "            })\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # 1. Combined scores by weight configuration\n",
    "    sns.barplot(data=plot_df, x='Weight_Config', y='Combined_Score', hue='Model', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Combined Scores by Weight Configuration')\n",
    "    axes[0, 0].set_ylabel('Combined Score')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].legend(title='Model')\n",
    "    \n",
    "    # 2. Accuracy vs Speed trade-off\n",
    "    for model in plot_df['Model'].unique():\n",
    "        model_data = plot_df[plot_df['Model'] == model]\n",
    "        axes[0, 1].scatter(model_data['Speed_Score'], model_data['Accuracy'], \n",
    "                          label=model, s=100, alpha=0.7)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Speed Score')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Accuracy vs Speed Trade-off')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Radar chart for first model\n",
    "    if multi_obj_results:\n",
    "        first_model = list(multi_obj_results.keys())[0]\n",
    "        weight_configs = list(multi_obj_results[first_model].keys())\n",
    "        \n",
    "        # Prepare radar chart data\n",
    "        categories = ['Accuracy', 'Speed', 'Simplicity']\n",
    "        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "        \n",
    "        ax = plt.subplot(2, 2, 3, projection='polar')\n",
    "        \n",
    "        for i, config_name in enumerate(weight_configs[:3]):  # Limit to 3 configs for clarity\n",
    "            config_data = multi_obj_results[first_model][config_name]['best_result']\n",
    "            values = [\n",
    "                config_data['accuracy'],\n",
    "                config_data['speed_score'],\n",
    "                config_data['simplicity_score']\n",
    "            ]\n",
    "            values += values[:1]  # Complete the circle\n",
    "            \n",
    "            ax.plot(angles, values, 'o-', linewidth=2, label=config_name)\n",
    "            ax.fill(angles, values, alpha=0.25)\n",
    "        \n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(categories)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title(f'Multi-Objective Performance - {first_model}')\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    \n",
    "    # 4. Pareto frontier analysis\n",
    "    if len(plot_df) > 0:\n",
    "        # Use accuracy and speed for Pareto analysis\n",
    "        accuracy_vals = plot_df['Accuracy'].values\n",
    "        speed_vals = plot_df['Speed_Score'].values\n",
    "        \n",
    "        # Find Pareto frontier\n",
    "        pareto_points = []\n",
    "        for i in range(len(accuracy_vals)):\n",
    "            is_pareto = True\n",
    "            for j in range(len(accuracy_vals)):\n",
    "                if i != j:\n",
    "                    if (accuracy_vals[j] >= accuracy_vals[i] and speed_vals[j] >= speed_vals[i] and\n",
    "                        (accuracy_vals[j] > accuracy_vals[i] or speed_vals[j] > speed_vals[i])):\n",
    "                        is_pareto = False\n",
    "                        break\n",
    "            if is_pareto:\n",
    "                pareto_points.append(i)\n",
    "        \n",
    "        # Plot all points\n",
    "        axes[1, 1].scatter(speed_vals, accuracy_vals, alpha=0.6, s=50, color='lightblue')\n",
    "        \n",
    "        # Highlight Pareto frontier\n",
    "        if pareto_points:\n",
    "            pareto_acc = [accuracy_vals[i] for i in pareto_points]\n",
    "            pareto_speed = [speed_vals[i] for i in pareto_points]\n",
    "            axes[1, 1].scatter(pareto_speed, pareto_acc, color='red', s=100, \n",
    "                             label='Pareto Frontier', alpha=0.8)\n",
    "        \n",
    "        axes[1, 1].set_xlabel('Speed Score')\n",
    "        axes[1, 1].set_ylabel('Accuracy')\n",
    "        axes[1, 1].set_title('Pareto Frontier Analysis')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save multi-objective visualization\n",
    "    save_figure(fig, 'multi_objective_optimization_results',\n",
    "               'Multi-objective optimization results showing trade-offs between objectives', 'multi_objective')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\nðŸ“Š Multi-Objective Optimization Summary:\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'Model':<15} {'Config':<18} {'Combined':<10} {'Accuracy':<10} {'Speed':<10} {'Simplicity':<10}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for _, row in plot_df.iterrows():\n",
    "        print(f\"{row['Model']:<15} {row['Weight_Config']:<18} {row['Combined_Score']:<10.4f} \"\n",
    "              f\"{row['Accuracy']:<10.4f} {row['Speed_Score']:<10.4f} {row['Simplicity_Score']:<10.4f}\")\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nâœ¨ Multi-objective optimization visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3d585",
   "metadata": {},
   "source": [
    "## 5. Bayesian Optimization {#bayesian}\n",
    "\n",
    "Implementing Bayesian optimization for efficient hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a126646",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Bayesian optimization implementation\n",
    "print(\"ðŸ”¬ Bayesian Optimization...\")\n",
    "\n",
    "class SimpleBayesianOptimizer:\n",
    "    \"\"\"Simplified Bayesian optimization using Gaussian Process.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_initial_points=5, n_calls=20):\n",
    "        self.n_initial_points = n_initial_points\n",
    "        self.n_calls = n_calls\n",
    "        self.X_observed = []\n",
    "        self.y_observed = []\n",
    "        self.gp = None\n",
    "    \n",
    "    def _sample_parameters(self, param_space):\n",
    "        \"\"\"Sample parameters from the parameter space.\"\"\"\n",
    "        params = {}\n",
    "        for param_name, param_range in param_space.items():\n",
    "            if isinstance(param_range, tuple) and len(param_range) == 2:\n",
    "                # Continuous parameter\n",
    "                params[param_name] = np.random.uniform(param_range[0], param_range[1])\n",
    "            elif isinstance(param_range, list):\n",
    "                # Categorical parameter\n",
    "                params[param_name] = np.random.choice(param_range)\n",
    "            else:\n",
    "                params[param_name] = param_range\n",
    "        return params\n",
    "    \n",
    "    def _params_to_vector(self, params, param_space):\n",
    "        \"\"\"Convert parameter dictionary to vector for GP.\"\"\"\n",
    "        vector = []\n",
    "        for param_name in sorted(param_space.keys()):\n",
    "            if param_name in params:\n",
    "                value = params[param_name]\n",
    "                param_range = param_space[param_name]\n",
    "                \n",
    "                if isinstance(param_range, tuple):\n",
    "                    # Normalize continuous parameters to [0, 1]\n",
    "                    normalized = (value - param_range[0]) / (param_range[1] - param_range[0])\n",
    "                    vector.append(normalized)\n",
    "                elif isinstance(param_range, list):\n",
    "                    # One-hot encode categorical parameters\n",
    "                    one_hot = [1 if v == value else 0 for v in param_range]\n",
    "                    vector.extend(one_hot)\n",
    "        return np.array(vector)\n",
    "    \n",
    "    def _vector_to_params(self, vector, param_space):\n",
    "        \"\"\"Convert vector back to parameter dictionary.\"\"\"\n",
    "        params = {}\n",
    "        idx = 0\n",
    "        \n",
    "        for param_name in sorted(param_space.keys()):\n",
    "            param_range = param_space[param_name]\n",
    "            \n",
    "            if isinstance(param_range, tuple):\n",
    "                # Denormalize continuous parameters\n",
    "                normalized_value = vector[idx]\n",
    "                value = param_range[0] + normalized_value * (param_range[1] - param_range[0])\n",
    "                \n",
    "                # Handle integer parameters\n",
    "                if param_name in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf']:\n",
    "                    value = int(round(value))\n",
    "                \n",
    "                params[param_name] = value\n",
    "                idx += 1\n",
    "            elif isinstance(param_range, list):\n",
    "                # Decode categorical parameters\n",
    "                one_hot_length = len(param_range)\n",
    "                one_hot = vector[idx:idx + one_hot_length]\n",
    "                selected_idx = np.argmax(one_hot)\n",
    "                params[param_name] = param_range[selected_idx]\n",
    "                idx += one_hot_length\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def _acquisition_function(self, X_candidate, gp):\n",
    "        \"\"\"Expected Improvement acquisition function.\"\"\"\n",
    "        try:\n",
    "            mu, sigma = gp.predict(X_candidate.reshape(1, -1), return_std=True)\n",
    "            \n",
    "            if len(self.y_observed) == 0:\n",
    "                return 0\n",
    "            \n",
    "            # Expected Improvement\n",
    "            f_best = max(self.y_observed)\n",
    "            xi = 0.01  # Exploration parameter\n",
    "            \n",
    "            if sigma[0] > 0:\n",
    "                z = (mu[0] - f_best - xi) / sigma[0]\n",
    "                ei = (mu[0] - f_best - xi) * norm.cdf(z) + sigma[0] * norm.pdf(z)\n",
    "                return ei\n",
    "            else:\n",
    "                return 0\n",
    "                \n",
    "        except Exception:\n",
    "            return 0\n",
    "    \n",
    "    def optimize(self, objective_function, param_space):\n",
    "        \"\"\"Perform Bayesian optimization.\"\"\"\n",
    "        from scipy.stats import norm\n",
    "        from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "        from sklearn.gaussian_process.kernels import Matern\n",
    "        \n",
    "        print(f\"  Starting Bayesian optimization with {self.n_calls} iterations...\")\n",
    "        \n",
    "        # Initial random exploration\n",
    "        for i in range(self.n_initial_points):\n",
    "            params = self._sample_parameters(param_space)\n",
    "            score = objective_function(params)\n",
    "            \n",
    "            if score is not None:\n",
    "                X_vec = self._params_to_vector(params, param_space)\n",
    "                self.X_observed.append(X_vec)\n",
    "                self.y_observed.append(score)\n",
    "                print(f\"    Initial {i+1}/{self.n_initial_points}: Score = {score:.4f}\")\n",
    "        \n",
    "        if len(self.X_observed) == 0:\n",
    "            print(\"    No valid initial points found!\")\n",
    "            return None, []\n",
    "        \n",
    "        # Convert to arrays\n",
    "        X_observed = np.array(self.X_observed)\n",
    "        y_observed = np.array(self.y_observed)\n",
    "        \n",
    "        # Bayesian optimization loop\n",
    "        for iteration in range(self.n_calls - self.n_initial_points):\n",
    "            try:\n",
    "                # Fit Gaussian Process\n",
    "                kernel = Matern(length_scale=1.0, nu=2.5)\n",
    "                self.gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, random_state=42)\n",
    "                self.gp.fit(X_observed, y_observed)\n",
    "                \n",
    "                # Find next point to evaluate using acquisition function\n",
    "                best_acquisition = -np.inf\n",
    "                best_candidate = None\n",
    "                \n",
    "                # Sample candidates and evaluate acquisition function\n",
    "                for _ in range(100):  # Sample 100 candidates\n",
    "                    candidate_params = self._sample_parameters(param_space)\n",
    "                    candidate_vec = self._params_to_vector(candidate_params, param_space)\n",
    "                    \n",
    "                    acquisition_val = self._acquisition_function(candidate_vec, self.gp)\n",
    "                    \n",
    "                    if acquisition_val > best_acquisition:\n",
    "                        best_acquisition = acquisition_val\n",
    "                        best_candidate = candidate_vec\n",
    "                \n",
    "                if best_candidate is not None:\n",
    "                    # Evaluate the best candidate\n",
    "                    best_params = self._vector_to_params(best_candidate, param_space)\n",
    "                    score = objective_function(best_params)\n",
    "                    \n",
    "                    if score is not None:\n",
    "                        self.X_observed.append(best_candidate)\n",
    "                        self.y_observed.append(score)\n",
    "                        X_observed = np.array(self.X_observed)\n",
    "                        y_observed = np.array(self.y_observed)\n",
    "                        \n",
    "                        print(f\"    Iteration {iteration+1}: Score = {score:.4f}, Acquisition = {best_acquisition:.6f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Iteration {iteration+1} failed: {str(e)}\")\n",
    "                # Fallback to random sampling\n",
    "                params = self._sample_parameters(param_space)\n",
    "                score = objective_function(params)\n",
    "                if score is not None:\n",
    "                    X_vec = self._params_to_vector(params, param_space)\n",
    "                    self.X_observed.append(X_vec)\n",
    "                    self.y_observed.append(score)\n",
    "                    X_observed = np.array(self.X_observed)\n",
    "                    y_observed = np.array(self.y_observed)\n",
    "        \n",
    "        # Find best result\n",
    "        if len(self.y_observed) > 0:\n",
    "            best_idx = np.argmax(self.y_observed)\n",
    "            best_score = self.y_observed[best_idx]\n",
    "            best_params = self._vector_to_params(self.X_observed[best_idx], param_space)\n",
    "            \n",
    "            return best_params, best_score\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "# Test Bayesian optimization\n",
    "print(\"\\n--- Testing Bayesian Optimization ---\")\n",
    "\n",
    "# Define objective function\n",
    "def rf_objective(params):\n",
    "    \"\"\"Objective function for Random Forest optimization.\"\"\"\n",
    "    try:\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=params['n_estimators'],\n",
    "            max_depth=params.get('max_depth'),\n",
    "            min_samples_split=params['min_samples_split'],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Cross-validation score\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
    "        return scores.mean()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Define parameter space for Bayesian optimization\n",
    "bayesian_param_space = {\n",
    "    'n_estimators': (10, 100),\n",
    "    'max_depth': (3, 20),\n",
    "    'min_samples_split': (2, 20)\n",
    "}\n",
    "\n",
    "# Run Bayesian optimization\n",
    "bayesian_optimizer = SimpleBayesianOptimizer(n_initial_points=5, n_calls=15)\n",
    "best_params, best_score = bayesian_optimizer.optimize(rf_objective, bayesian_param_space)\n",
    "\n",
    "if best_params:\n",
    "    print(f\"\\nðŸ† Bayesian Optimization Results:\")\n",
    "    print(f\"  Best Score: {best_score:.4f}\")\n",
    "    print(f\"  Best Parameters: {best_params}\")\n",
    "    \n",
    "    # Test final model\n",
    "    final_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "    final_model.fit(X_train, y_train)\n",
    "    test_score = final_model.score(X_test, y_test)\n",
    "    print(f\"  Test Score: {test_score:.4f}\")\n",
    "    \n",
    "    # Save Bayesian optimized model\n",
    "    bayesian_results = {\n",
    "        'best_cv_score': best_score,\n",
    "        'test_score': test_score,\n",
    "        'optimization_method': 'bayesian',\n",
    "        'n_iterations': len(bayesian_optimizer.y_observed),\n",
    "        'convergence_history': bayesian_optimizer.y_observed\n",
    "    }\n",
    "    \n",
    "    save_optimized_model(final_model, 'bayesian_random_forest',\n",
    "                       'Random Forest optimized using Bayesian optimization', bayesian_results)\n",
    "    \n",
    "    # Save optimization history\n",
    "    save_optimization_experiment('bayesian_optimization_history', {\n",
    "        'parameter_space': bayesian_param_space,\n",
    "        'optimization_history': [\n",
    "            {'iteration': i, 'score': score} \n",
    "            for i, score in enumerate(bayesian_optimizer.y_observed)\n",
    "        ],\n",
    "        'best_params': best_params,\n",
    "        'best_score': best_score\n",
    "    }, 'Complete history of Bayesian optimization process')\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Bayesian optimization failed\")\n",
    "\n",
    "print(\"\\nâœ¨ Bayesian optimization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd553b0c",
   "metadata": {},
   "source": [
    "## 6. Population-Based Training {#population}\n",
    "\n",
    "Implementing population-based training for parallel hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d9f684",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Population-based training implementation\n",
    "print(\"ðŸ‘¥ Population-Based Training...\")\n",
    "\n",
    "class PopulationBasedTrainer:\n",
    "    \"\"\"Population-based training for hyperparameter optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, population_size=6, generations=5, mutation_rate=0.3):\n",
    "        self.population_size = population_size\n",
    "        self.generations = generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.population = []\n",
    "        self.fitness_history = []\n",
    "    \n",
    "    def _initialize_population(self, param_space):\n",
    "        \"\"\"Initialize random population.\"\"\"\n",
    "        population = []\n",
    "        \n",
    "        for _ in range(self.population_size):\n",
    "            individual = {}\n",
    "            for param_name, param_range in param_space.items():\n",
    "                if isinstance(param_range, tuple):\n",
    "                    # Continuous parameter\n",
    "                    value = np.random.uniform(param_range[0], param_range[1])\n",
    "                    if param_name in ['n_estimators', 'max_depth', 'min_samples_split']:\n",
    "                        value = int(round(value))\n",
    "                    individual[param_name] = value\n",
    "                elif isinstance(param_range, list):\n",
    "                    # Categorical parameter\n",
    "                    individual[param_name] = np.random.choice(param_range)\n",
    "            \n",
    "            population.append(individual)\n",
    "        \n",
    "        return population\n",
    "    \n",
    "    def _evaluate_individual(self, individual, model_class, X_train, y_train):\n",
    "        \"\"\"Evaluate fitness of an individual.\"\"\"\n",
    "        try:\n",
    "            model = model_class(**individual, random_state=42)\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
    "            return scores.mean()\n",
    "        except Exception as e:\n",
    "            return 0.0  # Poor fitness for invalid configurations\n",
    "    \n",
    "    def _mutate_individual(self, individual, param_space):\n",
    "        \"\"\"Mutate an individual by randomly changing some parameters.\"\"\"\n",
    "        mutated = individual.copy()\n",
    "        \n",
    "        for param_name, param_range in param_space.items():\n",
    "            if np.random.random() < self.mutation_rate:\n",
    "                if isinstance(param_range, tuple):\n",
    "                    # Continuous parameter - add Gaussian noise\n",
    "                    current_value = mutated[param_name]\n",
    "                    noise_std = (param_range[1] - param_range[0]) * 0.1\n",
    "                    new_value = current_value + np.random.normal(0, noise_std)\n",
    "                    new_value = np.clip(new_value, param_range[0], param_range[1])\n",
    "                    \n",
    "                    if param_name in ['n_estimators', 'max_depth', 'min_samples_split']:\n",
    "                        new_value = int(round(new_value))\n",
    "                    \n",
    "                    mutated[param_name] = new_value\n",
    "                elif isinstance(param_range, list):\n",
    "                    # Categorical parameter - random choice\n",
    "                    mutated[param_name] = np.random.choice(param_range)\n",
    "        \n",
    "        return mutated\n",
    "    \n",
    "    def _crossover(self, parent1, parent2):\n",
    "        \"\"\"Create offspring through crossover.\"\"\"\n",
    "        child = {}\n",
    "        for param_name in parent1.keys():\n",
    "            # Random selection from either parent\n",
    "            child[param_name] = parent1[param_name] if np.random.random() < 0.5 else parent2[param_name]\n",
    "        return child\n",
    "    \n",
    "    def optimize(self, model_class, param_space, X_train, y_train):\n",
    "        \"\"\"Run population-based training.\"\"\"\n",
    "        print(f\"  Initializing population of {self.population_size} individuals...\")\n",
    "        \n",
    "        # Initialize population\n",
    "        self.population = self._initialize_population(param_space)\n",
    "        generation_fitness = []\n",
    "        \n",
    "        for generation in range(self.generations):\n",
    "            print(f\"\\n  Generation {generation + 1}/{self.generations}\")\n",
    "            \n",
    "            # Evaluate fitness for each individual\n",
    "            fitness_scores = []\n",
    "            for i, individual in enumerate(self.population):\n",
    "                fitness = self._evaluate_individual(individual, model_class, X_train, y_train)\n",
    "                fitness_scores.append(fitness)\n",
    "                print(f\"    Individual {i+1}: Fitness = {fitness:.4f}\")\n",
    "            \n",
    "            generation_fitness.append(fitness_scores)\n",
    "            self.fitness_history.extend(fitness_scores)\n",
    "            \n",
    "            # Selection: keep top 50% of population\n",
    "            population_with_fitness = list(zip(self.population, fitness_scores))\n",
    "            population_with_fitness.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            survivors = [individual for individual, _ in population_with_fitness[:self.population_size//2]]\n",
    "            \n",
    "            # Create new generation\n",
    "            new_population = survivors.copy()\n",
    "            \n",
    "            # Fill remaining spots with offspring and mutations\n",
    "            while len(new_population) < self.population_size:\n",
    "                if len(survivors) >= 2:\n",
    "                    # Crossover\n",
    "                    parent1, parent2 = np.random.choice(survivors, 2, replace=False)\n",
    "                    child = self._crossover(parent1, parent2)\n",
    "                    \n",
    "                    # Mutate child\n",
    "                    child = self._mutate_individual(child, param_space)\n",
    "                    new_population.append(child)\n",
    "                else:\n",
    "                    # Fallback: add random individual\n",
    "                    new_individual = self._initialize_population(param_space)[0]\n",
    "                    new_population.append(new_individual)\n",
    "            \n",
    "            self.population = new_population\n",
    "            \n",
    "            # Print generation statistics\n",
    "            best_fitness = max(fitness_scores)\n",
    "            avg_fitness = np.mean(fitness_scores)\n",
    "            print(f\"    Generation Stats: Best = {best_fitness:.4f}, Avg = {avg_fitness:.4f}\")\n",
    "        \n",
    "        # Return best individual\n",
    "        final_fitness_scores = []\n",
    "        for individual in self.population:\n",
    "            fitness = self._evaluate_individual(individual, model_class, X_train, y_train)\n",
    "            final_fitness_scores.append(fitness)\n",
    "        \n",
    "        best_idx = np.argmax(final_fitness_scores)\n",
    "        best_individual = self.population[best_idx]\n",
    "        best_fitness = final_fitness_scores[best_idx]\n",
    "        \n",
    "        return best_individual, best_fitness, generation_fitness\n",
    "\n",
    "# Test Population-Based Training\n",
    "print(\"\\n--- Testing Population-Based Training ---\")\n",
    "\n",
    "# Define parameter space for population-based training\n",
    "pbt_param_space = {\n",
    "    'n_estimators': (20, 100),\n",
    "    'max_depth': (3, 15),\n",
    "    'min_samples_split': (2, 10),\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Run Population-Based Training\n",
    "pbt_trainer = PopulationBasedTrainer(population_size=6, generations=4, mutation_rate=0.3)\n",
    "best_individual, best_fitness, generation_fitness = pbt_trainer.optimize(\n",
    "    RandomForestClassifier, pbt_param_space, X_train, y_train\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ† Population-Based Training Results:\")\n",
    "print(f\"  Best Fitness: {best_fitness:.4f}\")\n",
    "print(f\"  Best Individual: {best_individual}\")\n",
    "\n",
    "# Test final model\n",
    "final_model = RandomForestClassifier(**best_individual, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "test_score = final_model.score(X_test, y_test)\n",
    "print(f\"  Test Score: {test_score:.4f}\")\n",
    "\n",
    "# Save PBT results\n",
    "pbt_results = {\n",
    "    'best_fitness': best_fitness,\n",
    "    'test_score': test_score,\n",
    "    'optimization_method': 'population_based_training',\n",
    "    'population_size': pbt_trainer.population_size,\n",
    "    'generations': pbt_trainer.generations,\n",
    "    'fitness_history': pbt_trainer.fitness_history,\n",
    "    'generation_fitness': generation_fitness\n",
    "}\n",
    "\n",
    "save_optimized_model(final_model, 'pbt_random_forest',\n",
    "                   'Random Forest optimized using Population-Based Training', pbt_results)\n",
    "\n",
    "print(\"\\nâœ¨ Population-based training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dba702",
   "metadata": {},
   "source": [
    "### Population-Based Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639493e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize population-based training evolution\n",
    "print(\"ðŸ“Š Visualizing Population-Based Training Evolution...\")\n",
    "\n",
    "if 'generation_fitness' in locals() and generation_fitness:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Fitness evolution over generations\n",
    "    generations = range(1, len(generation_fitness) + 1)\n",
    "    best_fitness_per_gen = [max(gen_fitness) for gen_fitness in generation_fitness]\n",
    "    avg_fitness_per_gen = [np.mean(gen_fitness) for gen_fitness in generation_fitness]\n",
    "    worst_fitness_per_gen = [min(gen_fitness) for gen_fitness in generation_fitness]\n",
    "    \n",
    "    axes[0, 0].plot(generations, best_fitness_per_gen, 'g-o', label='Best', linewidth=2)\n",
    "    axes[0, 0].plot(generations, avg_fitness_per_gen, 'b-s', label='Average', linewidth=2)\n",
    "    axes[0, 0].plot(generations, worst_fitness_per_gen, 'r-^', label='Worst', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Generation')\n",
    "    axes[0, 0].set_ylabel('Fitness (Accuracy)')\n",
    "    axes[0, 0].set_title('Fitness Evolution Over Generations')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Population diversity over generations\n",
    "    diversity_per_gen = []\n",
    "    for gen_fitness in generation_fitness:\n",
    "        diversity = np.std(gen_fitness) if len(gen_fitness) > 1 else 0\n",
    "        diversity_per_gen.append(diversity)\n",
    "    \n",
    "    axes[0, 1].plot(generations, diversity_per_gen, 'purple', marker='o', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Generation')\n",
    "    axes[0, 1].set_ylabel('Fitness Diversity (Std Dev)')\n",
    "    axes[0, 1].set_title('Population Diversity Over Generations')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Fitness distribution in final generation\n",
    "    final_generation_fitness = generation_fitness[-1]\n",
    "    axes[1, 0].hist(final_generation_fitness, bins=min(10, len(final_generation_fitness)), \n",
    "                   alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1, 0].axvline(np.mean(final_generation_fitness), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(final_generation_fitness):.3f}')\n",
    "    axes[1, 0].axvline(max(final_generation_fitness), color='green', linestyle='--', \n",
    "                      label=f'Best: {max(final_generation_fitness):.3f}')\n",
    "    axes[1, 0].set_xlabel('Fitness (Accuracy)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Final Generation Fitness Distribution')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Convergence analysis\n",
    "    # Calculate improvement rate\n",
    "    improvement_rates = []\n",
    "    for i in range(1, len(best_fitness_per_gen)):\n",
    "        improvement = best_fitness_per_gen[i] - best_fitness_per_gen[i-1]\n",
    "        improvement_rates.append(improvement)\n",
    "    \n",
    "    if improvement_rates:\n",
    "        axes[1, 1].bar(range(2, len(generations) + 1), improvement_rates, \n",
    "                      alpha=0.7, color='orange')\n",
    "        axes[1, 1].set_xlabel('Generation')\n",
    "        axes[1, 1].set_ylabel('Fitness Improvement')\n",
    "        axes[1, 1].set_title('Fitness Improvement per Generation')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add zero line\n",
    "        axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save PBT evolution visualization\n",
    "    save_figure(fig, 'population_based_training_evolution',\n",
    "               'Evolution of population-based training showing fitness progression', 'pbt')\n",
    "    plt.show()\n",
    "    \n",
    "    # PBT Summary\n",
    "    print(\"\\nðŸ“Š Population-Based Training Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Initial Best Fitness: {best_fitness_per_gen[0]:.4f}\")\n",
    "    print(f\"Final Best Fitness: {best_fitness_per_gen[-1]:.4f}\")\n",
    "    print(f\"Total Improvement: {best_fitness_per_gen[-1] - best_fitness_per_gen[0]:.4f}\")\n",
    "    print(f\"Average Improvement per Generation: {np.mean(improvement_rates):.4f}\")\n",
    "    print(f\"Final Population Diversity: {diversity_per_gen[-1]:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nâœ¨ Population-based training visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a241c1",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation Strategies {#cross-validation}\n",
    "\n",
    "Exploring advanced cross-validation techniques for robust model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58667656",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Advanced cross-validation strategies\n",
    "print(\"ðŸ”„ Advanced Cross-Validation Strategies...\")\n",
    "\n",
    "class AdvancedCrossValidator:\n",
    "    \"\"\"Advanced cross-validation techniques for robust model evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cv_strategies = {\n",
    "            'Stratified K-Fold': StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "            'Group K-Fold': None,  # Will be set based on groups\n",
    "            'Time Series Split': TimeSeriesSplit(n_splits=5),\n",
    "            'Leave-One-Group-Out': None  # Will be set based on groups\n",
    "        }\n",
    "        self.results = {}\n",
    "    \n",
    "    def evaluate_cv_strategies(self, models, X, y, groups=None):\n",
    "        \"\"\"Evaluate different cross-validation strategies.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\n  Evaluating {model_name}...\")\n",
    "            results[model_name] = {}\n",
    "            \n",
    "            for cv_name, cv_strategy in self.cv_strategies.items():\n",
    "                try:\n",
    "                    # Skip group-based CV if no groups provided\n",
    "                    if cv_name in ['Group K-Fold', 'Leave-One-Group-Out'] and groups is None:\n",
    "                        print(f\"    {cv_name}: Skipped (no groups provided)\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Set up group-based CV strategies\n",
    "                    if cv_name == 'Group K-Fold' and groups is not None:\n",
    "                        cv_strategy = GroupKFold(n_splits=min(5, len(np.unique(groups))))\n",
    "                    elif cv_name == 'Leave-One-Group-Out' and groups is not None:\n",
    "                        from sklearn.model_selection import LeaveOneGroupOut\n",
    "                        cv_strategy = LeaveOneGroupOut()\n",
    "                    \n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    if groups is not None and cv_name in ['Group K-Fold', 'Leave-One-Group-Out']:\n",
    "                        scores = cross_val_score(model, X, y, cv=cv_strategy, groups=groups, \n",
    "                                               scoring='accuracy', n_jobs=-1)\n",
    "                    else:\n",
    "                        scores = cross_val_score(model, X, y, cv=cv_strategy, \n",
    "                                               scoring='accuracy', n_jobs=-1)\n",
    "                    \n",
    "                    evaluation_time = time.time() - start_time\n",
    "                    \n",
    "                    results[model_name][cv_name] = {\n",
    "                        'mean_score': scores.mean(),\n",
    "                        'std_score': scores.std(),\n",
    "                        'scores': scores.tolist(),\n",
    "                        'evaluation_time': evaluation_time,\n",
    "                        'n_splits': len(scores)\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"    {cv_name}: {scores.mean():.4f} Â± {scores.std():.4f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    {cv_name}: Failed ({str(e)})\")\n",
    "                    results[model_name][cv_name] = {'error': str(e)}\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def learning_curve_analysis(self, model, X, y, cv_strategy=None):\n",
    "        \"\"\"Analyze learning curves with different training set sizes.\"\"\"\n",
    "        if cv_strategy is None:\n",
    "            cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "        \n",
    "        train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "            model, X, y, train_sizes=train_sizes, cv=cv_strategy,\n",
    "            scoring='accuracy', n_jobs=-1, random_state=42\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'train_sizes': train_sizes_abs,\n",
    "            'train_scores_mean': train_scores.mean(axis=1),\n",
    "            'train_scores_std': train_scores.std(axis=1),\n",
    "            'val_scores_mean': val_scores.mean(axis=1),\n",
    "            'val_scores_std': val_scores.std(axis=1)\n",
    "        }\n",
    "    \n",
    "    def validation_curve_analysis(self, model, X, y, param_name, param_range, cv_strategy=None):\n",
    "        \"\"\"Analyze validation curves for hyperparameter sensitivity.\"\"\"\n",
    "        if cv_strategy is None:\n",
    "            cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        train_scores, val_scores = validation_curve(\n",
    "            model, X, y, param_name=param_name, param_range=param_range,\n",
    "            cv=cv_strategy, scoring='accuracy', n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'param_range': param_range,\n",
    "            'train_scores_mean': train_scores.mean(axis=1),\n",
    "            'train_scores_std': train_scores.std(axis=1),\n",
    "            'val_scores_mean': val_scores.mean(axis=1),\n",
    "            'val_scores_std': val_scores.std(axis=1)\n",
    "        }\n",
    "\n",
    "# Test advanced cross-validation\n",
    "print(\"\\n--- Testing Advanced Cross-Validation Strategies ---\")\n",
    "\n",
    "# Create synthetic groups for group-based CV\n",
    "np.random.seed(42)\n",
    "groups = np.random.randint(0, 10, size=len(X_train))\n",
    "\n",
    "# Models to evaluate\n",
    "cv_test_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Initialize advanced cross-validator\n",
    "cv_evaluator = AdvancedCrossValidator()\n",
    "\n",
    "# Evaluate different CV strategies\n",
    "cv_results = cv_evaluator.evaluate_cv_strategies(cv_test_models, X_train, y_train, groups)\n",
    "\n",
    "# Learning curve analysis\n",
    "print(f\"\\n  Analyzing learning curves...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "learning_curves = cv_evaluator.learning_curve_analysis(rf_model, X_train, y_train)\n",
    "\n",
    "# Validation curve analysis\n",
    "print(f\"  Analyzing validation curves...\")\n",
    "n_estimators_range = [10, 25, 50, 75, 100, 150, 200]\n",
    "validation_curves = cv_evaluator.validation_curve_analysis(\n",
    "    RandomForestClassifier(random_state=42), X_train, y_train,\n",
    "    'n_estimators', n_estimators_range\n",
    ")\n",
    "\n",
    "print(\"\\nâœ¨ Advanced cross-validation analysis complete!\")\n",
    "\n",
    "# Save cross-validation results\n",
    "cv_summary = {}\n",
    "for model_name, cv_strategies in cv_results.items():\n",
    "    cv_summary[model_name] = {}\n",
    "    for cv_name, results in cv_strategies.items():\n",
    "        if 'error' not in results:\n",
    "            cv_summary[model_name][cv_name] = {\n",
    "                'mean_score': results['mean_score'],\n",
    "                'std_score': results['std_score'],\n",
    "                'evaluation_time': results['evaluation_time'],\n",
    "                'n_splits': results['n_splits']\n",
    "            }\n",
    "\n",
    "save_optimization_experiment('cross_validation_strategies', {\n",
    "    'cv_strategy_comparison': cv_summary,\n",
    "    'learning_curve_analysis': learning_curves,\n",
    "    'validation_curve_analysis': validation_curves\n",
    "}, 'Comprehensive analysis of cross-validation strategies and learning curves')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9503c69",
   "metadata": {},
   "source": [
    "### Cross-Validation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cabfa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "print(\"ðŸ“Š Visualizing Cross-Validation Results...\")\n",
    "\n",
    "if cv_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. CV Strategy Comparison\n",
    "    cv_comparison_data = []\n",
    "    for model_name, cv_strategies in cv_results.items():\n",
    "        for cv_name, results in cv_strategies.items():\n",
    "            if 'error' not in results:\n",
    "                cv_comparison_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'CV_Strategy': cv_name,\n",
    "                    'Mean_Score': results['mean_score'],\n",
    "                    'Std_Score': results['std_score']\n",
    "                })\n",
    "    \n",
    "    if cv_comparison_data:\n",
    "        cv_df = pd.DataFrame(cv_comparison_data)\n",
    "        \n",
    "        # Group by CV strategy for better visualization\n",
    "        pivot_means = cv_df.pivot(index='CV_Strategy', columns='Model', values='Mean_Score')\n",
    "        pivot_stds = cv_df.pivot(index='CV_Strategy', columns='Model', values='Std_Score')\n",
    "        \n",
    "        # Bar plot with error bars\n",
    "        x_pos = np.arange(len(pivot_means.index))\n",
    "        width = 0.25\n",
    "        \n",
    "        for i, model in enumerate(pivot_means.columns):\n",
    "            means = pivot_means[model].values\n",
    "            stds = pivot_stds[model].values\n",
    "            axes[0, 0].bar(x_pos + i * width, means, width, yerr=stds, \n",
    "                          label=model, alpha=0.7, capsize=5)\n",
    "        \n",
    "        axes[0, 0].set_xlabel('Cross-Validation Strategy')\n",
    "        axes[0, 0].set_ylabel('Accuracy')\n",
    "        axes[0, 0].set_title('Cross-Validation Strategy Comparison')\n",
    "        axes[0, 0].set_xticks(x_pos + width)\n",
    "        axes[0, 0].set_xticklabels(pivot_means.index, rotation=45, ha='right')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Learning Curves\n",
    "    if 'learning_curves' in locals():\n",
    "        lc = learning_curves\n",
    "        axes[0, 1].plot(lc['train_sizes'], lc['train_scores_mean'], 'o-', color='blue', \n",
    "                       label='Training Score')\n",
    "        axes[0, 1].fill_between(lc['train_sizes'], \n",
    "                               lc['train_scores_mean'] - lc['train_scores_std'],\n",
    "                               lc['train_scores_mean'] + lc['train_scores_std'], \n",
    "                               alpha=0.3, color='blue')\n",
    "        \n",
    "        axes[0, 1].plot(lc['train_sizes'], lc['val_scores_mean'], 'o-', color='red', \n",
    "                       label='Validation Score')\n",
    "        axes[0, 1].fill_between(lc['train_sizes'], \n",
    "                               lc['val_scores_mean'] - lc['val_scores_std'],\n",
    "                               lc['val_scores_mean'] + lc['val_scores_std'], \n",
    "                               alpha=0.3, color='red')\n",
    "        \n",
    "        axes[0, 1].set_xlabel('Training Set Size')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].set_title('Learning Curves')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Validation Curves\n",
    "    if 'validation_curves' in locals():\n",
    "        vc = validation_curves\n",
    "        axes[1, 0].plot(vc['param_range'], vc['train_scores_mean'], 'o-', color='blue', \n",
    "                       label='Training Score')\n",
    "        axes[1, 0].fill_between(vc['param_range'], \n",
    "                               vc['train_scores_mean'] - vc['train_scores_std'],\n",
    "                               vc['train_scores_mean'] + vc['train_scores_std'], \n",
    "                               alpha=0.3, color='blue')\n",
    "        \n",
    "        axes[1, 0].plot(vc['param_range'], vc['val_scores_mean'], 'o-', color='red', \n",
    "                       label='Validation Score')\n",
    "        axes[1, 0].fill_between(vc['param_range'], \n",
    "                               vc['val_scores_mean'] - vc['val_scores_std'],\n",
    "                               vc['val_scores_mean'] + vc['val_scores_std'], \n",
    "                               alpha=0.3, color='red')\n",
    "        \n",
    "        axes[1, 0].set_xlabel('n_estimators')\n",
    "        axes[1, 0].set_ylabel('Accuracy')\n",
    "        axes[1, 0].set_title('Validation Curves (n_estimators)')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. CV Strategy Reliability (variance comparison)\n",
    "    if cv_comparison_data:\n",
    "        strategy_variance = cv_df.groupby('CV_Strategy')['Std_Score'].mean().sort_values()\n",
    "        \n",
    "        bars = axes[1, 1].bar(range(len(strategy_variance)), strategy_variance.values, \n",
    "                             alpha=0.7, color='lightgreen')\n",
    "        axes[1, 1].set_xlabel('Cross-Validation Strategy')\n",
    "        axes[1, 1].set_ylabel('Average Standard Deviation')\n",
    "        axes[1, 1].set_title('CV Strategy Reliability (Lower = More Stable)')\n",
    "        axes[1, 1].set_xticks(range(len(strategy_variance)))\n",
    "        axes[1, 1].set_xticklabels(strategy_variance.index, rotation=45, ha='right')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, strategy_variance.values):\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                           f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save cross-validation visualization\n",
    "    save_figure(fig, 'cross_validation_analysis',\n",
    "               'Comprehensive analysis of cross-validation strategies and learning curves', 'cv')\n",
    "    plt.show()\n",
    "    \n",
    "    # CV Summary\n",
    "    print(\"\\nðŸ“Š Cross-Validation Analysis Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if cv_comparison_data:\n",
    "        print(\"Best performing CV strategies by model:\")\n",
    "        for model in cv_df['Model'].unique():\n",
    "            model_data = cv_df[cv_df['Model'] == model]\n",
    "            best_cv = model_data.loc[model_data['Mean_Score'].idxmax()]\n",
    "            print(f\"  {model}: {best_cv['CV_Strategy']} (Score: {best_cv['Mean_Score']:.4f})\")\n",
    "        \n",
    "        print(f\"\\nMost reliable CV strategy: {strategy_variance.index[0]} (Std: {strategy_variance.iloc[0]:.4f})\")\n",
    "        print(f\"Least reliable CV strategy: {strategy_variance.index[-1]} (Std: {strategy_variance.iloc[-1]:.4f})\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ¨ Cross-validation visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a559d08",
   "metadata": {},
   "source": [
    "## 8. Model Selection Pipelines {#pipelines}\n",
    "\n",
    "Building comprehensive model selection pipelines that combine multiple techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54ceb1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive model selection pipeline\n",
    "print(\"ðŸ”§ Building Model Selection Pipelines...\")\n",
    "\n",
    "class ComprehensiveModelSelectionPipeline:\n",
    "    \"\"\"Complete pipeline for automated model selection and optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, optimization_budget=100, cv_folds=5, test_size=0.2):\n",
    "        self.optimization_budget = optimization_budget\n",
    "        self.cv_folds = cv_folds\n",
    "        self.test_size = test_size\n",
    "        self.results = {}\n",
    "        self.best_model = None\n",
    "        self.pipeline_history = []\n",
    "    \n",
    "    def run_pipeline(self, X, y, task_type='classification'):\n",
    "        \"\"\"Run the complete model selection pipeline.\"\"\"\n",
    "        print(\"ðŸš€ Starting Comprehensive Model Selection Pipeline...\")\n",
    "        \n",
    "        # Stage 1: Data preparation and splitting\n",
    "        print(\"\\nðŸ“Š Stage 1: Data Preparation\")\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=42, \n",
    "            stratify=y if task_type == 'classification' else None\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=0.25, random_state=42,\n",
    "            stratify=y_temp if task_type == 'classification' else None\n",
    "        )\n",
    "        \n",
    "        print(f\"  Training set: {X_train.shape}\")\n",
    "        print(f\"  Validation set: {X_val.shape}\")\n",
    "        print(f\"  Test set: {X_test.shape}\")\n",
    "        \n",
    "        # Stage 2: Quick model screening\n",
    "        print(\"\\nðŸ” Stage 2: Quick Model Screening\")\n",
    "        screening_results = self._quick_model_screening(X_train, y_train)\n",
    "        \n",
    "        # Stage 3: Hyperparameter optimization for top models\n",
    "        print(\"\\nâš™ï¸ Stage 3: Hyperparameter Optimization\")\n",
    "        optimization_results = self._optimize_top_models(\n",
    "            screening_results, X_train, y_train, X_val, y_val\n",
    "        )\n",
    "        \n",
    "        # Stage 4: Advanced validation\n",
    "        print(\"\\nðŸ”„ Stage 4: Advanced Cross-Validation\")\n",
    "        validation_results = self._advanced_validation(\n",
    "            optimization_results, X_train, y_train\n",
    "        )\n",
    "        \n",
    "        # Stage 5: Final model selection and testing\n",
    "        print(\"\\nðŸ† Stage 5: Final Model Selection\")\n",
    "        final_results = self._final_model_selection(\n",
    "            validation_results, X_train, y_train, X_test, y_test\n",
    "        )\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        report = self._generate_pipeline_report(final_results)\n",
    "        \n",
    "        return final_results, report\n",
    "    \n",
    "    def _quick_model_screening(self, X_train, y_train):\n",
    "        \"\"\"Quickly screen multiple models with default parameters.\"\"\"\n",
    "        candidate_models = {\n",
    "            'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'RandomForest': RandomForestClassifier(random_state=42),\n",
    "            'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "            'SVM': SVC(random_state=42),\n",
    "            'MLPClassifier': MLPClassifier(random_state=42, max_iter=500)\n",
    "        }\n",
    "        \n",
    "        screening_results = {}\n",
    "        \n",
    "        for name, model in candidate_models.items():\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
    "                screening_time = time.time() - start_time\n",
    "                \n",
    "                screening_results[name] = {\n",
    "                    'mean_score': scores.mean(),\n",
    "                    'std_score': scores.std(),\n",
    "                    'screening_time': screening_time,\n",
    "                    'model': model\n",
    "                }\n",
    "                \n",
    "                print(f\"  {name}: {scores.mean():.4f} Â± {scores.std():.4f} ({screening_time:.2f}s)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {name}: Failed ({str(e)})\")\n",
    "        \n",
    "        # Select top 3 models for optimization\n",
    "        valid_results = {k: v for k, v in screening_results.items() if 'model' in v}\n",
    "        top_models = sorted(valid_results.items(), key=lambda x: x[1]['mean_score'], reverse=True)[:3]\n",
    "        \n",
    "        print(f\"\\n  Selected top {len(top_models)} models for optimization:\")\n",
    "        for name, result in top_models:\n",
    "            print(f\"    {name}: {result['mean_score']:.4f}\")\n",
    "        \n",
    "        return dict(top_models)\n",
    "    \n",
    "    def _optimize_top_models(self, top_models, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Optimize hyperparameters for top performing models.\"\"\"\n",
    "        optimization_results = {}\n",
    "        \n",
    "        # Define parameter spaces\n",
    "        param_spaces = {\n",
    "            'RandomForest': {\n",
    "                'n_estimators': randint(50, 200),\n",
    "                'max_depth': randint(5, 20),\n",
    "                'min_samples_split': randint(2, 10)\n",
    "            },\n",
    "            'GradientBoosting': {\n",
    "                'n_estimators': randint(50, 150),\n",
    "                'learning_rate': uniform(0.05, 0.25),\n",
    "                'max_depth': randint(3, 8)\n",
    "            },\n",
    "            'LogisticRegression': {\n",
    "                'C': uniform(0.1, 10),\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'solver': ['liblinear', 'saga']\n",
    "            },\n",
    "            'SVM': {\n",
    "                'C': uniform(0.1, 10),\n",
    "                'gamma': ['scale'] + list(uniform(0.001, 1).rvs(3)),\n",
    "                'kernel': ['rbf', 'poly']\n",
    "            },\n",
    "            'MLPClassifier': {\n",
    "                'hidden_layer_sizes': [(50,), (100,), (50, 25), (100, 50)],\n",
    "                'alpha': uniform(0.0001, 0.01),\n",
    "                'learning_rate_init': uniform(0.001, 0.01)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for model_name, model_info in top_models.items():\n",
    "            if model_name in param_spaces:\n",
    "                print(f\"  Optimizing {model_name}...\")\n",
    "                \n",
    "                try:\n",
    "                    # Random search optimization\n",
    "                    search = RandomizedSearchCV(\n",
    "                        estimator=model_info['model'],\n",
    "                        param_distributions=param_spaces[model_name],\n",
    "                        n_iter=20,  # Reduced for demonstration\n",
    "                        cv=3,\n",
    "                        scoring='accuracy',\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1\n",
    "                    )\n",
    "                    \n",
    "                    start_time = time.time()\n",
    "                    search.fit(X_train, y_train)\n",
    "                    optimization_time = time.time() - start_time\n",
    "                    \n",
    "                    # Validate on holdout set\n",
    "                    val_score = search.best_estimator_.score(X_val, y_val)\n",
    "                    \n",
    "                    optimization_results[model_name] = {\n",
    "                        'best_estimator': search.best_estimator_,\n",
    "                        'best_params': search.best_params_,\n",
    "                        'best_cv_score': search.best_score_,\n",
    "                        'val_score': val_score,\n",
    "                        'optimization_time': optimization_time\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"    CV Score: {search.best_score_:.4f}\")\n",
    "                    print(f\"    Val Score: {val_score:.4f}\")\n",
    "                    print(f\"    Time: {optimization_time:.2f}s\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Failed: {str(e)}\")\n",
    "            else:\n",
    "                # Keep original model if no parameter space defined\n",
    "                optimization_results[model_name] = {\n",
    "                    'best_estimator': model_info['model'],\n",
    "                    'best_params': {},\n",
    "                    'best_cv_score': model_info['mean_score'],\n",
    "                    'val_score': model_info['model'].fit(X_train, y_train).score(X_val, y_val),\n",
    "                    'optimization_time': 0\n",
    "                }\n",
    "        \n",
    "        return optimization_results\n",
    "    \n",
    "    def _advanced_validation(self, optimization_results, X_train, y_train):\n",
    "        \"\"\"Perform advanced cross-validation on optimized models.\"\"\"\n",
    "        validation_results = {}\n",
    "        \n",
    "        # Different CV strategies\n",
    "        cv_strategies = {\n",
    "            'StratifiedKFold': StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "            'RepeatedStratifiedKFold': None  # Simplified for demo\n",
    "        }\n",
    "        \n",
    "        for model_name, model_info in optimization_results.items():\n",
    "            print(f\"  Validating {model_name}...\")\n",
    "            \n",
    "            validation_results[model_name] = model_info.copy()\n",
    "            validation_results[model_name]['cv_validation'] = {}\n",
    "            \n",
    "            model = model_info['best_estimator']\n",
    "            \n",
    "            for cv_name, cv_strategy in cv_strategies.items():\n",
    "                if cv_strategy is not None:\n",
    "                    try:\n",
    "                        scores = cross_val_score(model, X_train, y_train, cv=cv_strategy, \n",
    "                                               scoring='accuracy', n_jobs=-1)\n",
    "                        \n",
    "                        validation_results[model_name]['cv_validation'][cv_name] = {\n",
    "                            'mean_score': scores.mean(),\n",
    "                            'std_score': scores.std(),\n",
    "                            'scores': scores.tolist()\n",
    "                        }\n",
    "                        \n",
    "                        print(f\"    {cv_name}: {scores.mean():.4f} Â± {scores.std():.4f}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"    {cv_name}: Failed ({str(e)})\")\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    def _final_model_selection(self, validation_results, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Select final model and evaluate on test set.\"\"\"\n",
    "        # Select best model based on validation score\n",
    "        best_model_name = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for model_name, results in validation_results.items():\n",
    "            cv_results = results.get('cv_validation', {})\n",
    "            if 'StratifiedKFold' in cv_results:\n",
    "                score = cv_results['StratifiedKFold']['mean_score']\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_model_name = model_name\n",
    "        \n",
    "        if best_model_name:\n",
    "            print(f\"  Selected best model: {best_model_name}\")\n",
    "            \n",
    "            # Train final model on full training set\n",
    "            best_model = validation_results[best_model_name]['best_estimator']\n",
    "            best_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Final test evaluation\n",
    "            test_score = best_model.score(X_test, y_test)\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            \n",
    "            print(f\"  Final test score: {test_score:.4f}\")\n",
    "            \n",
    "            final_results = {\n",
    "                'best_model_name': best_model_name,\n",
    "                'best_model': best_model,\n",
    "                'best_params': validation_results[best_model_name]['best_params'],\n",
    "                'cv_score': best_score,\n",
    "                'test_score': test_score,\n",
    "                'y_pred': y_pred,\n",
    "                'y_test': y_test,\n",
    "                'all_validation_results': validation_results\n",
    "            }\n",
    "            \n",
    "            self.best_model = best_model\n",
    "            \n",
    "            return final_results\n",
    "        else:\n",
    "            print(\"  No valid model found!\")\n",
    "            return None\n",
    "    \n",
    "    def _generate_pipeline_report(self, final_results):\n",
    "        \"\"\"Generate comprehensive pipeline report.\"\"\"\n",
    "        if not final_results:\n",
    "            return \"Pipeline failed - no valid results.\"\n",
    "        \n",
    "        report = \"\\n\" + \"=\"*80 + \"\\n\"\n",
    "        report += \"COMPREHENSIVE MODEL SELECTION PIPELINE REPORT\\n\"\n",
    "        report += \"=\"*80 + \"\\n\\n\"\n",
    "        \n",
    "        report += f\"Best Model: {final_results['best_model_name']}\\n\"\n",
    "        report += f\"Best Parameters: {final_results['best_params']}\\n\"\n",
    "        report += f\"Cross-Validation Score: {final_results['cv_score']:.4f}\\n\"\n",
    "        report += f\"Final Test Score: {final_results['test_score']:.4f}\\n\\n\"\n",
    "        \n",
    "        report += \"Model Comparison Summary:\\n\"\n",
    "        report += \"-\" * 40 + \"\\n\"\n",
    "        \n",
    "        for model_name, results in final_results['all_validation_results'].items():\n",
    "            cv_results = results.get('cv_validation', {})\n",
    "            if 'StratifiedKFold' in cv_results:\n",
    "                score = cv_results['StratifiedKFold']['mean_score']\n",
    "                std = cv_results['StratifiedKFold']['std_score']\n",
    "                report += f\"{model_name}: {score:.4f} Â± {std:.4f}\\n\"\n",
    "        \n",
    "        report += \"\\n\" + \"=\"*80 + \"\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Run the comprehensive pipeline\n",
    "print(\"\\n--- Running Comprehensive Model Selection Pipeline ---\")\n",
    "\n",
    "# Use the complex dataset\n",
    "X_pipeline, y_pipeline = datasets['Complex'][0], datasets['Complex'][1]\n",
    "X_combined = np.vstack([X_pipeline, datasets['Complex'][2]])\n",
    "y_combined = np.hstack([y_pipeline, datasets['Complex'][3]])\n",
    "\n",
    "# Initialize and run pipeline\n",
    "pipeline = ComprehensiveModelSelectionPipeline(\n",
    "    optimization_budget=100,\n",
    "    cv_folds=5,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "try:\n",
    "    final_results, pipeline_report = pipeline.run_pipeline(X_combined, y_combined)\n",
    "    \n",
    "    if final_results:\n",
    "        print(pipeline_report)\n",
    "        \n",
    "        # Save pipeline results\n",
    "        pipeline_summary = {\n",
    "            'best_model_name': final_results['best_model_name'],\n",
    "            'best_params': final_results['best_params'],\n",
    "            'cv_score': final_results['cv_score'],\n",
    "            'test_score': final_results['test_score'],\n",
    "            'pipeline_stages': ['screening', 'optimization', 'validation', 'selection']\n",
    "        }\n",
    "        \n",
    "        save_optimization_experiment('comprehensive_pipeline_results', pipeline_summary,\n",
    "                                   'Results from comprehensive model selection pipeline')\n",
    "        \n",
    "        # Save the final best model\n",
    "        save_optimized_model(final_results['best_model'], 'pipeline_best_model',\n",
    "                           'Best model selected through comprehensive pipeline', pipeline_summary)\n",
    "        \n",
    "        # Save the pipeline itself\n",
    "        save_pipeline(pipeline, 'comprehensive_model_selection',\n",
    "                     'Complete model selection pipeline with all stages', pipeline_summary)\n",
    "        \n",
    "        # Save the report\n",
    "        save_model_selection_report(pipeline_report, 'comprehensive_pipeline_report', 'txt')\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ Pipeline failed to produce results\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Pipeline failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nâœ¨ Comprehensive model selection pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1062928",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This comprehensive notebook has demonstrated advanced model selection and hyperparameter tuning techniques including:\n",
    "\n",
    "- **Automated Model Selection**: Intelligent algorithm selection based on dataset characteristics\n",
    "- **Advanced Hyperparameter Optimization**: Grid search, random search, and custom optimization strategies\n",
    "- **Multi-Objective Optimization**: Balancing accuracy, speed, and model complexity\n",
    "- **Bayesian Optimization**: Efficient hyperparameter search using Gaussian processes\n",
    "- **Population-Based Training**: Evolutionary approaches to hyperparameter optimization\n",
    "- **Advanced Cross-Validation**: Robust evaluation strategies for different data scenarios\n",
    "- **Comprehensive Pipelines**: End-to-end model selection workflows\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Intelligent Automation**: Automated model selection based on dataset characteristics\n",
    "2. **Multi-Strategy Optimization**: Comparison of different optimization approaches\n",
    "3. **Robust Validation**: Advanced cross-validation techniques for reliable evaluation\n",
    "4. **Comprehensive Reporting**: Detailed analysis and comparison of results\n",
    "5. **Production Ready**: Complete pipelines with monitoring and result tracking\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Integrate with AutoML frameworks** for even more sophisticated automation\n",
    "2. **Add neural architecture search** for deep learning models\n",
    "3. **Implement distributed optimization** for large-scale hyperparameter search\n",
    "4. **Add cost-aware optimization** considering computational budgets\n",
    "5. **Extend to multi-modal datasets** and specialized domains\n",
    "\n",
    "The modular design allows for easy extension and customization for specific use cases, while the comprehensive result tracking ensures reproducibility and analysis of optimization strategies."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
