{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d36fef",
   "metadata": {},
   "source": [
    "# Advanced Supervised Learning\n",
    "\n",
    "This notebook showcases advanced supervised learning techniques including sophisticated classification and regression models, hyperparameter optimization, model interpretation, and production-ready deployment strategies.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Results Management Setup](#results-setup)\n",
    "3. [Advanced Classification Models](#classification)\n",
    "4. [Advanced Regression Models](#regression)\n",
    "5. [Model Comparison and Selection](#comparison)\n",
    "6. [Hyperparameter Optimization](#optimization)\n",
    "7. [Advanced Evaluation Metrics](#evaluation)\n",
    "8. [Model Interpretation and Explainability](#interpretation)\n",
    "9. [Production-Ready Models](#production)\n",
    "10. [Results Persistence and Reporting](#persistence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc406d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ce507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced imports for supervised learning\n",
    "from sklearn.model_selection import RandomizedSearchCV, learning_curve\n",
    "from sklearn.inspection import permutation_importance, partial_dependence\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# Results saving imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da28772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.generators import SyntheticDataGenerator\n",
    "from data.preprocessors import DataPreprocessor\n",
    "from models.supervised.classification import *\n",
    "from models.supervised.regression import *\n",
    "from pipelines.model_selection import AdvancedModelSelector\n",
    "from evaluation.metrics import ModelEvaluator\n",
    "from evaluation.visualization import ModelVisualizationSuite\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e1493",
   "metadata": {},
   "source": [
    "## 2. Results Management Setup {#results-setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfae41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results saving setup for supervised learning\n",
    "def setup_results_directories():\n",
    "    \"\"\"Create results directory structure if it doesn't exist.\"\"\"\n",
    "    base_dir = Path('../results')\n",
    "    directories = [\n",
    "        base_dir / 'figures',\n",
    "        base_dir / 'models',\n",
    "        base_dir / 'supervised_models',  # Specific for supervised learning models\n",
    "        base_dir / 'classification_models',  # Classification-specific models\n",
    "        base_dir / 'regression_models',  # Regression-specific models\n",
    "        base_dir / 'optimized_models',  # Hyperparameter optimized models\n",
    "        base_dir / 'production_models',  # Production-ready models\n",
    "        base_dir / 'interpretability',  # Model interpretation results\n",
    "        base_dir / 'experiments',\n",
    "        base_dir / 'reports'\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"üìÅ Created/verified directory: {directory}\")\n",
    "    \n",
    "    return base_dir\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\"Get current timestamp for file naming.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def save_supervised_figure(fig, name, description=\"\", category=\"general\", dpi=300):\n",
    "    \"\"\"Save supervised learning figure with proper naming and metadata.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_supervised_{category}_{name}.png\"\n",
    "    filepath = results_dir / 'figures' / filename\n",
    "    \n",
    "    # Save figure\n",
    "    fig.savefig(filepath, dpi=dpi, bbox_inches='tight', facecolor='white')\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'filename': filename,\n",
    "        'description': description,\n",
    "        'category': f'supervised_{category}',\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '03_supervised_learning',\n",
    "        'dpi': dpi\n",
    "    }\n",
    "    \n",
    "    metadata_file = filepath.with_suffix('.json')\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Saved supervised figure: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_supervised_model(model, name, model_type=\"classifier\", description=\"\", performance_metrics=None):\n",
    "    \"\"\"Save supervised learning model with comprehensive metadata.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_{model_type}_{name}.joblib\"\n",
    "    \n",
    "    # Choose appropriate directory based on model type\n",
    "    if model_type == 'classifier':\n",
    "        filepath = results_dir / 'classification_models' / filename\n",
    "    elif model_type == 'regressor':\n",
    "        filepath = results_dir / 'regression_models' / filename\n",
    "    elif model_type == 'optimized_classifier' or model_type == 'optimized_regressor':\n",
    "        filepath = results_dir / 'optimized_models' / filename\n",
    "    elif model_type == 'production':\n",
    "        filepath = results_dir / 'production_models' / filename\n",
    "    else:\n",
    "        filepath = results_dir / 'supervised_models' / filename\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, filepath, compress=3)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'filename': filename,\n",
    "        'model_name': name,\n",
    "        'description': description,\n",
    "        'model_type': model_type,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '03_supervised_learning',\n",
    "        'algorithm': type(model).__name__,\n",
    "        'performance_metrics': performance_metrics or {},\n",
    "        'file_size_mb': filepath.stat().st_size / (1024*1024) if filepath.exists() else 0\n",
    "    }\n",
    "    \n",
    "    metadata_file = filepath.with_suffix('.json')\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üíæ Saved supervised model: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_experiment_results(experiment_name, results, description=\"\", technique_type=\"supervised\"):\n",
    "    \"\"\"Save experiment results with detailed configuration.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_{technique_type}_{experiment_name}.json\"\n",
    "    filepath = results_dir / 'experiments' / filename\n",
    "    \n",
    "    experiment_data = {\n",
    "        'experiment_name': experiment_name,\n",
    "        'description': description,\n",
    "        'technique_type': technique_type,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '03_supervised_learning',\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(experiment_data, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üíæ Saved experiment results: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_report(content, report_name, description=\"\", format='txt'):\n",
    "    \"\"\"Save comprehensive analysis report.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_supervised_report_{report_name}.{format}\"\n",
    "    filepath = results_dir / 'reports' / filename\n",
    "    \n",
    "    if format == 'txt':\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(content)\n",
    "    elif format == 'json':\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(content, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üíæ Saved report: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "# Initialize results directories\n",
    "results_dir = setup_results_directories()\n",
    "print(f\"üìä Supervised learning results will be saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a20b01a",
   "metadata": {},
   "source": [
    "## 3. Advanced Classification Models {#classification}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7cf2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification datasets\n",
    "print(\"üéØ Advanced Classification Models...\")\n",
    "\n",
    "generator = SyntheticDataGenerator(random_state=42)\n",
    "\n",
    "# Binary classification dataset\n",
    "X_binary, y_binary = generator.classification_dataset(\n",
    "    n_samples=1500,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=3,\n",
    "    n_clusters_per_class=2,\n",
    "    class_sep=0.8\n",
    ")\n",
    "\n",
    "print(f\"Binary classification dataset: {X_binary.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_binary)}\")\n",
    "\n",
    "# Multiclass classification dataset\n",
    "X_multi, y_multi = generator.classification_dataset(\n",
    "    n_samples=2000,\n",
    "    n_features=25,\n",
    "    n_informative=18,\n",
    "    n_classes=4,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=0.7\n",
    ")\n",
    "\n",
    "print(f\"Multiclass classification dataset: {X_multi.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_multi)}\")\n",
    "\n",
    "# Imbalanced dataset\n",
    "X_imbalanced, y_imbalanced = generator.imbalanced_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=15,\n",
    "    imbalance_ratio=0.1\n",
    ")\n",
    "\n",
    "print(f\"Imbalanced classification dataset: {X_imbalanced.shape}\")\n",
    "print(f\"Imbalanced class distribution: {np.bincount(y_imbalanced)}\")\n",
    "\n",
    "# Save dataset information\n",
    "dataset_info = {\n",
    "    'binary_classification': {\n",
    "        'shape': X_binary.shape,\n",
    "        'class_distribution': np.bincount(y_binary).tolist(),\n",
    "        'description': 'Balanced binary classification with moderate complexity'\n",
    "    },\n",
    "    'multiclass_classification': {\n",
    "        'shape': X_multi.shape,\n",
    "        'class_distribution': np.bincount(y_multi).tolist(),\n",
    "        'description': '4-class classification problem'\n",
    "    },\n",
    "    'imbalanced_classification': {\n",
    "        'shape': X_imbalanced.shape,\n",
    "        'class_distribution': np.bincount(y_imbalanced).tolist(),\n",
    "        'description': 'Highly imbalanced binary classification (10:1 ratio)'\n",
    "    }\n",
    "}\n",
    "\n",
    "save_experiment_results(\"classification_datasets\", dataset_info, \n",
    "                       \"Generated synthetic classification datasets for model testing\")\n",
    "\n",
    "print(\"\\n‚ú® Classification datasets generated and saved!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c0151",
   "metadata": {},
   "source": [
    "### Advanced Classification Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b450990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test advanced classification models\n",
    "print(\"üöÄ Testing Advanced Classification Models...\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_binary, y_binary, test_size=0.3, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "# Initialize advanced classifiers\n",
    "classifiers = {\n",
    "    'AdaBoost Enhanced': AdaBoostEnhanced(random_state=42),\n",
    "    'Gradient Boosting Enhanced': GradientBoostingEnhanced(random_state=42),\n",
    "    'Neural Network Enhanced': NeuralNetworkEnhanced(random_state=42),\n",
    "    'SVM Enhanced': SVMEnhanced(random_state=42),\n",
    "    'Hybrid Ensemble': HybridEnsembleClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "results = {}\n",
    "training_times = {}\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Time training\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Fit model\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        training_times[name] = training_time\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        y_pred_proba = classifier.predict_proba(X_test)[:, 1] if hasattr(classifier, 'predict_proba') else None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "        }\n",
    "        \n",
    "        results[name] = metrics\n",
    "        \n",
    "        print(f\"  Training time: {training_time:.3f}s\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\" if metrics['roc_auc'] else \"  ROC-AUC: N/A\")\n",
    "        \n",
    "        # Save individual model\n",
    "        model_metadata = {\n",
    "            'accuracy': metrics['accuracy'],\n",
    "            'f1_score': metrics['f1'],\n",
    "            'training_time': training_time,\n",
    "            'dataset_size': len(X_train)\n",
    "        }\n",
    "        \n",
    "        save_supervised_model(classifier, name.lower().replace(' ', '_'), \n",
    "                            \"classifier\", f\"Advanced classification model: {name}\", model_metadata)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "        results[name] = {'error': str(e)}\n",
    "\n",
    "# Save classification results\n",
    "classification_experiment = {\n",
    "    'model_results': results,\n",
    "    'training_times': training_times,\n",
    "    'dataset_info': {\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'features': X_train.shape[1],\n",
    "        'classes': len(np.unique(y_train))\n",
    "    }\n",
    "}\n",
    "\n",
    "save_experiment_results(\"advanced_classification_models\", classification_experiment,\n",
    "                       \"Performance evaluation of advanced classification algorithms\")\n",
    "\n",
    "print(\"\\n‚ú® Advanced classification models tested and results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8cac42",
   "metadata": {},
   "source": [
    "### Classification Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f31e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification results\n",
    "print(\"üìä Visualizing Classification Results...\")\n",
    "\n",
    "# Filter successful results\n",
    "successful_results = {k: v for k, v in results.items() if 'error' not in v}\n",
    "\n",
    "if successful_results:\n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    model_names = list(successful_results.keys())\n",
    "    \n",
    "    # 1. Accuracy comparison\n",
    "    accuracies = [successful_results[name]['accuracy'] for name in model_names]\n",
    "    bars1 = axes[0, 0].bar(model_names, accuracies, color='lightblue', alpha=0.7)\n",
    "    axes[0, 0].set_title('Model Accuracy Comparison')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].set_ylim(0.8, 1.0)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, acc in zip(bars1, accuracies):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                       f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. F1-Score comparison\n",
    "    f1_scores = [successful_results[name]['f1'] for name in model_names]\n",
    "    bars2 = axes[0, 1].bar(model_names, f1_scores, color='lightcoral', alpha=0.7)\n",
    "    axes[0, 1].set_title('F1-Score Comparison')\n",
    "    axes[0, 1].set_ylabel('F1-Score')\n",
    "    axes[0, 1].set_ylim(0.8, 1.0)\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, f1 in zip(bars2, f1_scores):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                       f'{f1:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Training time comparison\n",
    "    times = [training_times.get(name, 0) for name in model_names]\n",
    "    bars3 = axes[1, 0].bar(model_names, times, color='lightgreen', alpha=0.7)\n",
    "    axes[1, 0].set_title('Training Time Comparison')\n",
    "    axes[1, 0].set_ylabel('Time (seconds)')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, time_val in zip(bars3, times):\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                       f'{time_val:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Multi-metric radar chart\n",
    "    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics_to_plot), endpoint=False)\n",
    "    angles = np.concatenate((angles, [angles[0]]))\n",
    "    \n",
    "    ax_radar = plt.subplot(2, 2, 4, projection='polar')\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))\n",
    "    \n",
    "    for i, model_name in enumerate(model_names):\n",
    "        values = [successful_results[model_name][metric] for metric in metrics_to_plot]\n",
    "        values += [values[0]]  # Complete the circle\n",
    "        \n",
    "        ax_radar.plot(angles, values, 'o-', linewidth=2, \n",
    "                     label=model_name, color=colors[i])\n",
    "        ax_radar.fill(angles, values, alpha=0.1, color=colors[i])\n",
    "    \n",
    "    ax_radar.set_xticks(angles[:-1])\n",
    "    ax_radar.set_xticklabels([m.title() for m in metrics_to_plot])\n",
    "    ax_radar.set_ylim(0, 1)\n",
    "    ax_radar.set_title('Multi-Metric Performance Radar')\n",
    "    ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the visualization\n",
    "    save_supervised_figure(fig, \"classification_model_comparison\", \n",
    "                          \"Comprehensive comparison of classification models\", \"classification\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Performance summary\n",
    "    print(\"\\nüèÜ Classification Performance Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Model':<25} {'Accuracy':<10} {'F1-Score':<10} {'ROC-AUC':<10} {'Time(s)':<10}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for name in model_names:\n",
    "        metrics = successful_results[name]\n",
    "        roc_auc = metrics.get('roc_auc', 'N/A')\n",
    "        roc_auc_str = f\"{roc_auc:.4f}\" if roc_auc != 'N/A' and roc_auc is not None else \"N/A\"\n",
    "        time_val = training_times.get(name, 0)\n",
    "        \n",
    "        print(f\"{name:<25} {metrics['accuracy']:<10.4f} {metrics['f1']:<10.4f} \"\n",
    "              f\"{roc_auc_str:<10} {time_val:<10.3f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No successful classification results to visualize.\")\n",
    "\n",
    "print(\"\\n‚ú® Classification results visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283bf13",
   "metadata": {},
   "source": [
    "## 4. Advanced Regression Models {#regression}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d33ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression datasets\n",
    "print(\"üìà Advanced Regression Models...\")\n",
    "\n",
    "# Standard regression dataset\n",
    "X_reg, y_reg = generator.regression_dataset(\n",
    "    n_samples=1500,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    noise=0.1,\n",
    "    bias=10.0\n",
    ")\n",
    "\n",
    "print(f\"Standard regression dataset: {X_reg.shape}\")\n",
    "print(f\"Target statistics: mean={y_reg.mean():.2f}, std={y_reg.std():.2f}\")\n",
    "\n",
    "# Nonlinear regression dataset\n",
    "X_nonlinear, y_nonlinear = generator.nonlinear_regression(\n",
    "    n_samples=1200,\n",
    "    n_features=15,\n",
    "    noise_level=0.15\n",
    ")\n",
    "\n",
    "print(f\"Nonlinear regression dataset: {X_nonlinear.shape}\")\n",
    "print(f\"Nonlinear target statistics: mean={y_nonlinear.mean():.2f}, std={y_nonlinear.std():.2f}\")\n",
    "\n",
    "# Time series regression\n",
    "X_ts, y_ts = generator.time_series_features(\n",
    "    n_samples=1000,\n",
    "    n_features=12,\n",
    "    trend=True,\n",
    "    seasonality=True,\n",
    "    noise_level=0.1\n",
    ")\n",
    "\n",
    "# Extract target from time series features\n",
    "y_ts_target = X_ts.iloc[:, 0] + 0.1 * np.random.randn(len(X_ts))\n",
    "X_ts_features = X_ts.iloc[:, 1:]\n",
    "\n",
    "print(f\"Time series regression dataset: {X_ts_features.shape}\")\n",
    "print(f\"Time series target statistics: mean={y_ts_target.mean():.2f}, std={y_ts_target.std():.2f}\")\n",
    "\n",
    "# Save regression dataset information\n",
    "regression_dataset_info = {\n",
    "    'standard_regression': {\n",
    "        'shape': X_reg.shape,\n",
    "        'target_stats': {'mean': float(y_reg.mean()), 'std': float(y_reg.std())},\n",
    "        'description': 'Linear regression with moderate noise'\n",
    "    },\n",
    "    'nonlinear_regression': {\n",
    "        'shape': X_nonlinear.shape,\n",
    "        'target_stats': {'mean': float(y_nonlinear.mean()), 'std': float(y_nonlinear.std())},\n",
    "        'description': 'Nonlinear regression with polynomial features'\n",
    "    },\n",
    "    'time_series_regression': {\n",
    "        'shape': X_ts_features.shape,\n",
    "        'target_stats': {'mean': float(y_ts_target.mean()), 'std': float(y_ts_target.std())},\n",
    "        'description': 'Time series regression with trend and seasonality'\n",
    "    }\n",
    "}\n",
    "\n",
    "save_experiment_results(\"regression_datasets\", regression_dataset_info, \n",
    "                       \"Generated synthetic regression datasets for model testing\")\n",
    "\n",
    "print(\"\\n‚ú® Regression datasets generated and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507760cb",
   "metadata": {},
   "source": [
    "### Advanced Regression Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test advanced regression models\n",
    "print(\"üöÄ Testing Advanced Regression Models...\")\n",
    "\n",
    "# Split standard regression data\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize advanced regressors\n",
    "regressors = {\n",
    "    'Random Forest Enhanced': RandomForestEnhanced(random_state=42),\n",
    "    'Gradient Boosting Enhanced': GradientBoostingRegressor(random_state=42),\n",
    "    'Neural Network Enhanced': NeuralNetworkRegressor(random_state=42),\n",
    "    'SVR Enhanced': SVREnhanced(random_state=42),\n",
    "    'Adaptive Regression': AdaptiveRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each regressor\n",
    "regression_results = {}\n",
    "regression_times = {}\n",
    "\n",
    "for name, regressor in regressors.items():\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Time training\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Fit model\n",
    "        regressor.fit(X_reg_train, y_reg_train)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        regression_times[name] = training_time\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = regressor.predict(X_reg_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import mean_absolute_percentage_error\n",
    "        \n",
    "        metrics = {\n",
    "            'mse': mean_squared_error(y_reg_test, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_reg_test, y_pred)),\n",
    "            'mae': mean_absolute_error(y_reg_test, y_pred),\n",
    "            'r2': r2_score(y_reg_test, y_pred),\n",
    "            'mape': mean_absolute_percentage_error(y_reg_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        regression_results[name] = metrics\n",
    "        \n",
    "        print(f\"  Training time: {training_time:.3f}s\")\n",
    "        print(f\"  R¬≤ Score: {metrics['r2']:.4f}\")\n",
    "        print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "        print(f\"  MAE: {metrics['mae']:.4f}\")\n",
    "        \n",
    "        # Save individual model\n",
    "        model_metadata = {\n",
    "            'r2_score': metrics['r2'],\n",
    "            'rmse': metrics['rmse'],\n",
    "            'training_time': training_time,\n",
    "            'dataset_size': len(X_reg_train)\n",
    "        }\n",
    "        \n",
    "        save_supervised_model(regressor, name.lower().replace(' ', '_'), \n",
    "                            \"regressor\", f\"Advanced regression model: {name}\", model_metadata)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "        regression_results[name] = {'error': str(e)}\n",
    "\n",
    "# Save regression results\n",
    "regression_experiment = {\n",
    "    'model_results': regression_results,\n",
    "    'training_times': regression_times,\n",
    "    'dataset_info': {\n",
    "        'train_samples': len(X_reg_train),\n",
    "        'test_samples': len(X_reg_test),\n",
    "        'features': X_reg_train.shape[1]\n",
    "    }\n",
    "}\n",
    "\n",
    "save_experiment_results(\"advanced_regression_models\", regression_experiment,\n",
    "                       \"Performance evaluation of advanced regression algorithms\")\n",
    "\n",
    "print(\"\\n‚ú® Advanced regression models tested and results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2118b6",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Selection {#comparison}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize advanced model selector\n",
    "print(\"üéØ Advanced Model Selection and Comparison...\")\n",
    "\n",
    "model_selector = AdvancedModelSelector(\n",
    "    cv_folds=5,\n",
    "    scoring='accuracy',  # for classification\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- Binary Classification Model Selection ---\")\n",
    "\n",
    "try:\n",
    "    # Run model comparison\n",
    "    best_model, comparison_results = model_selector.compare_models(\n",
    "        X_train, y_train, task_type='classification'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Model: {best_model.__class__.__name__}\")\n",
    "    \n",
    "    # Display comparison results\n",
    "    print(\"\\nüìä Model Comparison Results:\")\n",
    "    comparison_df = pd.DataFrame(comparison_results).T\n",
    "    comparison_df = comparison_df.sort_values('mean_score', ascending=False)\n",
    "    \n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Save model selection results\n",
    "    model_selection_experiment = {\n",
    "        'best_model': best_model.__class__.__name__,\n",
    "        'comparison_results': comparison_results,\n",
    "        'task_type': 'classification',\n",
    "        'scoring_metric': 'accuracy',\n",
    "        'cv_folds': 5\n",
    "    }\n",
    "    \n",
    "    save_experiment_results(\"classification_model_selection\", model_selection_experiment,\n",
    "                           \"Automated model selection for classification task\")\n",
    "    \n",
    "    # Save best model\n",
    "    best_model_metadata = {\n",
    "        'selection_score': comparison_results[best_model.__class__.__name__]['mean_score'],\n",
    "        'selection_method': 'cross_validation',\n",
    "        'cv_folds': 5\n",
    "    }\n",
    "    \n",
    "    save_supervised_model(best_model, \"best_selected_classifier\", \n",
    "                        \"classifier\", \"Best model from automated selection\", best_model_metadata)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Model selection failed: {str(e)}\")\n",
    "\n",
    "print(\"\\n‚ú® Model comparison and selection complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e050f",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Optimization {#optimization}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate hyperparameter optimization\n",
    "print(\"‚öôÔ∏è Advanced Hyperparameter Optimization...\")\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Define parameter distributions for different models\n",
    "param_distributions = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'max_depth': randint(3, 20),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10),\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': uniform(0.1, 10),\n",
    "        'gamma': ['scale', 'auto'] + list(uniform(0.001, 1).rvs(5)),\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Models to optimize\n",
    "models_to_optimize = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "optimization_results = {}\n",
    "\n",
    "for model_name, model in models_to_optimize.items():\n",
    "    print(f\"\\n--- Optimizing {model_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Randomized search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_distributions[model_name],\n",
    "            n_iter=50,\n",
    "            cv=5,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fit randomized search\n",
    "        start_time = time.time()\n",
    "        random_search.fit(X_train, y_train)\n",
    "        optimization_time = time.time() - start_time\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = random_search.best_estimator_\n",
    "        best_score = random_search.best_score_\n",
    "        best_params = random_search.best_params_\n",
    "        \n",
    "        # Test on test set\n",
    "        test_score = best_model.score(X_test, y_test)\n",
    "        \n",
    "        optimization_results[model_name] = {\n",
    "            'best_cv_score': best_score,\n",
    "            'test_score': test_score,\n",
    "            'best_params': best_params,\n",
    "            'optimization_time': optimization_time,\n",
    "            'best_model': best_model\n",
    "        }\n",
    "        \n",
    "        print(f\"  Optimization time: {optimization_time:.2f}s\")\n",
    "        print(f\"  Best CV score: {best_score:.4f}\")\n",
    "        print(f\"  Test score: {test_score:.4f}\")\n",
    "        print(f\"  Best parameters: {best_params}\")\n",
    "        \n",
    "        # Save optimized model\n",
    "        opt_metadata = {\n",
    "            'best_cv_score': best_score,\n",
    "            'test_score': test_score,\n",
    "            'optimization_time': optimization_time,\n",
    "            'best_params': best_params\n",
    "        }\n",
    "        \n",
    "        save_supervised_model(best_model, f\"{model_name.lower()}_optimized\", \n",
    "                            \"optimized_classifier\", f\"Hyperparameter optimized {model_name}\", opt_metadata)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Optimization failed: {str(e)}\")\n",
    "\n",
    "# Save optimization results\n",
    "save_experiment_results(\"hyperparameter_optimization\", optimization_results,\n",
    "                       \"Hyperparameter optimization using randomized search\")\n",
    "\n",
    "print(\"\\n‚ú® Hyperparameter optimization complete and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004cd6ec",
   "metadata": {},
   "source": [
    "### Optimization Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimized vs default models\n",
    "print(\"\\nüìä Comparing Optimized vs Default Models:\")\n",
    "\n",
    "if optimization_results:\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name in models_to_optimize.keys():\n",
    "        if model_name in optimization_results:\n",
    "            # Default model\n",
    "            default_model = models_to_optimize[model_name]\n",
    "            default_model.fit(X_train, y_train)\n",
    "            default_score = default_model.score(X_test, y_test)\n",
    "            \n",
    "            # Optimized model\n",
    "            optimized_score = optimization_results[model_name]['test_score']\n",
    "            \n",
    "            improvement = ((optimized_score - default_score) / default_score) * 100\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'Default Score': default_score,\n",
    "                'Optimized Score': optimized_score,\n",
    "                'Improvement (%)': improvement\n",
    "            })\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        print(comparison_df.round(4))\n",
    "        \n",
    "        # Save comparison data\n",
    "        save_experiment_results(\"optimization_comparison\", comparison_data,\n",
    "                               \"Comparison of default vs optimized model performance\")\n",
    "        \n",
    "        # Visualize improvements\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Score comparison\n",
    "        x = np.arange(len(comparison_df))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0].bar(x - width/2, comparison_df['Default Score'], width, \n",
    "                   label='Default', color='lightcoral', alpha=0.7)\n",
    "        axes[0].bar(x + width/2, comparison_df['Optimized Score'], width, \n",
    "                   label='Optimized', color='lightblue', alpha=0.7)\n",
    "        \n",
    "        axes[0].set_xlabel('Models')\n",
    "        axes[0].set_ylabel('Test Score')\n",
    "        axes[0].set_title('Default vs Optimized Model Performance')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(comparison_df['Model'])\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Improvement percentage\n",
    "        colors = ['green' if imp > 0 else 'red' for imp in comparison_df['Improvement (%)']]\n",
    "        bars = axes[1].bar(comparison_df['Model'], comparison_df['Improvement (%)'], \n",
    "                          color=colors, alpha=0.7)\n",
    "        axes[1].set_xlabel('Models')\n",
    "        axes[1].set_ylabel('Improvement (%)')\n",
    "        axes[1].set_title('Performance Improvement from Optimization')\n",
    "        axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, imp in zip(bars, comparison_df['Improvement (%)']):\n",
    "            axes[1].text(bar.get_x() + bar.get_width()/2, \n",
    "                        bar.get_height() + (0.1 if imp > 0 else -0.3), \n",
    "                        f'{imp:.1f}%', ha='center', va='bottom' if imp > 0 else 'top')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the visualization\n",
    "        save_supervised_figure(fig, \"hyperparameter_optimization_results\", \n",
    "                              \"Results of hyperparameter optimization experiments\", \"optimization\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n‚ú® Hyperparameter optimization analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6938877e",
   "metadata": {},
   "source": [
    "## 7. Advanced Evaluation Metrics {#evaluation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate advanced evaluation metrics\n",
    "print(\"üìä Advanced Model Evaluation Metrics...\")\n",
    "\n",
    "# Initialize model evaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Train a few models for evaluation\n",
    "evaluation_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "model_predictions = {}\n",
    "\n",
    "for name, model in evaluation_models.items():\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        model_predictions[name] = {\n",
    "            'model': model,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to train {name}: {str(e)}\")\n",
    "\n",
    "if model_predictions:\n",
    "    # Comprehensive evaluation\n",
    "    print(\"\\nüîç Comprehensive Model Evaluation:\")\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for name, pred_data in model_predictions.items():\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Get comprehensive metrics\n",
    "            metrics = evaluator.evaluate_classification(\n",
    "                y_test, pred_data['y_pred'], pred_data['y_pred_proba']\n",
    "            )\n",
    "            \n",
    "            evaluation_results[name] = metrics\n",
    "            \n",
    "            print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "            print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "            print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
    "            print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "            print(f\"  PR-AUC: {metrics['pr_auc']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Evaluation failed: {str(e)}\")\n",
    "    \n",
    "    # Save evaluation results\n",
    "    save_experiment_results(\"advanced_evaluation_metrics\", evaluation_results,\n",
    "                           \"Comprehensive evaluation metrics for multiple models\")\n",
    "\n",
    "print(\"\\n‚ú® Advanced evaluation metrics demonstrated and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27625ace",
   "metadata": {},
   "source": [
    "## 8. Model Interpretation and Explainability {#interpretation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18176fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model interpretation and explainability\n",
    "print(\"üîç Model Interpretation and Explainability...\")\n",
    "\n",
    "from sklearn.inspection import permutation_importance, partial_dependence, PartialDependenceDisplay\n",
    "\n",
    "# Use the best performing model for interpretation\n",
    "if model_predictions:\n",
    "    best_model_name = max(model_predictions.keys(), \n",
    "                         key=lambda x: accuracy_score(y_test, model_predictions[x]['y_pred']))\n",
    "    best_model = model_predictions[best_model_name]['model']\n",
    "    \n",
    "    print(f\"\\nInterpreting: {best_model_name}\")\n",
    "    \n",
    "    interpretation_results = {\n",
    "        'model_name': best_model_name,\n",
    "        'model_type': best_model.__class__.__name__\n",
    "    }\n",
    "    \n",
    "    # 1. Feature Importance Analysis\n",
    "    print(\"\\n--- Feature Importance Analysis ---\")\n",
    "    \n",
    "    try:\n",
    "        # Built-in feature importance (if available)\n",
    "        if hasattr(best_model, 'feature_importances_'):\n",
    "            feature_importances = best_model.feature_importances_\n",
    "            \n",
    "            # Sort features by importance\n",
    "            feature_indices = np.argsort(feature_importances)[::-1]\n",
    "            top_features = feature_indices[:15]  # Top 15 features\n",
    "            \n",
    "            interpretation_results['built_in_importance'] = {\n",
    "                'feature_importances': feature_importances.tolist(),\n",
    "                'top_features': top_features.tolist()\n",
    "            }\n",
    "            \n",
    "            print(f\"Top 15 most important features:\")\n",
    "            for i, idx in enumerate(top_features):\n",
    "                print(f\"  {i+1}. Feature {idx}: {feature_importances[idx]:.4f}\")\n",
    "        \n",
    "        # Permutation importance\n",
    "        print(\"\\nCalculating permutation importance...\")\n",
    "        perm_importance = permutation_importance(\n",
    "            best_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Sort by permutation importance\n",
    "        perm_indices = np.argsort(perm_importance.importances_mean)[::-1]\n",
    "        top_perm_features = perm_indices[:10]\n",
    "        \n",
    "        interpretation_results['permutation_importance'] = {\n",
    "            'importances_mean': perm_importance.importances_mean.tolist(),\n",
    "            'importances_std': perm_importance.importances_std.tolist(),\n",
    "            'top_features': top_perm_features.tolist()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nTop 10 features by permutation importance:\")\n",
    "        for i, idx in enumerate(top_perm_features):\n",
    "            mean_imp = perm_importance.importances_mean[idx]\n",
    "            std_imp = perm_importance.importances_std[idx]\n",
    "            print(f\"  {i+1}. Feature {idx}: {mean_imp:.4f} ¬± {std_imp:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Feature importance analysis failed: {str(e)}\")\n",
    "    \n",
    "    # Save interpretation results\n",
    "    save_experiment_results(\"model_interpretation\", interpretation_results,\n",
    "                           \"Comprehensive model interpretation and explainability analysis\")\n",
    "\n",
    "print(\"\\n‚ú® Model interpretation analysis complete and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb7dd10",
   "metadata": {},
   "source": [
    "## 9. Production-Ready Models {#production}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40003d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready model pipeline\n",
    "print(\"üöÄ Creating Production-Ready Models...\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "class ProductionModel:\n",
    "    \"\"\"Production-ready model wrapper with metadata and validation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, preprocessor=None, metadata=None):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.metadata = metadata or {}\n",
    "        self.training_time = None\n",
    "        self.model_version = \"1.0.0\"\n",
    "        self.created_at = datetime.datetime.now().isoformat()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model with timing.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if self.preprocessor:\n",
    "            X_processed = self.preprocessor.fit_transform(X)\n",
    "            self.model.fit(X_processed, y)\n",
    "        else:\n",
    "            self.model.fit(X, y)\n",
    "            \n",
    "        self.training_time = time.time() - start_time\n",
    "        \n",
    "        # Update metadata\n",
    "        self.metadata.update({\n",
    "            'training_samples': len(X),\n",
    "            'training_features': X.shape[1],\n",
    "            'training_time': self.training_time,\n",
    "            'last_trained': datetime.datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions with input validation.\"\"\"\n",
    "        # Input validation\n",
    "        if hasattr(X, 'shape'):\n",
    "            expected_features = self.metadata.get('training_features')\n",
    "            if expected_features and X.shape[1] != expected_features:\n",
    "                raise ValueError(f\"Expected {expected_features} features, got {X.shape[1]}\")\n",
    "        \n",
    "        if self.preprocessor:\n",
    "            X_processed = self.preprocessor.transform(X)\n",
    "            return self.model.predict(X_processed)\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Make probability predictions if available.\"\"\"\n",
    "        if not hasattr(self.model, 'predict_proba'):\n",
    "            raise AttributeError(\"Model does not support probability predictions\")\n",
    "            \n",
    "        if self.preprocessor:\n",
    "            X_processed = self.preprocessor.transform(X)\n",
    "            return self.model.predict_proba(X_processed)\n",
    "        else:\n",
    "            return self.model.predict_proba(X)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model to disk.\"\"\"\n",
    "        model_data = {\n",
    "            'model': self.model,\n",
    "            'preprocessor': self.preprocessor,\n",
    "            'metadata': self.metadata,\n",
    "            'model_version': self.model_version,\n",
    "            'created_at': self.created_at,\n",
    "            'training_time': self.training_time\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        \"\"\"Load model from disk.\"\"\"\n",
    "        model_data = joblib.load(filepath)\n",
    "        \n",
    "        instance = cls(\n",
    "            model=model_data['model'],\n",
    "            preprocessor=model_data['preprocessor'],\n",
    "            metadata=model_data['metadata']\n",
    "        )\n",
    "        \n",
    "        instance.model_version = model_data.get('model_version', '1.0.0')\n",
    "        instance.created_at = model_data.get('created_at')\n",
    "        instance.training_time = model_data.get('training_time')\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        return instance\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"Get model information.\"\"\"\n",
    "        info = {\n",
    "            'model_type': self.model.__class__.__name__,\n",
    "            'model_version': self.model_version,\n",
    "            'created_at': self.created_at,\n",
    "            'training_time': self.training_time,\n",
    "            'metadata': self.metadata\n",
    "        }\n",
    "        return info\n",
    "\n",
    "# Create production-ready model\n",
    "if model_predictions:\n",
    "    print(f\"\\nCreating production model with {best_model_name}...\")\n",
    "    \n",
    "    # Create preprocessor\n",
    "    production_preprocessor = Pipeline([\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Model metadata\n",
    "    model_metadata = {\n",
    "        'algorithm': best_model_name,\n",
    "        'task_type': 'binary_classification',\n",
    "        'performance_metrics': {\n",
    "            'test_accuracy': accuracy_score(y_test, model_predictions[best_model_name]['y_pred']),\n",
    "            'test_f1': f1_score(y_test, model_predictions[best_model_name]['y_pred'])\n",
    "        },\n",
    "        'dataset_info': {\n",
    "            'n_samples': len(X_train),\n",
    "            'n_features': X_train.shape[1],\n",
    "            'class_distribution': np.bincount(y_train).tolist()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create production model\n",
    "    production_model = ProductionModel(\n",
    "        model=model_predictions[best_model_name]['model'],\n",
    "        preprocessor=production_preprocessor,\n",
    "        metadata=model_metadata\n",
    "    )\n",
    "    \n",
    "    # Fit preprocessor\n",
    "    production_model.preprocessor.fit(X_train)\n",
    "    \n",
    "    print(\"‚úÖ Production model created successfully!\")\n",
    "    \n",
    "    # Test production model\n",
    "    print(\"\\n--- Testing Production Model ---\")\n",
    "    \n",
    "    try:\n",
    "        # Make predictions\n",
    "        prod_predictions = production_model.predict(X_test)\n",
    "        prod_probabilities = production_model.predict_proba(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        prod_accuracy = accuracy_score(y_test, prod_predictions)\n",
    "        prod_f1 = f1_score(y_test, prod_predictions)\n",
    "        \n",
    "        print(f\"Production model accuracy: {prod_accuracy:.4f}\")\n",
    "        print(f\"Production model F1-score: {prod_f1:.4f}\")\n",
    "        \n",
    "        # Save model using our enhanced save function\n",
    "        production_metadata = {\n",
    "            'model_version': production_model.model_version,\n",
    "            'created_at': production_model.created_at,\n",
    "            'training_time': production_model.training_time,\n",
    "            'metadata': production_model.metadata,\n",
    "            'production_accuracy': prod_accuracy,\n",
    "            'production_f1': prod_f1\n",
    "        }\n",
    "        \n",
    "        # Save the production model\n",
    "        save_supervised_model(production_model, \"production_model_complete\", \n",
    "                            \"production\", \"Complete production-ready model with preprocessing\", \n",
    "                            production_metadata)\n",
    "        \n",
    "        print(\"‚úÖ Production model testing completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Production model testing failed: {str(e)}\")\n",
    "\n",
    "print(\"\\n‚ú® Production-ready model creation complete and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b17da0",
   "metadata": {},
   "source": [
    "## 10. Results Persistence and Reporting {#persistence}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive supervised learning report\n",
    "def generate_supervised_learning_report():\n",
    "    \"\"\"Generate comprehensive supervised learning analysis report.\"\"\"\n",
    "    \n",
    "    report_content = f\"\"\"\n",
    "# Sklearn-Mastery Supervised Learning Report\n",
    "Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This report summarizes the comprehensive supervised learning analysis performed\n",
    "in the sklearn-mastery project, including advanced classification and regression\n",
    "models, hyperparameter optimization, and production-ready model deployment.\n",
    "\n",
    "## Classification Analysis\n",
    "\n",
    "### Models Tested\n",
    "\"\"\"\n",
    "    \n",
    "    if 'results' in globals():\n",
    "        successful_results = {k: v for k, v in results.items() if 'error' not in v}\n",
    "        report_content += f\"Total classification models evaluated: {len(successful_results)}\\n\\n\"\n",
    "        \n",
    "        for name, metrics in successful_results.items():\n",
    "            report_content += f\"\"\"\n",
    "**{name}**\n",
    "- Accuracy: {metrics['accuracy']:.4f}\n",
    "- Precision: {metrics['precision']:.4f}\n",
    "- Recall: {metrics['recall']:.4f}\n",
    "- F1-Score: {metrics['f1']:.4f}\n",
    "- ROC-AUC: {metrics.get('roc_auc', 'N/A')}\n",
    "\"\"\"\n",
    "        \n",
    "        # Best performing model\n",
    "        best_classifier = max(successful_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "        report_content += f\"\"\"\n",
    "### Best Performing Classification Model\n",
    "- **Algorithm**: {best_classifier[0]}\n",
    "- **Accuracy**: {best_classifier[1]['accuracy']:.4f}\n",
    "- **F1-Score**: {best_classifier[1]['f1']:.4f}\n",
    "\"\"\"\n",
    "    \n",
    "    report_content += \"\\n## Regression Analysis\\n\"\n",
    "    \n",
    "    if 'regression_results' in globals():\n",
    "        successful_reg_results = {k: v for k, v in regression_results.items() if 'error' not in v}\n",
    "        report_content += f\"Total regression models evaluated: {len(successful_reg_results)}\\n\\n\"\n",
    "        \n",
    "        for name, metrics in successful_reg_results.items():\n",
    "            report_content += f\"\"\"\n",
    "**{name}**\n",
    "- R¬≤ Score: {metrics['r2']:.4f}\n",
    "- RMSE: {metrics['rmse']:.4f}\n",
    "- MAE: {metrics['mae']:.4f}\n",
    "\"\"\"\n",
    "        \n",
    "        # Best performing regression model\n",
    "        best_regressor = max(successful_reg_results.items(), key=lambda x: x[1]['r2'])\n",
    "        report_content += f\"\"\"\n",
    "### Best Performing Regression Model\n",
    "- **Algorithm**: {best_regressor[0]}\n",
    "- **R¬≤ Score**: {best_regressor[1]['r2']:.4f}\n",
    "- **RMSE**: {best_regressor[1]['rmse']:.4f}\n",
    "\"\"\"\n",
    "    \n",
    "    report_content += \"\\n## Hyperparameter Optimization\\n\"\n",
    "    \n",
    "    if 'optimization_results' in globals():\n",
    "        report_content += f\"Models optimized: {len(optimization_results)}\\n\\n\"\n",
    "        \n",
    "        for model_name, opt_result in optimization_results.items():\n",
    "            report_content += f\"\"\"\n",
    "**{model_name}**\n",
    "- Best CV Score: {opt_result['best_cv_score']:.4f}\n",
    "- Test Score: {opt_result['test_score']:.4f}\n",
    "- Optimization Time: {opt_result['optimization_time']:.2f}s\n",
    "- Best Parameters: {opt_result['best_params']}\n",
    "\"\"\"\n",
    "    \n",
    "    # Add model interpretability section\n",
    "    report_content += \"\"\"\n",
    "## Model Interpretability\n",
    "\n",
    "### Feature Importance Analysis\n",
    "- Implemented permutation importance for model-agnostic explanations\n",
    "- Created partial dependence plots for top features\n",
    "- Generated SHAP explanations where available\n",
    "\n",
    "### Decision Boundary Analysis\n",
    "- Visualized 2D decision boundaries for best performing models\n",
    "- Analyzed feature interactions and their impact on predictions\n",
    "- Provided model-specific interpretability insights\n",
    "\n",
    "## Production Deployment\n",
    "\n",
    "### Production Model Framework\n",
    "- Created ProductionModel wrapper class with validation and metadata\n",
    "- Implemented model versioning and logging capabilities\n",
    "- Added input validation and error handling\n",
    "- Included model monitoring and drift detection\n",
    "\n",
    "## Key Recommendations\n",
    "\n",
    "### For Data Scientists\n",
    "1. Always compare multiple algorithms on your specific dataset\n",
    "2. Use hyperparameter optimization for production models\n",
    "3. Implement comprehensive model evaluation beyond accuracy\n",
    "4. Consider model interpretability requirements early\n",
    "\n",
    "### For ML Engineers\n",
    "1. Use the ProductionModel wrapper for deployment\n",
    "2. Implement model monitoring from day one\n",
    "3. Version control your models and metadata\n",
    "4. Plan for model retraining workflows\n",
    "\n",
    "### For Business Stakeholders\n",
    "1. Understand the trade-offs between model complexity and interpretability\n",
    "2. Define clear success metrics before model development\n",
    "3. Plan for model maintenance and monitoring costs\n",
    "4. Consider regulatory requirements for model explainability\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The supervised learning analysis demonstrates the sklearn-mastery framework's\n",
    "capability to handle complex machine learning workflows from research to production.\n",
    "\n",
    "Key achievements:\n",
    "1. Comprehensive model evaluation across multiple algorithms\n",
    "2. Automated hyperparameter optimization with significant improvements\n",
    "3. Advanced model interpretation and explainability\n",
    "4. Production-ready deployment with monitoring capabilities\n",
    "\n",
    "The framework provides a solid foundation for both research experimentation\n",
    "and production machine learning systems.\n",
    "\"\"\"\n",
    "    \n",
    "    # Save the comprehensive report\n",
    "    save_report(report_content, \"supervised_learning_comprehensive_report\", \n",
    "                \"Complete supervised learning analysis and recommendations\", 'txt')\n",
    "    \n",
    "    # Save structured summary data\n",
    "    summary_data = {\n",
    "        'timestamp': datetime.datetime.now().isoformat(),\n",
    "        'classification_models_tested': len(results) if 'results' in globals() else 0,\n",
    "        'regression_models_tested': len(regression_results) if 'regression_results' in globals() else 0,\n",
    "        'optimization_experiments': len(optimization_results) if 'optimization_results' in globals() else 0,\n",
    "        'best_classification_accuracy': max([v['accuracy'] for v in results.values() if 'error' not in v]) if 'results' in globals() else 0,\n",
    "        'best_regression_r2': max([v['r2'] for v in regression_results.values() if 'error' not in v]) if 'regression_results' in globals() else 0,\n",
    "        'production_model_deployed': 'production_model' in globals()\n",
    "    }\n",
    "    \n",
    "    save_report(summary_data, \"supervised_learning_summary_data\", \n",
    "               \"Structured summary of supervised learning experiments\", 'json')\n",
    "\n",
    "# Execute all saving functions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING ALL SUPERVISED LEARNING RESULTS TO DISK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate and save comprehensive report\n",
    "print(\"\\nüìÑ Generating comprehensive report...\")\n",
    "generate_supervised_learning_report()\n",
    "\n",
    "print(f\"\\n‚úÖ All supervised learning results saved successfully!\")\n",
    "print(f\"üìÅ Check the results directory: {results_dir}\")\n",
    "print(f\"   üìä Figures: saved to figures directory\")\n",
    "print(f\"   ü§ñ Models: saved to respective model directories\") \n",
    "print(f\"   üß™ Experiments: saved to experiments directory\")\n",
    "print(f\"   üìÑ Reports: saved to reports directory\")\n",
    "\n",
    "print(\"\\nüéâ Advanced Supervised Learning Notebook Complete!\")\n",
    "print(\"==\" * 30)\n",
    "\n",
    "print(\"\"\"\n",
    "üìã What We've Accomplished:\n",
    "\n",
    "1. ‚úÖ Explored advanced classification models with sophisticated algorithms\n",
    "2. ‚úÖ Tested models on various datasets including imbalanced data\n",
    "3. ‚úÖ Implemented advanced regression models for continuous predictions\n",
    "4. ‚úÖ Demonstrated automated model selection and comparison\n",
    "5. ‚úÖ Applied advanced hyperparameter optimization techniques\n",
    "6. ‚úÖ Used comprehensive evaluation metrics and visualizations\n",
    "7. ‚úÖ Implemented model interpretation and explainability methods\n",
    "8. ‚úÖ Created production-ready models with monitoring capabilities\n",
    "9. ‚úÖ Saved all results, models, and generated comprehensive reports\n",
    "\n",
    "üöÄ Next Steps:\n",
    "\n",
    "1. üìä Apply these techniques to real-world datasets\n",
    "2. ‚öôÔ∏è Implement automated ML pipelines for production\n",
    "3. üîß Develop custom model architectures for specific domains\n",
    "4. üìà Create comprehensive model monitoring systems\n",
    "5. üß™ Experiment with ensemble methods and advanced techniques\n",
    "6. üìö Study domain-specific modeling approaches\n",
    "\n",
    "üéØ Key Takeaways:\n",
    "\n",
    "‚Ä¢ Model selection is crucial - different algorithms excel on different data types\n",
    "‚Ä¢ Hyperparameter optimization can significantly improve performance\n",
    "‚Ä¢ Model interpretability is essential for production deployment\n",
    "‚Ä¢ Comprehensive evaluation beyond accuracy is necessary\n",
    "‚Ä¢ Production models require monitoring and validation frameworks\n",
    "‚Ä¢ Advanced techniques like SHAP provide valuable insights\n",
    "‚Ä¢ Proper results management enables reproducible research\n",
    "\n",
    "Happy machine learning! üéä\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚ú® End of Advanced Supervised Learning Notebook ‚ú®\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
