{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a81fd6c",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning Techniques\n",
    "\n",
    "This notebook explores cutting-edge machine learning techniques and advanced methodologies available in the sklearn-mastery project, including neural architecture search, meta-learning, active learning, and interpretable machine learning.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Neural Network Optimization](#neural-networks)\n",
    "3. [Meta-Learning and Few-Shot Learning](#meta-learning)\n",
    "4. [Active Learning Strategies](#active-learning)\n",
    "5. [Interpretable Machine Learning](#interpretability)\n",
    "6. [Uncertainty Quantification](#uncertainty)\n",
    "7. [Advanced Ensemble Methods](#ensembles)\n",
    "8. [Automated Feature Engineering](#feature-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b72f71",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fadf70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced ML imports\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "import joblib\n",
    "\n",
    "# Results saving imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6efcad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.generators import SyntheticDataGenerator\n",
    "from models.supervised.classification import AdvancedClassifier\n",
    "from evaluation.metrics import ModelEvaluator\n",
    "from evaluation.visualization import ModelVisualizationSuite\n",
    "from utils.helpers import performance_timer\n",
    "from utils.decorators import memory_profiler\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e1a2a",
   "metadata": {},
   "source": [
    "### Results Management Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb629590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results saving setup for advanced techniques\n",
    "def setup_results_directories():\n",
    "    \"\"\"Create results directory structure if it doesn't exist.\"\"\"\n",
    "    base_dir = Path('../results')\n",
    "    directories = [\n",
    "        base_dir / 'figures',\n",
    "        base_dir / 'models',\n",
    "        base_dir / 'advanced_models',  # Specific for advanced technique models\n",
    "        base_dir / 'neural_architectures',  # Neural architecture search results\n",
    "        base_dir / 'meta_learning',  # Meta-learning models and results\n",
    "        base_dir / 'active_learning',  # Active learning experiments\n",
    "        base_dir / 'interpretability',  # Interpretability analysis\n",
    "        base_dir / 'uncertainty',  # Uncertainty quantification results\n",
    "        base_dir / 'pipelines',\n",
    "        base_dir / 'experiments',\n",
    "        base_dir / 'reports'\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"üìÅ Created/verified directory: {directory}\")\n",
    "    \n",
    "    return base_dir\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\"Get current timestamp for file naming.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def save_figure(fig, name, description=\"\", category=\"general\", dpi=300):\n",
    "    \"\"\"Save figure with proper naming and metadata.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_advanced_{category}_{name}.png\"\n",
    "    filepath = results_dir / 'figures' / filename\n",
    "    \n",
    "    # Save figure\n",
    "    fig.savefig(filepath, dpi=dpi, bbox_inches='tight', facecolor='white')\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'filename': filename,\n",
    "        'description': description,\n",
    "        'category': category,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '07_advanced_techniques',\n",
    "        'dpi': dpi\n",
    "    }\n",
    "    \n",
    "    metadata_file = filepath.with_suffix('.json')\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Saved figure: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_advanced_model(model, name, description=\"\", technique_type=\"general\", performance_metrics=None):\n",
    "    \"\"\"Save advanced technique model with comprehensive metadata.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_{technique_type}_{name}.joblib\"\n",
    "    \n",
    "    # Choose appropriate directory based on technique type\n",
    "    if technique_type == 'neural_architecture':\n",
    "        filepath = results_dir / 'neural_architectures' / filename\n",
    "    elif technique_type == 'meta_learning':\n",
    "        filepath = results_dir / 'meta_learning' / filename\n",
    "    elif technique_type == 'active_learning':\n",
    "        filepath = results_dir / 'active_learning' / filename\n",
    "    elif technique_type == 'interpretability':\n",
    "        filepath = results_dir / 'interpretability' / filename\n",
    "    elif technique_type == 'uncertainty':\n",
    "        filepath = results_dir / 'uncertainty' / filename\n",
    "    else:\n",
    "        filepath = results_dir / 'advanced_models' / filename\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, filepath, compress=3)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'filename': filename,\n",
    "        'model_name': name,\n",
    "        'description': description,\n",
    "        'technique_type': technique_type,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '07_advanced_techniques',\n",
    "        'model_type': type(model).__name__,\n",
    "        'performance_metrics': performance_metrics or {},\n",
    "        'file_size_mb': filepath.stat().st_size / (1024*1024) if filepath.exists() else 0\n",
    "    }\n",
    "    \n",
    "    metadata_file = filepath.with_suffix('.json')\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üíæ Saved advanced model: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_experiment_results(experiment_name, results, description=\"\", technique_type=\"general\"):\n",
    "    \"\"\"Save experiment results with detailed configuration.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_{technique_type}_{experiment_name}.json\"\n",
    "    filepath = results_dir / 'experiments' / filename\n",
    "    \n",
    "    experiment_data = {\n",
    "        'experiment_name': experiment_name,\n",
    "        'description': description,\n",
    "        'technique_type': technique_type,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '07_advanced_techniques',\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(experiment_data, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üíæ Saved experiment results: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_technique_report(content, report_name, technique_type=\"general\", format='txt'):\n",
    "    \"\"\"Save comprehensive technique analysis report.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_{technique_type}_report_{report_name}.{format}\"\n",
    "    filepath = results_dir / 'reports' / filename\n",
    "    \n",
    "    if format == 'txt':\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(content)\n",
    "    elif format == 'json':\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(content, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üíæ Saved report: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "# Initialize results directories\n",
    "results_dir = setup_results_directories()\n",
    "print(f\"üìä Results will be saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745d757",
   "metadata": {},
   "source": [
    "## 2. Neural Network Optimization {#neural-networks}\n",
    "\n",
    "Advanced neural network optimization techniques including architecture search and automated hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5226939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architecture optimization\n",
    "print(\"üß† Neural Network Architecture Optimization...\")\n",
    "\n",
    "class NeuralArchitectureSearch:\n",
    "    \"\"\"Automated Neural Architecture Search for MLPs.\"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.results = []\n",
    "        self.best_architecture = None\n",
    "        \n",
    "    def generate_architecture(self):\n",
    "        \"\"\"Generate a random neural network architecture.\"\"\"\n",
    "        # Number of hidden layers (1-4)\n",
    "        n_layers = np.random.randint(1, 5)\n",
    "        \n",
    "        # Layer sizes\n",
    "        input_size = self.X_train.shape[1]\n",
    "        hidden_layers = []\n",
    "        \n",
    "        current_size = input_size\n",
    "        for i in range(n_layers):\n",
    "            # Each layer can be 10-500 neurons\n",
    "            layer_size = np.random.randint(10, min(500, current_size * 2))\n",
    "            hidden_layers.append(layer_size)\n",
    "            current_size = layer_size\n",
    "        \n",
    "        # Activation function\n",
    "        activation = np.random.choice(['relu', 'tanh', 'logistic'])\n",
    "        \n",
    "        # Learning rate\n",
    "        learning_rate = 10 ** np.random.uniform(-4, -1)\n",
    "        \n",
    "        # Alpha (L2 regularization)\n",
    "        alpha = 10 ** np.random.uniform(-6, -1)\n",
    "        \n",
    "        # Solver\n",
    "        solver = np.random.choice(['adam', 'lbfgs'])\n",
    "        \n",
    "        return {\n",
    "            'hidden_layer_sizes': tuple(hidden_layers),\n",
    "            'activation': activation,\n",
    "            'learning_rate_init': learning_rate,\n",
    "            'alpha': alpha,\n",
    "            'solver': solver,\n",
    "            'max_iter': 1000,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    \n",
    "    def evaluate_architecture(self, architecture):\n",
    "        \"\"\"Evaluate a neural network architecture.\"\"\"\n",
    "        try:\n",
    "            # Create and train model\n",
    "            model = MLPClassifier(**architecture)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate performance\n",
    "            train_score = model.score(self.X_train, self.y_train)\n",
    "            val_score = model.score(self.X_val, self.y_val)\n",
    "            \n",
    "            # Calculate complexity score\n",
    "            total_params = sum([\n",
    "                self.X_train.shape[1] * architecture['hidden_layer_sizes'][0]\n",
    "            ] + [\n",
    "                architecture['hidden_layer_sizes'][i] * architecture['hidden_layer_sizes'][i+1]\n",
    "                for i in range(len(architecture['hidden_layer_sizes'])-1)\n",
    "            ] + [\n",
    "                architecture['hidden_layer_sizes'][-1] * len(np.unique(self.y_train))\n",
    "            ])\n",
    "            \n",
    "            return {\n",
    "                'architecture': architecture,\n",
    "                'train_score': train_score,\n",
    "                'val_score': val_score,\n",
    "                'training_time': training_time,\n",
    "                'n_params': total_params,\n",
    "                'n_layers': len(architecture['hidden_layer_sizes']),\n",
    "                'converged': model.n_iter_ < model.max_iter\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def search(self, n_trials=50):\n",
    "        \"\"\"Perform neural architecture search.\"\"\"\n",
    "        print(f\"Running Neural Architecture Search with {n_trials} trials...\")\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            if trial % 10 == 0:\n",
    "                print(f\"  Trial {trial}/{n_trials}\")\n",
    "            \n",
    "            architecture = self.generate_architecture()\n",
    "            result = self.evaluate_architecture(architecture)\n",
    "            \n",
    "            if result:\n",
    "                self.results.append(result)\n",
    "        \n",
    "        # Find best architecture\n",
    "        if self.results:\n",
    "            # Sort by validation score\n",
    "            self.results.sort(key=lambda x: x['val_score'], reverse=True)\n",
    "            self.best_architecture = self.results[0]\n",
    "            \n",
    "            print(f\"\\nüèÜ Best Architecture Found:\")\n",
    "            print(f\"  Validation Score: {self.best_architecture['val_score']:.4f}\")\n",
    "            print(f\"  Architecture: {self.best_architecture['architecture']['hidden_layer_sizes']}\")\n",
    "            print(f\"  Activation: {self.best_architecture['architecture']['activation']}\")\n",
    "            print(f\"  Learning Rate: {self.best_architecture['architecture']['learning_rate_init']:.6f}\")\n",
    "            print(f\"  Parameters: {self.best_architecture['n_params']}\")\n",
    "            print(f\"  Training Time: {self.best_architecture['training_time']:.2f}s\")\n",
    "            \n",
    "            return self.best_architecture\n",
    "        else:\n",
    "            print(\"No valid architectures found!\")\n",
    "            return None\n",
    "\n",
    "# Generate data for neural architecture search\n",
    "generator = SyntheticDataGenerator(random_state=42)\n",
    "X_nas, y_nas = generator.classification_dataset(\n",
    "    n_samples=1500,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_classes=3,\n",
    "    class_sep=0.8\n",
    ")\n",
    "\n",
    "# Split data for NAS\n",
    "X_train_nas, X_temp, y_train_nas, y_temp = train_test_split(\n",
    "    X_nas, y_nas, test_size=0.4, random_state=42, stratify=y_nas\n",
    ")\n",
    "X_val_nas, X_test_nas, y_val_nas, y_test_nas = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"NAS Dataset - Train: {X_train_nas.shape}, Val: {X_val_nas.shape}, Test: {X_test_nas.shape}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_nas_scaled = scaler.fit_transform(X_train_nas)\n",
    "X_val_nas_scaled = scaler.transform(X_val_nas)\n",
    "X_test_nas_scaled = scaler.transform(X_test_nas)\n",
    "\n",
    "# Run Neural Architecture Search\n",
    "nas = NeuralArchitectureSearch(X_train_nas_scaled, y_train_nas, X_val_nas_scaled, y_val_nas)\n",
    "best_arch = nas.search(n_trials=30)\n",
    "\n",
    "if best_arch:\n",
    "    # Train final model with best architecture\n",
    "    final_model = MLPClassifier(**best_arch['architecture'])\n",
    "    final_model.fit(X_train_nas_scaled, y_train_nas)\n",
    "    \n",
    "    # Test final model\n",
    "    test_score = final_model.score(X_test_nas_scaled, y_test_nas)\n",
    "    print(f\"\\nFinal Test Score: {test_score:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    nas_results = {\n",
    "        'best_architecture': best_arch['architecture'],\n",
    "        'val_score': best_arch['val_score'],\n",
    "        'test_score': test_score,\n",
    "        'n_params': best_arch['n_params'],\n",
    "        'training_time': best_arch['training_time'],\n",
    "        'search_trials': len(nas.results)\n",
    "    }\n",
    "    \n",
    "    save_advanced_model(final_model, 'best_neural_architecture', \n",
    "                       'Best neural network found through architecture search',\n",
    "                       'neural_architecture', nas_results)\n",
    "    \n",
    "    save_experiment_results('neural_architecture_search', nas_results,\n",
    "                           'Results from automated neural architecture search', 'neural_architecture')\n",
    "\n",
    "print(\"\\n‚ú® Neural Architecture Search complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5abdc0",
   "metadata": {},
   "source": [
    "### Neural Architecture Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85334a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze neural architecture search results\n",
    "print(\"üìä Analyzing Neural Architecture Search Results...\")\n",
    "\n",
    "if 'nas' in locals() and nas.results:\n",
    "    # Extract data for analysis\n",
    "    val_scores = [r['val_score'] for r in nas.results]\n",
    "    train_scores = [r['train_score'] for r in nas.results]\n",
    "    n_params = [r['n_params'] for r in nas.results]\n",
    "    n_layers = [r['n_layers'] for r in nas.results]\n",
    "    training_times = [r['training_time'] for r in nas.results]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Validation score distribution\n",
    "    axes[0, 0].hist(val_scores, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].axvline(np.mean(val_scores), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(val_scores):.3f}')\n",
    "    axes[0, 0].axvline(max(val_scores), color='green', linestyle='--', \n",
    "                      label=f'Best: {max(val_scores):.3f}')\n",
    "    axes[0, 0].set_xlabel('Validation Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Distribution of Validation Scores')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Performance vs Complexity\n",
    "    scatter = axes[0, 1].scatter(n_params, val_scores, c=training_times, \n",
    "                                cmap='viridis', alpha=0.7, s=50)\n",
    "    axes[0, 1].set_xlabel('Number of Parameters')\n",
    "    axes[0, 1].set_ylabel('Validation Score')\n",
    "    axes[0, 1].set_title('Performance vs Model Complexity')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=axes[0, 1])\n",
    "    cbar.set_label('Training Time (s)')\n",
    "    \n",
    "    # 3. Architecture depth analysis\n",
    "    depth_performance = {}\n",
    "    for depth in set(n_layers):\n",
    "        depth_scores = [val_scores[i] for i, d in enumerate(n_layers) if d == depth]\n",
    "        if depth_scores:\n",
    "            depth_performance[depth] = {\n",
    "                'mean': np.mean(depth_scores),\n",
    "                'std': np.std(depth_scores),\n",
    "                'count': len(depth_scores)\n",
    "            }\n",
    "    \n",
    "    depths = sorted(depth_performance.keys())\n",
    "    means = [depth_performance[d]['mean'] for d in depths]\n",
    "    stds = [depth_performance[d]['std'] for d in depths]\n",
    "    counts = [depth_performance[d]['count'] for d in depths]\n",
    "    \n",
    "    bars = axes[1, 0].bar(depths, means, yerr=stds, capsize=5, alpha=0.7, \n",
    "                         color='lightgreen')\n",
    "    axes[1, 0].set_xlabel('Number of Hidden Layers')\n",
    "    axes[1, 0].set_ylabel('Average Validation Score')\n",
    "    axes[1, 0].set_title('Performance by Architecture Depth')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add count labels\n",
    "    for bar, count in zip(bars, counts):\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                       f'n={count}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Training efficiency\n",
    "    axes[1, 1].scatter(training_times, val_scores, alpha=0.7, color='coral')\n",
    "    axes[1, 1].set_xlabel('Training Time (seconds)')\n",
    "    axes[1, 1].set_ylabel('Validation Score')\n",
    "    axes[1, 1].set_title('Training Efficiency Analysis')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(training_times, val_scores, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[1, 1].plot(sorted(training_times), p(sorted(training_times)), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save NAS analysis figure\n",
    "    save_figure(fig, 'neural_architecture_search_analysis',\n",
    "               'Comprehensive analysis of neural architecture search results', 'neural_architecture')\n",
    "    plt.show()\n",
    "    \n",
    "    # Architecture summary\n",
    "    print(f\"\\nüìä Neural Architecture Search Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total architectures evaluated: {len(nas.results)}\")\n",
    "    print(f\"Best validation score: {max(val_scores):.4f}\")\n",
    "    print(f\"Average validation score: {np.mean(val_scores):.4f} ¬± {np.std(val_scores):.4f}\")\n",
    "    print(f\"Best architecture depth: {nas.best_architecture['n_layers']} layers\")\n",
    "    print(f\"Best architecture size: {nas.best_architecture['architecture']['hidden_layer_sizes']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n‚ú® Neural architecture analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d13dd",
   "metadata": {},
   "source": [
    "## 3. Meta-Learning and Few-Shot Learning {#meta-learning}\n",
    "\n",
    "Implementing meta-learning approaches for rapid adaptation to new tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac496d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-learning implementation\n",
    "print(\"üéØ Meta-Learning and Few-Shot Learning...\")\n",
    "\n",
    "class ModelAgnosticMetaLearning:\n",
    "    \"\"\"Model-Agnostic Meta-Learning (MAML) implementation for classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_class, meta_lr=0.01, inner_lr=0.1, inner_steps=5):\n",
    "        self.base_model_class = base_model_class\n",
    "        self.meta_lr = meta_lr\n",
    "        self.inner_lr = inner_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.meta_model = None\n",
    "        self.task_history = []\n",
    "    \n",
    "    def create_base_model(self, **kwargs):\n",
    "        \"\"\"Create a base model instance.\"\"\"\n",
    "        return self.base_model_class(**kwargs)\n",
    "    \n",
    "    def inner_loop_update(self, model, X_support, y_support, X_query, y_query):\n",
    "        \"\"\"Perform inner loop optimization for a single task.\"\"\"\n",
    "        # Simulate gradient steps (simplified for sklearn models)\n",
    "        # In practice, this would involve actual gradient computation\n",
    "        \n",
    "        inner_model = self.create_base_model(random_state=42)\n",
    "        \n",
    "        # Train on support set\n",
    "        inner_model.fit(X_support, y_support)\n",
    "        \n",
    "        # Evaluate on query set\n",
    "        query_score = inner_model.score(X_query, y_query)\n",
    "        \n",
    "        return inner_model, query_score\n",
    "    \n",
    "    def meta_train(self, task_generator, n_episodes=100, k_shot=5, n_query=10):\n",
    "        \"\"\"Meta-training phase.\"\"\"\n",
    "        print(f\"Meta-training with {n_episodes} episodes, {k_shot}-shot learning...\")\n",
    "        \n",
    "        episode_scores = []\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            if episode % 20 == 0:\n",
    "                print(f\"  Episode {episode}/{n_episodes}\")\n",
    "            \n",
    "            # Generate a task (dataset)\n",
    "            X_task, y_task = task_generator()\n",
    "            \n",
    "            # Create support and query sets\n",
    "            X_support, X_query, y_support, y_query = train_test_split(\n",
    "                X_task, y_task, \n",
    "                train_size=k_shot * len(np.unique(y_task)),\n",
    "                random_state=episode,\n",
    "                stratify=y_task\n",
    "            )\n",
    "            \n",
    "            # Limit query set size\n",
    "            if len(X_query) > n_query:\n",
    "                X_query = X_query[:n_query]\n",
    "                y_query = y_query[:n_query]\n",
    "            \n",
    "            # Perform inner loop update\n",
    "            task_model, query_score = self.inner_loop_update(\n",
    "                None, X_support, y_support, X_query, y_query\n",
    "            )\n",
    "            \n",
    "            episode_scores.append(query_score)\n",
    "            \n",
    "            # Store task information\n",
    "            self.task_history.append({\n",
    "                'episode': episode,\n",
    "                'query_score': query_score,\n",
    "                'support_size': len(X_support),\n",
    "                'query_size': len(X_query),\n",
    "                'n_classes': len(np.unique(y_task))\n",
    "            })\n",
    "        \n",
    "        self.meta_model = self.create_base_model(random_state=42)\n",
    "        \n",
    "        print(f\"Meta-training complete. Average query score: {np.mean(episode_scores):.4f}\")\n",
    "        return np.mean(episode_scores)\n",
    "    \n",
    "    def meta_test(self, X_test, y_test, k_shot=5):\n",
    "        \"\"\"Meta-testing on a new task.\"\"\"\n",
    "        # Create support set from test data\n",
    "        support_indices = []\n",
    "        for class_label in np.unique(y_test):\n",
    "            class_indices = np.where(y_test == class_label)[0]\n",
    "            selected_indices = np.random.choice(class_indices, \n",
    "                                              min(k_shot, len(class_indices)), \n",
    "                                              replace=False)\n",
    "            support_indices.extend(selected_indices)\n",
    "        \n",
    "        # Remaining data as query set\n",
    "        query_indices = [i for i in range(len(X_test)) if i not in support_indices]\n",
    "        \n",
    "        X_support = X_test[support_indices]\n",
    "        y_support = y_test[support_indices]\n",
    "        X_query = X_test[query_indices]\n",
    "        y_query = y_test[query_indices]\n",
    "        \n",
    "        # Adapt to new task\n",
    "        adapted_model = self.create_base_model(random_state=42)\n",
    "        adapted_model.fit(X_support, y_support)\n",
    "        \n",
    "        # Evaluate on query set\n",
    "        query_score = adapted_model.score(X_query, y_query)\n",
    "        \n",
    "        return adapted_model, query_score, len(X_support), len(X_query)\n",
    "\n",
    "class ProtoTypicalNetworks:\n",
    "    \"\"\"Prototypical Networks for few-shot learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=10):\n",
    "        self.n_components = n_components\n",
    "        self.prototypes = {}\n",
    "        self.embedding_model = None\n",
    "    \n",
    "    def learn_embedding(self, X_train, y_train):\n",
    "        \"\"\"Learn embedding space using PCA (simplified).\"\"\"\n",
    "        self.embedding_model = PCA(n_components=self.n_components, random_state=42)\n",
    "        self.embedding_model.fit(X_train)\n",
    "        return self.embedding_model.transform(X_train)\n",
    "    \n",
    "    def compute_prototypes(self, X_support, y_support):\n",
    "        \"\"\"Compute class prototypes from support set.\"\"\"\n",
    "        X_embedded = self.embedding_model.transform(X_support)\n",
    "        prototypes = {}\n",
    "        \n",
    "        for class_label in np.unique(y_support):\n",
    "            class_mask = y_support == class_label\n",
    "            class_embeddings = X_embedded[class_mask]\n",
    "            prototype = np.mean(class_embeddings, axis=0)\n",
    "            prototypes[class_label] = prototype\n",
    "        \n",
    "        return prototypes\n",
    "    \n",
    "    def classify_query(self, X_query, prototypes):\n",
    "        \"\"\"Classify query points using prototypes.\"\"\"\n",
    "        X_query_embedded = self.embedding_model.transform(X_query)\n",
    "        predictions = []\n",
    "        \n",
    "        for query_point in X_query_embedded:\n",
    "            distances = {}\n",
    "            for class_label, prototype in prototypes.items():\n",
    "                distance = np.linalg.norm(query_point - prototype)\n",
    "                distances[class_label] = distance\n",
    "            \n",
    "            # Predict class with minimum distance\n",
    "            predicted_class = min(distances, key=distances.get)\n",
    "            predictions.append(predicted_class)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def few_shot_learning(self, X_support, y_support, X_query, y_query):\n",
    "        \"\"\"Perform few-shot learning on a task.\"\"\"\n",
    "        # Compute prototypes\n",
    "        prototypes = self.compute_prototypes(X_support, y_support)\n",
    "        \n",
    "        # Classify query points\n",
    "        predictions = self.classify_query(X_query, prototypes)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_query, predictions)\n",
    "        \n",
    "        return predictions, accuracy\n",
    "\n",
    "# Task generator for meta-learning\n",
    "def generate_classification_task():\n",
    "    \"\"\"Generate a random classification task.\"\"\"\n",
    "    generator = SyntheticDataGenerator()\n",
    "    \n",
    "    # Random task parameters\n",
    "    n_samples = np.random.randint(100, 300)\n",
    "    n_features = np.random.randint(10, 25)\n",
    "    n_classes = np.random.randint(2, 5)\n",
    "    \n",
    "    X, y = generator.classification_dataset(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_classes=n_classes,\n",
    "        n_informative=min(n_features-2, n_features),\n",
    "        class_sep=np.random.uniform(0.5, 1.5),\n",
    "        random_state=np.random.randint(0, 1000)\n",
    "    )\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Test Meta-Learning approaches\n",
    "print(\"\\n--- Testing Model-Agnostic Meta-Learning ---\")\n",
    "\n",
    "# Initialize MAML\n",
    "maml = ModelAgnosticMetaLearning(\n",
    "    base_model_class=LogisticRegression,\n",
    "    meta_lr=0.01,\n",
    "    inner_lr=0.1,\n",
    "    inner_steps=5\n",
    ")\n",
    "\n",
    "# Meta-training\n",
    "meta_score = maml.meta_train(\n",
    "    task_generator=generate_classification_task,\n",
    "    n_episodes=50,\n",
    "    k_shot=3,\n",
    "    n_query=15\n",
    ")\n",
    "\n",
    "# Generate test task\n",
    "X_test_task, y_test_task = generate_classification_task()\n",
    "scaler_test = StandardScaler()\n",
    "X_test_task_scaled = scaler_test.fit_transform(X_test_task)\n",
    "\n",
    "# Meta-testing\n",
    "adapted_model, test_score, n_support, n_query = maml.meta_test(\n",
    "    X_test_task_scaled, y_test_task, k_shot=3\n",
    ")\n",
    "\n",
    "print(f\"Meta-test results:\")\n",
    "print(f\"  Support set size: {n_support}\")\n",
    "print(f\"  Query set size: {n_query}\")\n",
    "print(f\"  Query accuracy: {test_score:.4f}\")\n",
    "\n",
    "# Test Prototypical Networks\n",
    "print(\"\\n--- Testing Prototypical Networks ---\")\n",
    "\n",
    "# Generate training data for embedding learning\n",
    "X_embed_train, y_embed_train = generator.classification_dataset(\n",
    "    n_samples=1000, n_features=20, n_classes=5\n",
    ")\n",
    "X_embed_train_scaled = StandardScaler().fit_transform(X_embed_train)\n",
    "\n",
    "# Initialize Prototypical Networks\n",
    "proto_net = ProtoTypicalNetworks(n_components=10)\n",
    "\n",
    "# Learn embedding\n",
    "X_embedded = proto_net.learn_embedding(X_embed_train_scaled, y_embed_train)\n",
    "\n",
    "# Test few-shot learning\n",
    "few_shot_results = []\n",
    "for trial in range(5):\n",
    "    # Generate new task\n",
    "    X_fs_task, y_fs_task = generate_classification_task()\n",
    "    X_fs_task_scaled = StandardScaler().fit_transform(X_fs_task)\n",
    "    \n",
    "    # Create support and query sets\n",
    "    X_support, X_query, y_support, y_query = train_test_split(\n",
    "        X_fs_task_scaled, y_fs_task, \n",
    "        train_size=3 * len(np.unique(y_fs_task)),\n",
    "        random_state=trial,\n",
    "        stratify=y_fs_task\n",
    "    )\n",
    "    \n",
    "    # Few-shot learning\n",
    "    predictions, accuracy = proto_net.few_shot_learning(\n",
    "        X_support, y_support, X_query, y_query\n",
    "    )\n",
    "    \n",
    "    few_shot_results.append(accuracy)\n",
    "    print(f\"  Trial {trial+1}: {accuracy:.4f}\")\n",
    "\n",
    "avg_few_shot_accuracy = np.mean(few_shot_results)\n",
    "print(f\"Average few-shot accuracy: {avg_few_shot_accuracy:.4f}\")\n",
    "\n",
    "# Save meta-learning results\n",
    "meta_learning_results = {\n",
    "    'maml_meta_score': meta_score,\n",
    "    'maml_test_score': test_score,\n",
    "    'prototypical_avg_accuracy': avg_few_shot_accuracy,\n",
    "    'prototypical_results': few_shot_results,\n",
    "    'n_meta_episodes': len(maml.task_history),\n",
    "    'embedding_dimensions': proto_net.n_components\n",
    "}\n",
    "\n",
    "save_experiment_results('meta_learning_comparison', meta_learning_results,\n",
    "                       'Comparison of meta-learning approaches for few-shot learning', 'meta_learning')\n",
    "\n",
    "print(\"\\n‚ú® Meta-learning experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5496e5",
   "metadata": {},
   "source": [
    "### Meta-Learning Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc49e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize meta-learning results\n",
    "print(\"üìä Visualizing Meta-Learning Results...\")\n",
    "\n",
    "if 'maml' in locals() and maml.task_history:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. MAML learning curve\n",
    "    episodes = [task['episode'] for task in maml.task_history]\n",
    "    scores = [task['query_score'] for task in maml.task_history]\n",
    "    \n",
    "    # Running average\n",
    "    window_size = 10\n",
    "    if len(scores) >= window_size:\n",
    "        running_avg = np.convolve(scores, np.ones(window_size)/window_size, mode='valid')\n",
    "        running_episodes = episodes[window_size-1:]\n",
    "        \n",
    "        axes[0, 0].plot(episodes, scores, alpha=0.3, color='blue', label='Episode Score')\n",
    "        axes[0, 0].plot(running_episodes, running_avg, color='red', linewidth=2, \n",
    "                       label=f'Running Average ({window_size})')\n",
    "    else:\n",
    "        axes[0, 0].plot(episodes, scores, color='blue', label='Episode Score')\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Query Accuracy')\n",
    "    axes[0, 0].set_title('MAML Learning Progress')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Task complexity analysis\n",
    "    support_sizes = [task['support_size'] for task in maml.task_history]\n",
    "    task_scores = [task['query_score'] for task in maml.task_history]\n",
    "    \n",
    "    axes[0, 1].scatter(support_sizes, task_scores, alpha=0.6, color='green')\n",
    "    axes[0, 1].set_xlabel('Support Set Size')\n",
    "    axes[0, 1].set_ylabel('Query Accuracy')\n",
    "    axes[0, 1].set_title('Performance vs Task Complexity')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    if len(support_sizes) > 1:\n",
    "        z = np.polyfit(support_sizes, task_scores, 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[0, 1].plot(sorted(support_sizes), p(sorted(support_sizes)), \"r--\", alpha=0.8)\n",
    "    \n",
    "    # 3. Few-shot learning comparison\n",
    "    if 'few_shot_results' in locals():\n",
    "        methods = ['MAML', 'Prototypical Networks']\n",
    "        scores = [test_score, avg_few_shot_accuracy]\n",
    "        colors = ['lightblue', 'lightcoral']\n",
    "        \n",
    "        bars = axes[1, 0].bar(methods, scores, color=colors, alpha=0.7)\n",
    "        axes[1, 0].set_ylabel('Accuracy')\n",
    "        axes[1, 0].set_title('Few-Shot Learning Comparison')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars, scores):\n",
    "            axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                           f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Prototypical Networks trial results\n",
    "    if 'few_shot_results' in locals():\n",
    "        axes[1, 1].plot(range(1, len(few_shot_results)+1), few_shot_results, \n",
    "                       'o-', color='orange', linewidth=2, markersize=8)\n",
    "        axes[1, 1].axhline(y=avg_few_shot_accuracy, color='red', linestyle='--', \n",
    "                          label=f'Average: {avg_few_shot_accuracy:.3f}')\n",
    "        axes[1, 1].set_xlabel('Trial')\n",
    "        axes[1, 1].set_ylabel('Accuracy')\n",
    "        axes[1, 1].set_title('Prototypical Networks - Trial Results')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save meta-learning visualization\n",
    "    save_figure(fig, 'meta_learning_analysis',\n",
    "               'Comprehensive analysis of meta-learning approaches', 'meta_learning')\n",
    "    plt.show()\n",
    "    \n",
    "    # Meta-learning summary\n",
    "    print(f\"\\nüìä Meta-Learning Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"MAML Episodes: {len(maml.task_history)}\")\n",
    "    print(f\"MAML Meta-Training Score: {meta_score:.4f}\")\n",
    "    print(f\"MAML Test Score: {test_score:.4f}\")\n",
    "    print(f\"Prototypical Networks Score: {avg_few_shot_accuracy:.4f}\")\n",
    "    print(f\"Best Method: {'MAML' if test_score > avg_few_shot_accuracy else 'Prototypical Networks'}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n‚ú® Meta-learning visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae91f1",
   "metadata": {},
   "source": [
    "## 4. Active Learning Strategies {#active-learning}\n",
    "\n",
    "Implementing active learning for efficient data labeling and model improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a90ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active learning implementation\n",
    "print(\"üéØ Active Learning Strategies...\")\n",
    "\n",
    "class ActiveLearningStrategy:\n",
    "    \"\"\"Base class for active learning strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_labeled_size=10):\n",
    "        self.initial_labeled_size = initial_labeled_size\n",
    "        self.labeled_indices = []\n",
    "        self.unlabeled_indices = []\n",
    "        self.query_history = []\n",
    "    \n",
    "    def initialize(self, X, y):\n",
    "        \"\"\"Initialize with a small labeled set.\"\"\"\n",
    "        # Random initial selection\n",
    "        all_indices = np.arange(len(X))\n",
    "        np.random.shuffle(all_indices)\n",
    "        \n",
    "        self.labeled_indices = all_indices[:self.initial_labeled_size].tolist()\n",
    "        self.unlabeled_indices = all_indices[self.initial_labeled_size:].tolist()\n",
    "        \n",
    "        return self.labeled_indices, self.unlabeled_indices\n",
    "    \n",
    "    def query(self, model, X, y, n_queries=5):\n",
    "        \"\"\"Query points to label next (to be implemented by subclasses).\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class UncertaintySampling(ActiveLearningStrategy):\n",
    "    \"\"\"Uncertainty-based active learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, strategy='least_confident', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.strategy = strategy\n",
    "    \n",
    "    def query(self, model, X, y, n_queries=5):\n",
    "        \"\"\"Query most uncertain points.\"\"\"\n",
    "        if len(self.unlabeled_indices) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Get unlabeled data\n",
    "        X_unlabeled = X[self.unlabeled_indices]\n",
    "        \n",
    "        # Get prediction probabilities\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            proba = model.predict_proba(X_unlabeled)\n",
    "            \n",
    "            if self.strategy == 'least_confident':\n",
    "                # Select points with lowest maximum probability\n",
    "                max_proba = np.max(proba, axis=1)\n",
    "                uncertainty_scores = 1 - max_proba\n",
    "            elif self.strategy == 'margin':\n",
    "                # Select points with smallest margin between top two classes\n",
    "                sorted_proba = np.sort(proba, axis=1)\n",
    "                uncertainty_scores = -(sorted_proba[:, -1] - sorted_proba[:, -2])\n",
    "            elif self.strategy == 'entropy':\n",
    "                # Select points with highest entropy\n",
    "                uncertainty_scores = -np.sum(proba * np.log(proba + 1e-10), axis=1)\n",
    "            else:\n",
    "                uncertainty_scores = 1 - np.max(proba, axis=1)\n",
    "        else:\n",
    "            # Fallback: random selection\n",
    "            uncertainty_scores = np.random.random(len(X_unlabeled))\n",
    "        \n",
    "        # Select top uncertain points\n",
    "        n_queries = min(n_queries, len(self.unlabeled_indices))\n",
    "        top_uncertain_indices = np.argsort(uncertainty_scores)[-n_queries:]\n",
    "        \n",
    "        # Get actual indices\n",
    "        queried_indices = [self.unlabeled_indices[i] for i in top_uncertain_indices]\n",
    "        \n",
    "        # Update labeled/unlabeled sets\n",
    "        self.labeled_indices.extend(queried_indices)\n",
    "        for idx in queried_indices:\n",
    "            self.unlabeled_indices.remove(idx)\n",
    "        \n",
    "        # Store query information\n",
    "        self.query_history.append({\n",
    "            'queried_indices': queried_indices,\n",
    "            'uncertainty_scores': uncertainty_scores[top_uncertain_indices],\n",
    "            'n_labeled': len(self.labeled_indices),\n",
    "            'n_unlabeled': len(self.unlabeled_indices)\n",
    "        })\n",
    "        \n",
    "        return queried_indices\n",
    "\n",
    "class QueryByCommittee(ActiveLearningStrategy):\n",
    "    \"\"\"Query by Committee active learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, committee_models=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if committee_models is None:\n",
    "            self.committee_models = [\n",
    "                RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                SVC(probability=True, random_state=42),\n",
    "                LogisticRegression(random_state=42, max_iter=1000)\n",
    "            ]\n",
    "        else:\n",
    "            self.committee_models = committee_models\n",
    "    \n",
    "    def query(self, model, X, y, n_queries=5):\n",
    "        \"\"\"Query points with highest disagreement among committee.\"\"\"\n",
    "        if len(self.unlabeled_indices) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Train committee on labeled data\n",
    "        X_labeled = X[self.labeled_indices]\n",
    "        y_labeled = y[self.labeled_indices]\n",
    "        \n",
    "        committee_predictions = []\n",
    "        for committee_model in self.committee_models:\n",
    "            try:\n",
    "                committee_model.fit(X_labeled, y_labeled)\n",
    "                if hasattr(committee_model, 'predict_proba'):\n",
    "                    pred = committee_model.predict_proba(X[self.unlabeled_indices])\n",
    "                else:\n",
    "                    pred = committee_model.predict(X[self.unlabeled_indices])\n",
    "                committee_predictions.append(pred)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if not committee_predictions:\n",
    "            # Fallback to random selection\n",
    "            n_queries = min(n_queries, len(self.unlabeled_indices))\n",
    "            queried_indices = np.random.choice(self.unlabeled_indices, n_queries, replace=False)\n",
    "        else:\n",
    "            # Calculate disagreement (variance in predictions)\n",
    "            if len(committee_predictions[0].shape) > 1:  # Probability predictions\n",
    "                # Calculate variance in class probabilities\n",
    "                stacked_predictions = np.stack(committee_predictions)\n",
    "                disagreement_scores = np.var(stacked_predictions, axis=0).max(axis=1)\n",
    "            else:  # Hard predictions\n",
    "                # Calculate vote entropy\n",
    "                stacked_predictions = np.stack(committee_predictions)\n",
    "                disagreement_scores = []\n",
    "                for i in range(stacked_predictions.shape[1]):\n",
    "                    votes = stacked_predictions[:, i]\n",
    "                    unique_votes, counts = np.unique(votes, return_counts=True)\n",
    "                    vote_probs = counts / len(votes)\n",
    "                    entropy = -np.sum(vote_probs * np.log(vote_probs + 1e-10))\n",
    "                    disagreement_scores.append(entropy)\n",
    "                disagreement_scores = np.array(disagreement_scores)\n",
    "            \n",
    "            # Select points with highest disagreement\n",
    "            n_queries = min(n_queries, len(self.unlabeled_indices))\n",
    "            top_disagreement_indices = np.argsort(disagreement_scores)[-n_queries:]\n",
    "            queried_indices = [self.unlabeled_indices[i] for i in top_disagreement_indices]\n",
    "        \n",
    "        # Update labeled/unlabeled sets\n",
    "        self.labeled_indices.extend(queried_indices)\n",
    "        for idx in queried_indices:\n",
    "            self.unlabeled_indices.remove(idx)\n",
    "        \n",
    "        # Store query information\n",
    "        self.query_history.append({\n",
    "            'queried_indices': queried_indices,\n",
    "            'n_labeled': len(self.labeled_indices),\n",
    "            'n_unlabeled': len(self.unlabeled_indices),\n",
    "            'committee_size': len(committee_predictions)\n",
    "        })\n",
    "        \n",
    "        return queried_indices\n",
    "\n",
    "def active_learning_simulation(X, y, strategy, base_model, n_iterations=10, queries_per_iteration=5):\n",
    "    \"\"\"Simulate active learning process.\"\"\"\n",
    "    print(f\"Running active learning simulation with {strategy.__class__.__name__}...\")\n",
    "    \n",
    "    # Initialize strategy\n",
    "    labeled_indices, unlabeled_indices = strategy.initialize(X, y)\n",
    "    \n",
    "    # Track performance\n",
    "    performance_history = []\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        if len(strategy.unlabeled_indices) == 0:\n",
    "            print(f\"  No more unlabeled data at iteration {iteration}\")\n",
    "            break\n",
    "        \n",
    "        # Train model on current labeled data\n",
    "        X_labeled = X[strategy.labeled_indices]\n",
    "        y_labeled = y[strategy.labeled_indices]\n",
    "        \n",
    "        model = base_model.__class__(**base_model.get_params())\n",
    "        model.fit(X_labeled, y_labeled)\n",
    "        \n",
    "        # Evaluate on remaining unlabeled data (as test set)\n",
    "        if len(strategy.unlabeled_indices) > 0:\n",
    "            X_test = X[strategy.unlabeled_indices]\n",
    "            y_test = y[strategy.unlabeled_indices]\n",
    "            test_score = model.score(X_test, y_test)\n",
    "        else:\n",
    "            test_score = 0.0\n",
    "        \n",
    "        performance_history.append({\n",
    "            'iteration': iteration,\n",
    "            'n_labeled': len(strategy.labeled_indices),\n",
    "            'test_score': test_score\n",
    "        })\n",
    "        \n",
    "        print(f\"  Iteration {iteration}: {len(strategy.labeled_indices)} labeled, score: {test_score:.4f}\")\n",
    "        \n",
    "        # Query new points\n",
    "        queried_indices = strategy.query(model, X, y, queries_per_iteration)\n",
    "        \n",
    "        if not queried_indices:\n",
    "            break\n",
    "    \n",
    "    return performance_history\n",
    "\n",
    "# Generate data for active learning\n",
    "X_al, y_al = generator.classification_dataset(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_classes=3,\n",
    "    class_sep=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler_al = StandardScaler()\n",
    "X_al_scaled = scaler_al.fit_transform(X_al)\n",
    "\n",
    "print(f\"Active Learning Dataset: {X_al_scaled.shape}\")\n",
    "\n",
    "# Test different active learning strategies\n",
    "base_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "strategies = {\n",
    "    'Uncertainty (Least Confident)': UncertaintySampling(strategy='least_confident', initial_labeled_size=15),\n",
    "    'Uncertainty (Margin)': UncertaintySampling(strategy='margin', initial_labeled_size=15),\n",
    "    'Uncertainty (Entropy)': UncertaintySampling(strategy='entropy', initial_labeled_size=15),\n",
    "    'Query by Committee': QueryByCommittee(initial_labeled_size=15)\n",
    "}\n",
    "\n",
    "al_results = {}\n",
    "\n",
    "for strategy_name, strategy in strategies.items():\n",
    "    print(f\"\\n--- Testing {strategy_name} ---\")\n",
    "    \n",
    "    # Reset indices for each strategy\n",
    "    np.random.seed(42)  # Ensure consistent initialization\n",
    "    \n",
    "    performance_history = active_learning_simulation(\n",
    "        X_al_scaled, y_al, strategy, base_model,\n",
    "        n_iterations=15, queries_per_iteration=10\n",
    "    )\n",
    "    \n",
    "    al_results[strategy_name] = {\n",
    "        'performance_history': performance_history,\n",
    "        'final_labeled': len(strategy.labeled_indices),\n",
    "        'query_history': strategy.query_history\n",
    "    }\n",
    "\n",
    "# Random baseline for comparison\n",
    "print(f\"\\n--- Random Baseline ---\")\n",
    "random_strategy = UncertaintySampling(initial_labeled_size=15)\n",
    "# Override query method for random selection\n",
    "def random_query(self, model, X, y, n_queries=5):\n",
    "    if len(self.unlabeled_indices) == 0:\n",
    "        return []\n",
    "    n_queries = min(n_queries, len(self.unlabeled_indices))\n",
    "    queried_indices = np.random.choice(self.unlabeled_indices, n_queries, replace=False).tolist()\n",
    "    self.labeled_indices.extend(queried_indices)\n",
    "    for idx in queried_indices:\n",
    "        self.unlabeled_indices.remove(idx)\n",
    "    return queried_indices\n",
    "\n",
    "random_strategy.query = lambda model, X, y, n_queries=5: random_query(random_strategy, model, X, y, n_queries)\n",
    "np.random.seed(42)\n",
    "random_performance = active_learning_simulation(\n",
    "    X_al_scaled, y_al, random_strategy, base_model,\n",
    "    n_iterations=15, queries_per_iteration=10\n",
    ")\n",
    "\n",
    "al_results['Random Baseline'] = {\n",
    "    'performance_history': random_performance,\n",
    "    'final_labeled': len(random_strategy.labeled_indices),\n",
    "    'query_history': []\n",
    "}\n",
    "\n",
    "# Save active learning results\n",
    "al_summary = {}\n",
    "for strategy_name, results in al_results.items():\n",
    "    if results['performance_history']:\n",
    "        final_score = results['performance_history'][-1]['test_score']\n",
    "        final_labeled = results['performance_history'][-1]['n_labeled']\n",
    "        \n",
    "        al_summary[strategy_name] = {\n",
    "            'final_score': final_score,\n",
    "            'final_labeled': final_labeled,\n",
    "            'n_iterations': len(results['performance_history'])\n",
    "        }\n",
    "\n",
    "save_experiment_results('active_learning_comparison', al_summary,\n",
    "                       'Comparison of active learning strategies', 'active_learning')\n",
    "\n",
    "print(\"\\n‚ú® Active learning experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831a5fd",
   "metadata": {},
   "source": [
    "### Active Learning Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize active learning results\n",
    "print(\"üìä Visualizing Active Learning Results...\")\n",
    "\n",
    "if al_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Learning curves comparison\n",
    "    for strategy_name, results in al_results.items():\n",
    "        if results['performance_history']:\n",
    "            history = results['performance_history']\n",
    "            n_labeled = [h['n_labeled'] for h in history]\n",
    "            scores = [h['test_score'] for h in history]\n",
    "            \n",
    "            axes[0, 0].plot(n_labeled, scores, 'o-', label=strategy_name, linewidth=2, markersize=4)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Number of Labeled Samples')\n",
    "    axes[0, 0].set_ylabel('Test Accuracy')\n",
    "    axes[0, 0].set_title('Active Learning Curves')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Final performance comparison\n",
    "    strategy_names = []\n",
    "    final_scores = []\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(al_results)))\n",
    "    \n",
    "    for i, (strategy_name, results) in enumerate(al_results.items()):\n",
    "        if results['performance_history']:\n",
    "            strategy_names.append(strategy_name.replace(' ', '\\n'))\n",
    "            final_scores.append(results['performance_history'][-1]['test_score'])\n",
    "    \n",
    "    bars = axes[0, 1].bar(range(len(strategy_names)), final_scores, \n",
    "                         color=colors, alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Active Learning Strategy')\n",
    "    axes[0, 1].set_ylabel('Final Test Accuracy')\n",
    "    axes[0, 1].set_title('Final Performance Comparison')\n",
    "    axes[0, 1].set_xticks(range(len(strategy_names)))\n",
    "    axes[0, 1].set_xticklabels(strategy_names, rotation=45, ha='right')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, final_scores):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                       f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Efficiency analysis (performance vs samples)\n",
    "    best_strategies = []\n",
    "    random_baseline = None\n",
    "    \n",
    "    for strategy_name, results in al_results.items():\n",
    "        if results['performance_history']:\n",
    "            if 'Random' in strategy_name:\n",
    "                random_baseline = results['performance_history']\n",
    "            else:\n",
    "                best_strategies.append((strategy_name, results['performance_history']))\n",
    "    \n",
    "    if random_baseline and best_strategies:\n",
    "        random_scores = [h['test_score'] for h in random_baseline]\n",
    "        random_labeled = [h['n_labeled'] for h in random_baseline]\n",
    "        \n",
    "        # Find best non-random strategy\n",
    "        best_strategy_name, best_history = max(best_strategies, \n",
    "                                             key=lambda x: x[1][-1]['test_score'])\n",
    "        best_scores = [h['test_score'] for h in best_history]\n",
    "        best_labeled = [h['n_labeled'] for h in best_history]\n",
    "        \n",
    "        axes[1, 0].plot(random_labeled, random_scores, 'r--', label='Random', linewidth=2)\n",
    "        axes[1, 0].plot(best_labeled, best_scores, 'g-', label=f'Best ({best_strategy_name})', linewidth=2)\n",
    "        axes[1, 0].fill_between(best_labeled, random_scores[:len(best_labeled)], best_scores, \n",
    "                               alpha=0.3, color='green', label='Improvement')\n",
    "        \n",
    "        axes[1, 0].set_xlabel('Number of Labeled Samples')\n",
    "        axes[1, 0].set_ylabel('Test Accuracy')\n",
    "        axes[1, 0].set_title('Active Learning Efficiency')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Sample efficiency metrics\n",
    "    if al_results:\n",
    "        efficiency_metrics = []\n",
    "        strategy_names_clean = []\n",
    "        \n",
    "        for strategy_name, results in al_results.items():\n",
    "            if results['performance_history'] and 'Random' not in strategy_name:\n",
    "                history = results['performance_history']\n",
    "                # Calculate area under the curve as efficiency metric\n",
    "                n_labeled = [h['n_labeled'] for h in history]\n",
    "                scores = [h['test_score'] for h in history]\n",
    "                \n",
    "                if len(n_labeled) > 1:\n",
    "                    auc = np.trapz(scores, n_labeled) / (max(n_labeled) - min(n_labeled))\n",
    "                    efficiency_metrics.append(auc)\n",
    "                    strategy_names_clean.append(strategy_name.replace(' ', '\\n'))\n",
    "        \n",
    "        if efficiency_metrics:\n",
    "            bars = axes[1, 1].bar(range(len(strategy_names_clean)), efficiency_metrics, \n",
    "                                 color=colors[:len(efficiency_metrics)], alpha=0.7)\n",
    "            axes[1, 1].set_xlabel('Strategy')\n",
    "            axes[1, 1].set_ylabel('Sample Efficiency (AUC)')\n",
    "            axes[1, 1].set_title('Sample Efficiency Comparison')\n",
    "            axes[1, 1].set_xticks(range(len(strategy_names_clean)))\n",
    "            axes[1, 1].set_xticklabels(strategy_names_clean, rotation=45, ha='right')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save active learning visualization\n",
    "    save_figure(fig, 'active_learning_analysis',\n",
    "               'Comprehensive analysis of active learning strategies', 'active_learning')\n",
    "    plt.show()\n",
    "    \n",
    "    # Active learning summary\n",
    "    print(f\"\\nüìä Active Learning Summary:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Strategy':<25} {'Final Score':<12} {'Samples Used':<12} {'Efficiency':<10}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for strategy_name, results in al_results.items():\n",
    "        if results['performance_history']:\n",
    "            final_score = results['performance_history'][-1]['test_score']\n",
    "            samples_used = results['performance_history'][-1]['n_labeled']\n",
    "            efficiency = final_score / samples_used if samples_used > 0 else 0\n",
    "            \n",
    "            print(f\"{strategy_name:<25} {final_score:<12.4f} {samples_used:<12} {efficiency:<10.6f}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚ú® Active learning visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e6a5e",
   "metadata": {},
   "source": [
    "## 5. Interpretable Machine Learning {#interpretability}\n",
    "\n",
    "Implementing techniques for model interpretability and explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretable machine learning techniques\n",
    "print(\"üîç Interpretable Machine Learning...\")\n",
    "\n",
    "class ModelInterpreter:\n",
    "    \"\"\"Comprehensive model interpretation toolkit.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, X_train, y_train, feature_names=None):\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.feature_names = feature_names or [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
    "        self.interpretations = {}\n",
    "    \n",
    "    def permutation_feature_importance(self, X_test, y_test, n_repeats=10):\n",
    "        \"\"\"Calculate permutation-based feature importance.\"\"\"\n",
    "        print(\"  Calculating permutation feature importance...\")\n",
    "        \n",
    "        perm_importance = permutation_importance(\n",
    "            self.model, X_test, y_test, \n",
    "            n_repeats=n_repeats, random_state=42,\n",
    "            scoring='accuracy'\n",
    "        )\n",
    "        \n",
    "        # Sort features by importance\n",
    "        sorted_indices = perm_importance.importances_mean.argsort()[::-1]\n",
    "        \n",
    "        importance_data = {\n",
    "            'feature_names': [self.feature_names[i] for i in sorted_indices],\n",
    "            'importances_mean': perm_importance.importances_mean[sorted_indices],\n",
    "            'importances_std': perm_importance.importances_std[sorted_indices],\n",
    "            'sorted_indices': sorted_indices\n",
    "        }\n",
    "        \n",
    "        self.interpretations['permutation_importance'] = importance_data\n",
    "        return importance_data\n",
    "    \n",
    "    def feature_importance_analysis(self):\n",
    "        \"\"\"Analyze built-in feature importance (for tree-based models).\"\"\"\n",
    "        if hasattr(self.model, 'feature_importances_'):\n",
    "            print(\"  Analyzing built-in feature importance...\")\n",
    "            \n",
    "            importances = self.model.feature_importances_\n",
    "            sorted_indices = importances.argsort()[::-1]\n",
    "            \n",
    "            importance_data = {\n",
    "                'feature_names': [self.feature_names[i] for i in sorted_indices],\n",
    "                'importances': importances[sorted_indices],\n",
    "                'sorted_indices': sorted_indices\n",
    "            }\n",
    "            \n",
    "            self.interpretations['builtin_importance'] = importance_data\n",
    "            return importance_data\n",
    "        else:\n",
    "            print(\"  Model doesn't have built-in feature importance\")\n",
    "            return None\n",
    "    \n",
    "    def local_interpretability_lime(self, instance, n_features=10):\n",
    "        \"\"\"LIME-like local interpretability (simplified implementation).\"\"\"\n",
    "        print(\"  Generating local explanation...\")\n",
    "        \n",
    "        # Simplified LIME: perturb features and see impact on prediction\n",
    "        original_pred = self.model.predict_proba(instance.reshape(1, -1))[0]\n",
    "        \n",
    "        feature_impacts = []\n",
    "        \n",
    "        for i in range(len(instance)):\n",
    "            # Create perturbed instance\n",
    "            perturbed_instance = instance.copy()\n",
    "            perturbed_instance[i] = np.random.normal(\n",
    "                np.mean(self.X_train[:, i]), \n",
    "                np.std(self.X_train[:, i])\n",
    "            )\n",
    "            \n",
    "            # Get prediction for perturbed instance\n",
    "            perturbed_pred = self.model.predict_proba(perturbed_instance.reshape(1, -1))[0]\n",
    "            \n",
    "            # Calculate impact (change in prediction)\n",
    "            impact = np.abs(original_pred - perturbed_pred).max()\n",
    "            feature_impacts.append(impact)\n",
    "        \n",
    "        # Sort by impact\n",
    "        sorted_indices = np.argsort(feature_impacts)[::-1][:n_features]\n",
    "        \n",
    "        local_explanation = {\n",
    "            'feature_names': [self.feature_names[i] for i in sorted_indices],\n",
    "            'feature_impacts': [feature_impacts[i] for i in sorted_indices],\n",
    "            'original_prediction': original_pred,\n",
    "            'predicted_class': np.argmax(original_pred)\n",
    "        }\n",
    "        \n",
    "        return local_explanation\n",
    "    \n",
    "    def model_complexity_analysis(self):\n",
    "        \"\"\"Analyze model complexity and structure.\"\"\"\n",
    "        complexity_info = {\n",
    "            'model_type': type(self.model).__name__\n",
    "        }\n",
    "        \n",
    "        # Tree-based model analysis\n",
    "        if hasattr(self.model, 'estimators_'):\n",
    "            complexity_info['n_estimators'] = len(self.model.estimators_)\n",
    "            if hasattr(self.model.estimators_[0], 'tree_'):\n",
    "                depths = [estimator.tree_.max_depth for estimator in self.model.estimators_]\n",
    "                complexity_info['avg_tree_depth'] = np.mean(depths)\n",
    "                complexity_info['max_tree_depth'] = np.max(depths)\n",
    "                complexity_info['total_nodes'] = sum([estimator.tree_.node_count for estimator in self.model.estimators_])\n",
    "        \n",
    "        # Neural network analysis\n",
    "        elif hasattr(self.model, 'hidden_layer_sizes'):\n",
    "            complexity_info['hidden_layers'] = len(self.model.hidden_layer_sizes)\n",
    "            complexity_info['total_neurons'] = sum(self.model.hidden_layer_sizes)\n",
    "            complexity_info['architecture'] = self.model.hidden_layer_sizes\n",
    "        \n",
    "        # Linear model analysis\n",
    "        elif hasattr(self.model, 'coef_'):\n",
    "            complexity_info['n_coefficients'] = len(self.model.coef_.flatten())\n",
    "            if hasattr(self.model, 'intercept_'):\n",
    "                complexity_info['has_intercept'] = True\n",
    "        \n",
    "        self.interpretations['complexity'] = complexity_info\n",
    "        return complexity_info\n",
    "    \n",
    "    def decision_boundary_analysis(self, X_sample, resolution=100):\n",
    "        \"\"\"Analyze decision boundary (for 2D visualization).\"\"\"\n",
    "        if X_sample.shape[1] != 2:\n",
    "            print(\"  Decision boundary analysis only available for 2D data\")\n",
    "            return None\n",
    "        \n",
    "        print(\"  Analyzing decision boundary...\")\n",
    "        \n",
    "        # Create grid\n",
    "        x_min, x_max = X_sample[:, 0].min() - 0.5, X_sample[:, 0].max() + 0.5\n",
    "        y_min, y_max = X_sample[:, 1].min() - 0.5, X_sample[:, 1].max() + 0.5\n",
    "        \n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution),\n",
    "                            np.linspace(y_min, y_max, resolution))\n",
    "        \n",
    "        # Get predictions for grid\n",
    "        grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "        \n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            Z = self.model.predict_proba(grid_points)[:, 1]  # Probability of positive class\n",
    "        else:\n",
    "            Z = self.model.predict(grid_points)\n",
    "        \n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        boundary_data = {\n",
    "            'xx': xx,\n",
    "            'yy': yy,\n",
    "            'Z': Z,\n",
    "            'x_range': (x_min, x_max),\n",
    "            'y_range': (y_min, y_max)\n",
    "        }\n",
    "        \n",
    "        self.interpretations['decision_boundary'] = boundary_data\n",
    "        return boundary_data\n",
    "    \n",
    "    def generate_interpretation_report(self):\n",
    "        \"\"\"Generate comprehensive interpretation report.\"\"\"\n",
    "        report = \"\\n\" + \"=\"*80 + \"\\n\"\n",
    "        report += \"MODEL INTERPRETABILITY REPORT\\n\"\n",
    "        report += \"=\"*80 + \"\\n\\n\"\n",
    "        \n",
    "        report += f\"Model Type: {type(self.model).__name__}\\n\"\n",
    "        report += f\"Training Data Shape: {self.X_train.shape}\\n\"\n",
    "        report += f\"Number of Features: {len(self.feature_names)}\\n\\n\"\n",
    "        \n",
    "        # Feature importance\n",
    "        if 'permutation_importance' in self.interpretations:\n",
    "            perm_imp = self.interpretations['permutation_importance']\n",
    "            report += \"PERMUTATION FEATURE IMPORTANCE (Top 10):\\n\"\n",
    "            report += \"-\" * 50 + \"\\n\"\n",
    "            for i, (name, imp, std) in enumerate(zip(\n",
    "                perm_imp['feature_names'][:10],\n",
    "                perm_imp['importances_mean'][:10],\n",
    "                perm_imp['importances_std'][:10]\n",
    "            )):\n",
    "                report += f\"{i+1:2d}. {name:<20}: {imp:.4f} ¬± {std:.4f}\\n\"\n",
    "            report += \"\\n\"\n",
    "        \n",
    "        # Model complexity\n",
    "        if 'complexity' in self.interpretations:\n",
    "            complexity = self.interpretations['complexity']\n",
    "            report += \"MODEL COMPLEXITY ANALYSIS:\\n\"\n",
    "            report += \"-\" * 30 + \"\\n\"\n",
    "            for key, value in complexity.items():\n",
    "                report += f\"{key}: {value}\\n\"\n",
    "            report += \"\\n\"\n",
    "        \n",
    "        report += \"=\"*80 + \"\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "class SHAPAnalyzer:\n",
    "    \"\"\"Simplified SHAP-like analysis for feature attribution.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, X_background):\n",
    "        self.model = model\n",
    "        self.X_background = X_background\n",
    "        self.baseline_prediction = self._get_baseline_prediction()\n",
    "    \n",
    "    def _get_baseline_prediction(self):\n",
    "        \"\"\"Calculate baseline prediction (average over background).\"\"\"\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            preds = self.model.predict_proba(self.X_background)\n",
    "            return np.mean(preds, axis=0)\n",
    "        else:\n",
    "            preds = self.model.predict(self.X_background)\n",
    "            unique_classes = np.unique(preds)\n",
    "            baseline = []\n",
    "            for cls in unique_classes:\n",
    "                baseline.append(np.mean(preds == cls))\n",
    "            return np.array(baseline)\n",
    "    \n",
    "    def explain_instance(self, instance, n_samples=100):\n",
    "        \"\"\"Explain a single instance using Shapley-like values.\"\"\"\n",
    "        feature_contributions = []\n",
    "        \n",
    "        for feature_idx in range(len(instance)):\n",
    "            # Calculate marginal contribution of this feature\n",
    "            contributions = []\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                # Random subset of other features\n",
    "                other_features = list(range(len(instance)))\n",
    "                other_features.remove(feature_idx)\n",
    "                subset_size = np.random.randint(0, len(other_features))\n",
    "                \n",
    "                if subset_size > 0:\n",
    "                    selected_features = np.random.choice(other_features, subset_size, replace=False)\n",
    "                else:\n",
    "                    selected_features = []\n",
    "                \n",
    "                # Create coalition without target feature\n",
    "                coalition_without = self._create_coalition(instance, selected_features, exclude_target=True)\n",
    "                pred_without = self._predict_coalition(coalition_without)\n",
    "                \n",
    "                # Create coalition with target feature\n",
    "                coalition_with = self._create_coalition(instance, list(selected_features) + [feature_idx])\n",
    "                pred_with = self._predict_coalition(coalition_with)\n",
    "                \n",
    "                # Marginal contribution\n",
    "                contribution = self._prediction_difference(pred_with, pred_without)\n",
    "                contributions.append(contribution)\n",
    "            \n",
    "            feature_contributions.append(np.mean(contributions))\n",
    "        \n",
    "        return np.array(feature_contributions)\n",
    "    \n",
    "    def _create_coalition(self, instance, feature_indices, exclude_target=False):\n",
    "        \"\"\"Create coalition by replacing unselected features with background values.\"\"\"\n",
    "        coalition = np.random.choice(self.X_background.flatten(), size=len(instance))\n",
    "        \n",
    "        for idx in feature_indices:\n",
    "            coalition[idx] = instance[idx]\n",
    "        \n",
    "        return coalition\n",
    "    \n",
    "    def _predict_coalition(self, coalition):\n",
    "        \"\"\"Get prediction for coalition.\"\"\"\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            return self.model.predict_proba(coalition.reshape(1, -1))[0]\n",
    "        else:\n",
    "            pred = self.model.predict(coalition.reshape(1, -1))[0]\n",
    "            # Convert to probability-like format\n",
    "            n_classes = len(np.unique(self.model.classes_)) if hasattr(self.model, 'classes_') else 2\n",
    "            proba = np.zeros(n_classes)\n",
    "            proba[pred] = 1.0\n",
    "            return proba\n",
    "    \n",
    "    def _prediction_difference(self, pred1, pred2):\n",
    "        \"\"\"Calculate meaningful difference between predictions.\"\"\"\n",
    "        if len(pred1.shape) > 0 and len(pred1) > 1:\n",
    "            return np.abs(pred1 - pred2).max()\n",
    "        else:\n",
    "            return abs(pred1 - pred2)\n",
    "\n",
    "# Generate data for interpretability analysis\n",
    "X_interp, y_interp = generator.classification_dataset(\n",
    "    n_samples=800,\n",
    "    n_features=15,\n",
    "    n_informative=10,\n",
    "    n_classes=2,\n",
    "    class_sep=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create meaningful feature names\n",
    "feature_names = [\n",
    "    'Age', 'Income', 'Education', 'Experience', 'Skills',\n",
    "    'Location', 'Health', 'Credit_Score', 'Risk_Tolerance', 'Motivation',\n",
    "    'Network_Size', 'Innovation', 'Leadership', 'Communication', 'Technical'\n",
    "]\n",
    "\n",
    "# Split data\n",
    "X_train_interp, X_test_interp, y_train_interp, y_test_interp = train_test_split(\n",
    "    X_interp, y_interp, test_size=0.3, random_state=42, stratify=y_interp\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler_interp = StandardScaler()\n",
    "X_train_interp_scaled = scaler_interp.fit_transform(X_train_interp)\n",
    "X_test_interp_scaled = scaler_interp.transform(X_test_interp)\n",
    "\n",
    "print(f\"Interpretability Dataset: {X_train_interp_scaled.shape}\")\n",
    "\n",
    "# Train interpretable models\n",
    "interpretable_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "interpretation_results = {}\n",
    "\n",
    "for model_name, model in interpretable_models.items():\n",
    "    print(f\"\\n--- Interpreting {model_name} ---\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_interp_scaled, y_train_interp)\n",
    "    test_score = model.score(X_test_interp_scaled, y_test_interp)\n",
    "    print(f\"Test Accuracy: {test_score:.4f}\")\n",
    "    \n",
    "    # Initialize interpreter\n",
    "    interpreter = ModelInterpreter(model, X_train_interp_scaled, y_train_interp, feature_names)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    perm_importance = interpreter.permutation_feature_importance(X_test_interp_scaled, y_test_interp)\n",
    "    builtin_importance = interpreter.feature_importance_analysis()\n",
    "    complexity_info = interpreter.model_complexity_analysis()\n",
    "    \n",
    "    # Local interpretability for a sample instance\n",
    "    sample_instance = X_test_interp_scaled[0]\n",
    "    local_explanation = interpreter.local_interpretability_lime(sample_instance)\n",
    "    \n",
    "    # SHAP-like analysis\n",
    "    shap_analyzer = SHAPAnalyzer(model, X_train_interp_scaled[:50])  # Use subset for efficiency\n",
    "    shap_values = shap_analyzer.explain_instance(sample_instance)\n",
    "    \n",
    "    # Generate interpretation report\n",
    "    report = interpreter.generate_interpretation_report()\n",
    "    \n",
    "    interpretation_results[model_name] = {\n",
    "        'test_score': test_score,\n",
    "        'permutation_importance': perm_importance,\n",
    "        'builtin_importance': builtin_importance,\n",
    "        'complexity_info': complexity_info,\n",
    "        'local_explanation': local_explanation,\n",
    "        'shap_values': shap_values,\n",
    "        'interpretation_report': report,\n",
    "        'interpreter': interpreter\n",
    "    }\n",
    "    \n",
    "    # Save model and interpretation\n",
    "    interp_metrics = {\n",
    "        'test_score': test_score,\n",
    "        'top_features': perm_importance['feature_names'][:5],\n",
    "        'model_complexity': complexity_info\n",
    "    }\n",
    "    \n",
    "    save_advanced_model(model, f\"interpretable_{model_name.lower().replace(' ', '_')}\", \n",
    "                       f\"Interpretable {model_name} with analysis\",\n",
    "                       'interpretability', interp_metrics)\n",
    "\n",
    "# Save interpretation results\n",
    "interpretation_summary = {}\n",
    "for model_name, results in interpretation_results.items():\n",
    "    interpretation_summary[model_name] = {\n",
    "        'test_score': results['test_score'],\n",
    "        'top_5_features': results['permutation_importance']['feature_names'][:5],\n",
    "        'top_5_importances': results['permutation_importance']['importances_mean'][:5].tolist(),\n",
    "        'complexity': results['complexity_info']\n",
    "    }\n",
    "\n",
    "save_experiment_results('interpretability_analysis', interpretation_summary,\n",
    "                       'Comprehensive interpretability analysis of different models', 'interpretability')\n",
    "\n",
    "print(\"\\n‚ú® Model interpretability analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e70bd6c",
   "metadata": {},
   "source": [
    "### Interpretability Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interpretability results\n",
    "print(\"üìä Visualizing Model Interpretability...\")\n",
    "\n",
    "if interpretation_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Feature importance comparison across models\n",
    "    model_names = list(interpretation_results.keys())\n",
    "    n_top_features = 8\n",
    "    \n",
    "    # Get top features from Random Forest (as reference)\n",
    "    if 'Random Forest' in interpretation_results:\n",
    "        rf_perm_imp = interpretation_results['Random Forest']['permutation_importance']\n",
    "        top_features = rf_perm_imp['feature_names'][:n_top_features]\n",
    "        \n",
    "        # Create comparison matrix\n",
    "        importance_matrix = []\n",
    "        for model_name in model_names:\n",
    "            model_importances = []\n",
    "            perm_imp = interpretation_results[model_name]['permutation_importance']\n",
    "            \n",
    "            for feature in top_features:\n",
    "                if feature in perm_imp['feature_names']:\n",
    "                    idx = perm_imp['feature_names'].index(feature)\n",
    "                    importance = perm_imp['importances_mean'][idx]\n",
    "                else:\n",
    "                    importance = 0\n",
    "                model_importances.append(importance)\n",
    "            \n",
    "            importance_matrix.append(model_importances)\n",
    "        \n",
    "        # Plot heatmap\n",
    "        im = axes[0, 0].imshow(importance_matrix, cmap='Blues', aspect='auto')\n",
    "        axes[0, 0].set_xticks(range(len(top_features)))\n",
    "        axes[0, 0].set_xticklabels(top_features, rotation=45, ha='right')\n",
    "        axes[0, 0].set_yticks(range(len(model_names)))\n",
    "        axes[0, 0].set_yticklabels(model_names)\n",
    "        axes[0, 0].set_title('Feature Importance Heatmap')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=axes[0, 0])\n",
    "        cbar.set_label('Importance Score')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(len(top_features)):\n",
    "                text = axes[0, 0].text(j, i, f'{importance_matrix[i][j]:.3f}',\n",
    "                                     ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "    \n",
    "    # 2. Model complexity comparison\n",
    "    complexity_metrics = []\n",
    "    complexity_labels = []\n",
    "    \n",
    "    for model_name, results in interpretation_results.items():\n",
    "        complexity = results['complexity_info']\n",
    "        \n",
    "        if 'n_estimators' in complexity:\n",
    "            complexity_metrics.append(complexity['n_estimators'])\n",
    "            complexity_labels.append(f\"{model_name}\\n(Trees)\")\n",
    "        elif 'total_neurons' in complexity:\n",
    "            complexity_metrics.append(complexity['total_neurons'])\n",
    "            complexity_labels.append(f\"{model_name}\\n(Neurons)\")\n",
    "        elif 'n_coefficients' in complexity:\n",
    "            complexity_metrics.append(complexity['n_coefficients'])\n",
    "            complexity_labels.append(f\"{model_name}\\n(Coeffs)\")\n",
    "        else:\n",
    "            complexity_metrics.append(1)\n",
    "            complexity_labels.append(f\"{model_name}\\n(Simple)\")\n",
    "    \n",
    "    bars = axes[0, 1].bar(range(len(complexity_labels)), complexity_metrics, \n",
    "                         color=['lightblue', 'lightgreen', 'lightcoral'][:len(complexity_labels)], \n",
    "                         alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Model')\n",
    "    axes[0, 1].set_ylabel('Complexity Measure')\n",
    "    axes[0, 1].set_title('Model Complexity Comparison')\n",
    "    axes[0, 1].set_xticks(range(len(complexity_labels)))\n",
    "    axes[0, 1].set_xticklabels(complexity_labels)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, complexity_metrics):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(complexity_metrics)*0.01, \n",
    "                       f'{value}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Local explanation visualization\n",
    "    if 'Random Forest' in interpretation_results:\n",
    "        local_exp = interpretation_results['Random Forest']['local_explanation']\n",
    "        \n",
    "        # Top contributing features for the sample instance\n",
    "        n_show = min(8, len(local_exp['feature_names']))\n",
    "        feature_names_local = local_exp['feature_names'][:n_show]\n",
    "        feature_impacts = local_exp['feature_impacts'][:n_show]\n",
    "        \n",
    "        colors = ['red' if impact > np.mean(feature_impacts) else 'blue' \n",
    "                 for impact in feature_impacts]\n",
    "        \n",
    "        bars = axes[1, 0].barh(range(len(feature_names_local)), feature_impacts, \n",
    "                              color=colors, alpha=0.7)\n",
    "        axes[1, 0].set_yticks(range(len(feature_names_local)))\n",
    "        axes[1, 0].set_yticklabels(feature_names_local)\n",
    "        axes[1, 0].set_xlabel('Feature Impact')\n",
    "        axes[1, 0].set_title('Local Explanation (Sample Instance)')\n",
    "        \n",
    "        # Add prediction info\n",
    "        pred_class = local_exp['predicted_class']\n",
    "        pred_prob = local_exp['original_prediction'][pred_class]\n",
    "        axes[1, 0].text(0.02, 0.98, f'Predicted Class: {pred_class}\\nConfidence: {pred_prob:.3f}',\n",
    "                       transform=axes[1, 0].transAxes, va='top', \n",
    "                       bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # 4. Performance vs Interpretability trade-off\n",
    "    test_scores = [results['test_score'] for results in interpretation_results.values()]\n",
    "    \n",
    "    # Simple interpretability score (inverse of complexity)\n",
    "    interpretability_scores = []\n",
    "    for model_name, results in interpretation_results.items():\n",
    "        complexity = results['complexity_info']\n",
    "        \n",
    "        if 'n_estimators' in complexity:\n",
    "            # Tree-based: fewer trees = more interpretable\n",
    "            interp_score = 1.0 / (1.0 + complexity['n_estimators'] / 100)\n",
    "        elif 'total_neurons' in complexity:\n",
    "            # Neural network: fewer neurons = more interpretable\n",
    "            interp_score = 1.0 / (1.0 + complexity['total_neurons'] / 100)\n",
    "        elif 'n_coefficients' in complexity:\n",
    "            # Linear: fewer coefficients = more interpretable\n",
    "            interp_score = 1.0 / (1.0 + complexity['n_coefficients'] / 20)\n",
    "        else:\n",
    "            interp_score = 1.0  # Most interpretable\n",
    "        \n",
    "        interpretability_scores.append(interp_score)\n",
    "    \n",
    "    scatter = axes[1, 1].scatter(interpretability_scores, test_scores, \n",
    "                                c=range(len(model_names)), cmap='viridis', \n",
    "                                s=100, alpha=0.7)\n",
    "    \n",
    "    # Add model labels\n",
    "    for i, (interp, perf, name) in enumerate(zip(interpretability_scores, test_scores, model_names)):\n",
    "        axes[1, 1].annotate(name, (interp, perf), xytext=(5, 5), \n",
    "                           textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Interpretability Score')\n",
    "    axes[1, 1].set_ylabel('Test Accuracy')\n",
    "    axes[1, 1].set_title('Performance vs Interpretability Trade-off')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save interpretability visualization\n",
    "    save_figure(fig, 'interpretability_analysis',\n",
    "               'Comprehensive model interpretability analysis and comparison', 'interpretability')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print interpretation reports\n",
    "    print(f\"\\nüìã Model Interpretation Reports:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for model_name, results in interpretation_results.items():\n",
    "        print(f\"\\n{model_name.upper()} INTERPRETATION:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Test Accuracy: {results['test_score']:.4f}\")\n",
    "        \n",
    "        # Top 5 features\n",
    "        perm_imp = results['permutation_importance']\n",
    "        print(\"Top 5 Most Important Features:\")\n",
    "        for i in range(min(5, len(perm_imp['feature_names']))):\n",
    "            name = perm_imp['feature_names'][i]\n",
    "            importance = perm_imp['importances_mean'][i]\n",
    "            std = perm_imp['importances_std'][i]\n",
    "            print(f\"  {i+1}. {name}: {importance:.4f} ¬± {std:.4f}\")\n",
    "\n",
    "print(\"\\n‚ú® Interpretability visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b38338",
   "metadata": {},
   "source": [
    "## 6. Uncertainty Quantification {#uncertainty}\n",
    "\n",
    "Implementing techniques for measuring and quantifying model uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty quantification techniques\n",
    "print(\"üìä Uncertainty Quantification...\")\n",
    "\n",
    "class UncertaintyQuantifier:\n",
    "    \"\"\"Comprehensive uncertainty quantification toolkit.\"\"\"\n",
    "    \n",
    "    def __init__(self, models=None):\n",
    "        self.models = models or []\n",
    "        self.uncertainty_estimates = {}\n",
    "    \n",
    "    def monte_carlo_dropout_uncertainty(self, model, X, n_samples=100):\n",
    "        \"\"\"Monte Carlo Dropout for uncertainty estimation (simplified for sklearn).\"\"\"\n",
    "        print(\"  Calculating Monte Carlo uncertainty...\")\n",
    "        \n",
    "        # For sklearn models, we'll simulate uncertainty through bootstrap sampling\n",
    "        predictions = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            # Bootstrap sample from training data indices\n",
    "            n_train = len(X)\n",
    "            bootstrap_indices = np.random.choice(n_train, size=n_train, replace=True)\n",
    "            \n",
    "            # This is a simplified approach - in practice, you'd retrain or use dropout\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                # Add noise to simulate uncertainty\n",
    "                base_pred = model.predict_proba(X)\n",
    "                noise = np.random.normal(0, 0.01, base_pred.shape)\n",
    "                noisy_pred = np.clip(base_pred + noise, 0, 1)\n",
    "                # Renormalize\n",
    "                noisy_pred = noisy_pred / np.sum(noisy_pred, axis=1, keepdims=True)\n",
    "                predictions.append(noisy_pred)\n",
    "            else:\n",
    "                pred = model.predict(X)\n",
    "                predictions.append(pred)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        if len(predictions.shape) == 3:  # Probability predictions\n",
    "            mean_pred = np.mean(predictions, axis=0)\n",
    "            uncertainty = np.std(predictions, axis=0)\n",
    "            epistemic_uncertainty = np.mean(uncertainty, axis=1)\n",
    "        else:  # Hard predictions\n",
    "            # Calculate prediction variance\n",
    "            epistemic_uncertainty = []\n",
    "            for i in range(X.shape[0]):\n",
    "                sample_preds = predictions[:, i]\n",
    "                unique_preds, counts = np.unique(sample_preds, return_counts=True)\n",
    "                pred_probs = counts / len(sample_preds)\n",
    "                entropy = -np.sum(pred_probs * np.log(pred_probs + 1e-10))\n",
    "                epistemic_uncertainty.append(entropy)\n",
    "            \n",
    "            epistemic_uncertainty = np.array(epistemic_uncertainty)\n",
    "            mean_pred = None\n",
    "        \n",
    "        return {\n",
    "            'epistemic_uncertainty': epistemic_uncertainty,\n",
    "            'mean_prediction': mean_pred,\n",
    "            'all_predictions': predictions\n",
    "        }\n",
    "    \n",
    "    def ensemble_uncertainty(self, models, X):\n",
    "        \"\"\"Calculate uncertainty using ensemble disagreement.\"\"\"\n",
    "        print(\"  Calculating ensemble uncertainty...\")\n",
    "        \n",
    "        predictions = []\n",
    "        for model in models:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                pred = model.predict_proba(X)\n",
    "            else:\n",
    "                # Convert hard predictions to probabilities\n",
    "                hard_pred = model.predict(X)\n",
    "                n_classes = len(np.unique(hard_pred))\n",
    "                pred = np.zeros((len(X), n_classes))\n",
    "                for i, p in enumerate(hard_pred):\n",
    "                    pred[i, p] = 1.0\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # Mean prediction across ensemble\n",
    "        mean_pred = np.mean(predictions, axis=0)\n",
    "        \n",
    "        # Uncertainty measures\n",
    "        epistemic_uncertainty = np.var(predictions, axis=0).mean(axis=1)\n",
    "        \n",
    "        # Mutual information (simplified)\n",
    "        entropy_mean = -np.sum(mean_pred * np.log(mean_pred + 1e-10), axis=1)\n",
    "        mean_entropy = np.mean(-np.sum(predictions * np.log(predictions + 1e-10), axis=2), axis=0)\n",
    "        mutual_info = entropy_mean - mean_entropy\n",
    "        \n",
    "        return {\n",
    "            'epistemic_uncertainty': epistemic_uncertainty,\n",
    "            'mutual_information': mutual_info,\n",
    "            'mean_prediction': mean_pred,\n",
    "            'prediction_variance': np.var(predictions, axis=0)\n",
    "        }\n",
    "    \n",
    "    def calibration_analysis(self, model, X, y_true):\n",
    "        \"\"\"Analyze model calibration.\"\"\"\n",
    "        print(\"  Analyzing model calibration...\")\n",
    "        \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            proba = model.predict_proba(X)\n",
    "            \n",
    "            # For binary classification\n",
    "            if proba.shape[1] == 2:\n",
    "                predicted_proba = proba[:, 1]\n",
    "                \n",
    "                # Calibration curve\n",
    "                n_bins = 10\n",
    "                bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "                bin_lowers = bin_boundaries[:-1]\n",
    "                bin_uppers = bin_boundaries[1:]\n",
    "                \n",
    "                calibration_data = []\n",
    "                \n",
    "                for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "                    # Find predictions in this bin\n",
    "                    in_bin = (predicted_proba > bin_lower) & (predicted_proba <= bin_upper)\n",
    "                    prop_in_bin = in_bin.mean()\n",
    "                    \n",
    "                    if prop_in_bin > 0:\n",
    "                        accuracy_in_bin = y_true[in_bin].mean()\n",
    "                        avg_confidence_in_bin = predicted_proba[in_bin].mean()\n",
    "                        \n",
    "                        calibration_data.append({\n",
    "                            'bin_lower': bin_lower,\n",
    "                            'bin_upper': bin_upper,\n",
    "                            'accuracy': accuracy_in_bin,\n",
    "                            'confidence': avg_confidence_in_bin,\n",
    "                            'proportion': prop_in_bin\n",
    "                        })\n",
    "                \n",
    "                # Expected Calibration Error (ECE)\n",
    "                ece = 0\n",
    "                for data in calibration_data:\n",
    "                    ece += data['proportion'] * abs(data['accuracy'] - data['confidence'])\n",
    "                \n",
    "                return {\n",
    "                    'calibration_curve': calibration_data,\n",
    "                    'ece': ece,\n",
    "                    'predicted_probabilities': predicted_proba\n",
    "                }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def prediction_intervals(self, model, X, confidence_level=0.95):\n",
    "        \"\"\"Calculate prediction intervals using quantile regression approach.\"\"\"\n",
    "        print(f\"  Calculating {confidence_level*100}% prediction intervals...\")\n",
    "        \n",
    "        # Simplified approach using bootstrap\n",
    "        n_bootstrap = 100\n",
    "        predictions = []\n",
    "        \n",
    "        # Get base prediction\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            base_pred = model.predict_proba(X)\n",
    "        else:\n",
    "            base_pred = model.predict(X)\n",
    "        \n",
    "        # Bootstrap predictions (simplified simulation)\n",
    "        for _ in range(n_bootstrap):\n",
    "            # Add noise to simulate prediction uncertainty\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                noise = np.random.normal(0, 0.02, base_pred.shape)\n",
    "                noisy_pred = np.clip(base_pred + noise, 0, 1)\n",
    "                predictions.append(noisy_pred)\n",
    "            else:\n",
    "                # For hard predictions, occasionally flip\n",
    "                flip_prob = 0.05\n",
    "                noisy_pred = base_pred.copy()\n",
    "                flip_mask = np.random.random(len(noisy_pred)) < flip_prob\n",
    "                if np.any(flip_mask):\n",
    "                    # Flip to random class\n",
    "                    n_classes = len(np.unique(base_pred))\n",
    "                    noisy_pred[flip_mask] = np.random.randint(0, n_classes, np.sum(flip_mask))\n",
    "                predictions.append(noisy_pred)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # Calculate intervals\n",
    "        alpha = 1 - confidence_level\n",
    "        lower_percentile = (alpha / 2) * 100\n",
    "        upper_percentile = (1 - alpha / 2) * 100\n",
    "        \n",
    "        if len(predictions.shape) == 3:  # Probability predictions\n",
    "            lower_bound = np.percentile(predictions, lower_percentile, axis=0)\n",
    "            upper_bound = np.percentile(predictions, upper_percentile, axis=0)\n",
    "        else:  # Hard predictions\n",
    "            # Calculate class probability intervals\n",
    "            lower_bound = []\n",
    "            upper_bound = []\n",
    "            \n",
    "            for i in range(len(X)):\n",
    "                sample_preds = predictions[:, i]\n",
    "                unique_classes, counts = np.unique(sample_preds, return_counts=True)\n",
    "                probs = counts / len(sample_preds)\n",
    "                \n",
    "                # Use the range of probabilities as interval\n",
    "                lower_bound.append(np.min(probs))\n",
    "                upper_bound.append(np.max(probs))\n",
    "            \n",
    "            lower_bound = np.array(lower_bound)\n",
    "            upper_bound = np.array(upper_bound)\n",
    "        \n",
    "        return {\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'mean_prediction': np.mean(predictions, axis=0),\n",
    "            'confidence_level': confidence_level\n",
    "        }\n",
    "\n",
    "class BayesianClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Simplified Bayesian classifier for uncertainty quantification.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator=None, n_estimators=10):\n",
    "        self.base_estimator = base_estimator or LogisticRegression(random_state=42)\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimators_ = []\n",
    "        self.classes_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.classes_ = unique_labels(y)\n",
    "        \n",
    "        # Train multiple models with bootstrap sampling\n",
    "        for i in range(self.n_estimators):\n",
    "            # Bootstrap sample\n",
    "            n_samples = len(X)\n",
    "            bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_bootstrap = X[bootstrap_indices]\n",
    "            y_bootstrap = y[bootstrap_indices]\n",
    "            \n",
    "            # Train estimator\n",
    "            estimator = self.base_estimator.__class__(**self.base_estimator.get_params())\n",
    "            estimator.fit(X_bootstrap, y_bootstrap)\n",
    "            self.estimators_.append(estimator)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        check_array(X)\n",
    "        \n",
    "        # Get predictions from all estimators\n",
    "        predictions = []\n",
    "        for estimator in self.estimators_:\n",
    "            if hasattr(estimator, 'predict_proba'):\n",
    "                pred = estimator.predict_proba(X)\n",
    "            else:\n",
    "                # Convert hard predictions to probabilities\n",
    "                hard_pred = estimator.predict(X)\n",
    "                pred = np.zeros((len(X), len(self.classes_)))\n",
    "                for i, p in enumerate(hard_pred):\n",
    "                    class_idx = np.where(self.classes_ == p)[0][0]\n",
    "                    pred[i, class_idx] = 1.0\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Average predictions\n",
    "        return np.mean(predictions, axis=0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(proba, axis=1)]\n",
    "    \n",
    "    def predict_with_uncertainty(self, X):\n",
    "        \"\"\"Predict with uncertainty estimates.\"\"\"\n",
    "        predictions = []\n",
    "        for estimator in self.estimators_:\n",
    "            if hasattr(estimator, 'predict_proba'):\n",
    "                pred = estimator.predict_proba(X)\n",
    "            else:\n",
    "                hard_pred = estimator.predict(X)\n",
    "                pred = np.zeros((len(X), len(self.classes_)))\n",
    "                for i, p in enumerate(hard_pred):\n",
    "                    class_idx = np.where(self.classes_ == p)[0][0]\n",
    "                    pred[i, class_idx] = 1.0\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # Mean and uncertainty\n",
    "        mean_pred = np.mean(predictions, axis=0)\n",
    "        uncertainty = np.std(predictions, axis=0).mean(axis=1)\n",
    "        \n",
    "        return mean_pred, uncertainty\n",
    "\n",
    "# Generate data for uncertainty quantification\n",
    "X_unc, y_unc = generator.classification_dataset(\n",
    "    n_samples=1000,\n",
    "    n_features=15,\n",
    "    n_informative=10,\n",
    "    n_classes=2,\n",
    "    class_sep=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train_unc, X_test_unc, y_train_unc, y_test_unc = train_test_split(\n",
    "    X_unc, y_unc, test_size=0.3, random_state=42, stratify=y_unc\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler_unc = StandardScaler()\n",
    "X_train_unc_scaled = scaler_unc.fit_transform(X_train_unc)\n",
    "X_test_unc_scaled = scaler_unc.transform(X_test_unc)\n",
    "\n",
    "print(f\"Uncertainty Quantification Dataset: {X_train_unc_scaled.shape}\")\n",
    "\n",
    "# Test uncertainty quantification methods\n",
    "uncertainty_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Bayesian Classifier': BayesianClassifier(LogisticRegression(random_state=42), n_estimators=20)\n",
    "}\n",
    "\n",
    "uncertainty_results = {}\n",
    "\n",
    "for model_name, model in uncertainty_models.items():\n",
    "    print(f\"\\n--- Uncertainty Analysis: {model_name} ---\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_unc_scaled, y_train_unc)\n",
    "    test_score = model.score(X_test_unc_scaled, y_test_unc)\n",
    "    print(f\"Test Accuracy: {test_score:.4f}\")\n",
    "    \n",
    "    # Initialize uncertainty quantifier\n",
    "    quantifier = UncertaintyQuantifier()\n",
    "    \n",
    "    # Monte Carlo uncertainty\n",
    "    mc_uncertainty = quantifier.monte_carlo_dropout_uncertainty(model, X_test_unc_scaled, n_samples=50)\n",
    "    \n",
    "    # Calibration analysis\n",
    "    calibration_results = quantifier.calibration_analysis(model, X_test_unc_scaled, y_test_unc)\n",
    "    \n",
    "    # Prediction intervals\n",
    "    prediction_intervals = quantifier.prediction_intervals(model, X_test_unc_scaled)\n",
    "    \n",
    "    # Special analysis for Bayesian classifier\n",
    "    if hasattr(model, 'predict_with_uncertainty'):\n",
    "        bayesian_pred, bayesian_unc = model.predict_with_uncertainty(X_test_unc_scaled)\n",
    "    else:\n",
    "        bayesian_pred, bayesian_unc = None, None\n",
    "    \n",
    "    uncertainty_results[model_name] = {\n",
    "        'test_score': test_score,\n",
    "        'mc_uncertainty': mc_uncertainty,\n",
    "        'calibration': calibration_results,\n",
    "        'prediction_intervals': prediction_intervals,\n",
    "        'bayesian_uncertainty': bayesian_unc,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "# Ensemble uncertainty analysis\n",
    "ensemble_models = [uncertainty_models['Random Forest'], \n",
    "                  uncertainty_models['Logistic Regression']]\n",
    "\n",
    "if len(ensemble_models) > 1:\n",
    "    print(f\"\\n--- Ensemble Uncertainty Analysis ---\")\n",
    "    quantifier = UncertaintyQuantifier()\n",
    "    ensemble_uncertainty = quantifier.ensemble_uncertainty(ensemble_models, X_test_unc_scaled)\n",
    "    uncertainty_results['Ensemble'] = {\n",
    "        'ensemble_uncertainty': ensemble_uncertainty\n",
    "    }\n",
    "\n",
    "# Save uncertainty results\n",
    "uncertainty_summary = {}\n",
    "for model_name, results in uncertainty_results.items():\n",
    "    if 'test_score' in results:\n",
    "        uncertainty_summary[model_name] = {\n",
    "            'test_score': results['test_score'],\n",
    "            'avg_epistemic_uncertainty': np.mean(results['mc_uncertainty']['epistemic_uncertainty']),\n",
    "            'calibration_ece': results['calibration']['ece'] if results['calibration'] else None\n",
    "        }\n",
    "\n",
    "save_experiment_results('uncertainty_quantification', uncertainty_summary,\n",
    "                       'Comprehensive uncertainty quantification analysis', 'uncertainty')\n",
    "\n",
    "print(\"\\n‚ú® Uncertainty quantification analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3028c73",
   "metadata": {},
   "source": [
    "### Uncertainty Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf241b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uncertainty quantification results\n",
    "print(\"üìä Visualizing Uncertainty Quantification...\")\n",
    "\n",
    "if uncertainty_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Epistemic uncertainty comparison\n",
    "    model_names = []\n",
    "    avg_uncertainties = []\n",
    "    \n",
    "    for model_name, results in uncertainty_results.items():\n",
    "        if 'mc_uncertainty' in results:\n",
    "            model_names.append(model_name)\n",
    "            epistemic_unc = results['mc_uncertainty']['epistemic_uncertainty']\n",
    "            avg_uncertainties.append(np.mean(epistemic_unc))\n",
    "    \n",
    "    if model_names:\n",
    "        bars = axes[0, 0].bar(range(len(model_names)), avg_uncertainties, \n",
    "                             color=['lightblue', 'lightgreen', 'lightcoral'][:len(model_names)], \n",
    "                             alpha=0.7)\n",
    "        axes[0, 0].set_xlabel('Model')\n",
    "        axes[0, 0].set_ylabel('Average Epistemic Uncertainty')\n",
    "        axes[0, 0].set_title('Model Uncertainty Comparison')\n",
    "        axes[0, 0].set_xticks(range(len(model_names)))\n",
    "        axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, avg_uncertainties):\n",
    "            axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(avg_uncertainties)*0.01, \n",
    "                           f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Calibration curves\n",
    "    calibration_models = [(name, results) for name, results in uncertainty_results.items() \n",
    "                         if results.get('calibration') is not None]\n",
    "    \n",
    "    if calibration_models:\n",
    "        # Plot perfect calibration line\n",
    "        axes[0, 1].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "        \n",
    "        colors = ['blue', 'green', 'red', 'orange', 'purple']\n",
    "        for i, (model_name, results) in enumerate(calibration_models):\n",
    "            cal_data = results['calibration']['calibration_curve']\n",
    "            \n",
    "            if cal_data:\n",
    "                confidences = [d['confidence'] for d in cal_data]\n",
    "                accuracies = [d['accuracy'] for d in cal_data]\n",
    "                \n",
    "                axes[0, 1].plot(confidences, accuracies, 'o-', \n",
    "                               color=colors[i % len(colors)], \n",
    "                               label=f\"{model_name} (ECE: {results['calibration']['ece']:.3f})\")\n",
    "        \n",
    "        axes[0, 1].set_xlabel('Mean Predicted Probability')\n",
    "        axes[0, 1].set_ylabel('Fraction of Positives')\n",
    "        axes[0, 1].set_title('Calibration Curves')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Uncertainty vs Accuracy scatter\n",
    "    if 'Random Forest' in uncertainty_results:\n",
    "        rf_results = uncertainty_results['Random Forest']\n",
    "        epistemic_unc = rf_results['mc_uncertainty']['epistemic_uncertainty']\n",
    "        \n",
    "        # Calculate per-sample accuracy\n",
    "        rf_model = rf_results['model']\n",
    "        predictions = rf_model.predict(X_test_unc_scaled)\n",
    "        per_sample_correct = (predictions == y_test_unc).astype(int)\n",
    "        \n",
    "        # Scatter plot\n",
    "        scatter = axes[1, 0].scatter(epistemic_unc, per_sample_correct, \n",
    "                                    alpha=0.6, c=epistemic_unc, cmap='viridis')\n",
    "        axes[1, 0].set_xlabel('Epistemic Uncertainty')\n",
    "        axes[1, 0].set_ylabel('Prediction Correct (0/1)')\n",
    "        axes[1, 0].set_title('Uncertainty vs Prediction Accuracy')\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(epistemic_unc, per_sample_correct, 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[1, 0].plot(sorted(epistemic_unc), p(sorted(epistemic_unc)), \"r--\", alpha=0.8)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
    "        cbar.set_label('Epistemic Uncertainty')\n",
    "    \n",
    "    # 4. Prediction intervals visualization\n",
    "    if 'Random Forest' in uncertainty_results:\n",
    "        rf_results = uncertainty_results['Random Forest']\n",
    "        pred_intervals = rf_results['prediction_intervals']\n",
    "        \n",
    "        # Show intervals for first 20 samples\n",
    "        n_show = min(20, len(X_test_unc_scaled))\n",
    "        indices = range(n_show)\n",
    "        \n",
    "        if hasattr(pred_intervals['mean_prediction'], 'shape') and len(pred_intervals['mean_prediction'].shape) > 1:\n",
    "            # Probability predictions - show for positive class\n",
    "            mean_pred = pred_intervals['mean_prediction'][:n_show, 1]\n",
    "            lower_bound = pred_intervals['lower_bound'][:n_show, 1]\n",
    "            upper_bound = pred_intervals['upper_bound'][:n_show, 1]\n",
    "        else:\n",
    "            # Use full prediction if available\n",
    "            mean_pred = pred_intervals['mean_prediction'][:n_show]\n",
    "            lower_bound = pred_intervals['lower_bound'][:n_show]\n",
    "            upper_bound = pred_intervals['upper_bound'][:n_show]\n",
    "        \n",
    "        # Plot prediction intervals\n",
    "        axes[1, 1].errorbar(indices, mean_pred, \n",
    "                           yerr=[mean_pred - lower_bound, upper_bound - mean_pred],\n",
    "                           fmt='o', capsize=3, alpha=0.7)\n",
    "        \n",
    "        # Add true labels for comparison\n",
    "        true_labels = y_test_unc[:n_show]\n",
    "        axes[1, 1].scatter(indices, true_labels, color='red', marker='x', s=50, \n",
    "                          label='True Labels', alpha=0.8)\n",
    "        \n",
    "        axes[1, 1].set_xlabel('Sample Index')\n",
    "        axes[1, 1].set_ylabel('Prediction / True Label')\n",
    "        axes[1, 1].set_title('Prediction Intervals (95% Confidence)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save uncertainty visualization\n",
    "    save_figure(fig, 'uncertainty_quantification_analysis',\n",
    "               'Comprehensive uncertainty quantification analysis and visualization', 'uncertainty')\n",
    "    plt.show()\n",
    "    \n",
    "    # Uncertainty summary\n",
    "    print(f\"\\nüìä Uncertainty Quantification Summary:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for model_name, results in uncertainty_results.items():\n",
    "        if 'test_score' in results:\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(f\"  Test Accuracy: {results['test_score']:.4f}\")\n",
    "            \n",
    "            if 'mc_uncertainty' in results:\n",
    "                avg_unc = np.mean(results['mc_uncertainty']['epistemic_uncertainty'])\n",
    "                print(f\"  Average Epistemic Uncertainty: {avg_unc:.4f}\")\n",
    "            \n",
    "            if results.get('calibration'):\n",
    "                ece = results['calibration']['ece']\n",
    "                print(f\"  Expected Calibration Error: {ece:.4f}\")\n",
    "            \n",
    "            if results.get('bayesian_uncertainty') is not None:\n",
    "                avg_bayesian_unc = np.mean(results['bayesian_uncertainty'])\n",
    "                print(f\"  Average Bayesian Uncertainty: {avg_bayesian_unc:.4f}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚ú® Uncertainty quantification visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c716df",
   "metadata": {},
   "source": [
    "## 7. Advanced Ensemble Methods {#ensembles}\n",
    "\n",
    "Reference to previously implemented ensemble methods with advanced uncertainty integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced ensemble diversity analysis with uncertainty integration\n",
    "print(\"\\nüî¨ Enhanced Ensemble Diversity Analysis with Uncertainty...\")\n",
    "\n",
    "def calculate_enhanced_diversity_metrics(models_dict, X, y_true, include_uncertainty=True):\n",
    "    \"\"\"Calculate comprehensive diversity metrics including uncertainty-aware measures.\"\"\"\n",
    "    \n",
    "    model_names = list(models_dict.keys())\n",
    "    n_models = len(model_names)\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    # Get predictions and uncertainties from all models\n",
    "    predictions = {}\n",
    "    uncertainties = {}\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        pred = model.predict(X)\n",
    "        predictions[name] = pred\n",
    "        \n",
    "        if include_uncertainty and hasattr(model, 'predict_proba'):\n",
    "            proba = model.predict_proba(X)\n",
    "            # Calculate prediction uncertainty (entropy)\n",
    "            uncertainty = -np.sum(proba * np.log(proba + 1e-10), axis=1)\n",
    "            uncertainties[name] = uncertainty\n",
    "        else:\n",
    "            uncertainties[name] = np.zeros(n_samples)\n",
    "    \n",
    "    diversity_metrics = {}\n",
    "    \n",
    "    # 1. Classical Disagreement Measure\n",
    "    disagreement_matrix = np.zeros((n_models, n_models))\n",
    "    for i, model_i in enumerate(model_names):\n",
    "        for j, model_j in enumerate(model_names):\n",
    "            if i != j:\n",
    "                disagreement = np.mean(predictions[model_i] != predictions[model_j])\n",
    "                disagreement_matrix[i, j] = disagreement\n",
    "    \n",
    "    diversity_metrics['disagreement_matrix'] = disagreement_matrix\n",
    "    diversity_metrics['avg_disagreement'] = np.mean(disagreement_matrix[disagreement_matrix > 0])\n",
    "    \n",
    "    # 2. Q-statistic (Yule's Q) with uncertainty weighting\n",
    "    q_statistics = []\n",
    "    uncertainty_weighted_q = []\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        for j in range(i + 1, n_models):\n",
    "            model_i, model_j = model_names[i], model_names[j]\n",
    "            \n",
    "            # Standard Q-statistic\n",
    "            correct_i = (predictions[model_i] == y_true)\n",
    "            correct_j = (predictions[model_j] == y_true)\n",
    "            \n",
    "            both_correct = np.sum(correct_i & correct_j)\n",
    "            both_wrong = np.sum(~correct_i & ~correct_j)\n",
    "            i_correct_j_wrong = np.sum(correct_i & ~correct_j)\n",
    "            i_wrong_j_correct = np.sum(~correct_i & correct_j)\n",
    "            \n",
    "            denominator = both_correct * both_wrong + i_correct_j_wrong * i_wrong_j_correct\n",
    "            if denominator != 0:\n",
    "                q_stat = (both_correct * both_wrong - i_correct_j_wrong * i_wrong_j_correct) / denominator\n",
    "                q_statistics.append(q_stat)\n",
    "                \n",
    "                # Uncertainty-weighted Q-statistic\n",
    "                if include_uncertainty:\n",
    "                    uncertainty_weights = 1.0 / (uncertainties[model_i] + uncertainties[model_j] + 1e-8)\n",
    "                    weighted_q = np.average(\n",
    "                        [q_stat] * len(uncertainty_weights), \n",
    "                        weights=uncertainty_weights\n",
    "                    )\n",
    "                    uncertainty_weighted_q.append(weighted_q)\n",
    "    \n",
    "    diversity_metrics['q_statistics'] = q_statistics\n",
    "    diversity_metrics['avg_q_statistic'] = np.mean(q_statistics) if q_statistics else 0\n",
    "    \n",
    "    if include_uncertainty and uncertainty_weighted_q:\n",
    "        diversity_metrics['uncertainty_weighted_q'] = np.mean(uncertainty_weighted_q)\n",
    "    \n",
    "    # 3. Prediction Entropy Diversity\n",
    "    sample_entropies = []\n",
    "    uncertainty_weighted_entropies = []\n",
    "    \n",
    "    for sample_idx in range(n_samples):\n",
    "        sample_preds = [predictions[model][sample_idx] for model in model_names]\n",
    "        unique_preds, counts = np.unique(sample_preds, return_counts=True)\n",
    "        probabilities = counts / n_models\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "        sample_entropies.append(entropy)\n",
    "        \n",
    "        if include_uncertainty:\n",
    "            # Weight entropy by inverse uncertainty (more weight to confident predictions)\n",
    "            sample_uncertainties = [uncertainties[model][sample_idx] for model in model_names]\n",
    "            weights = 1.0 / (np.array(sample_uncertainties) + 1e-8)\n",
    "            weights = weights / np.sum(weights)\n",
    "            weighted_entropy = entropy * np.mean(weights)\n",
    "            uncertainty_weighted_entropies.append(weighted_entropy)\n",
    "    \n",
    "    diversity_metrics['sample_entropies'] = sample_entropies\n",
    "    diversity_metrics['avg_entropy'] = np.mean(sample_entropies)\n",
    "    \n",
    "    if include_uncertainty:\n",
    "        diversity_metrics['uncertainty_weighted_entropy'] = np.mean(uncertainty_weighted_entropies)\n",
    "    \n",
    "    # 4. Correlation-based Diversity\n",
    "    correlation_matrix = np.zeros((n_models, n_models))\n",
    "    for i, model_i in enumerate(model_names):\n",
    "        for j, model_j in enumerate(model_names):\n",
    "            if i != j:\n",
    "                correct_i = (predictions[model_i] == y_true).astype(int)\n",
    "                correct_j = (predictions[model_j] == y_true).astype(int)\n",
    "                \n",
    "                if np.std(correct_i) > 0 and np.std(correct_j) > 0:\n",
    "                    correlation = np.corrcoef(correct_i, correct_j)[0, 1]\n",
    "                    correlation_matrix[i, j] = correlation\n",
    "    \n",
    "    diversity_metrics['correlation_matrix'] = correlation_matrix\n",
    "    diversity_metrics['avg_correlation'] = np.mean(correlation_matrix[correlation_matrix != 0])\n",
    "    \n",
    "    # 5. Uncertainty-Based Diversity Metrics\n",
    "    if include_uncertainty:\n",
    "        # Uncertainty disagreement\n",
    "        uncertainty_disagreement = 0\n",
    "        pair_count = 0\n",
    "        \n",
    "        for i in range(n_models):\n",
    "            for j in range(i + 1, n_models):\n",
    "                model_i, model_j = model_names[i], model_names[j]\n",
    "                # Measure how differently models are uncertain\n",
    "                unc_diff = np.mean(np.abs(uncertainties[model_i] - uncertainties[model_j]))\n",
    "                uncertainty_disagreement += unc_diff\n",
    "                pair_count += 1\n",
    "        \n",
    "        diversity_metrics['uncertainty_disagreement'] = uncertainty_disagreement / pair_count if pair_count > 0 else 0\n",
    "        \n",
    "        # Confidence diversity\n",
    "        confidence_diversity = []\n",
    "        for sample_idx in range(n_samples):\n",
    "            sample_uncertainties = [uncertainties[model][sample_idx] for model in model_names]\n",
    "            conf_std = np.std(sample_uncertainties)\n",
    "            confidence_diversity.append(conf_std)\n",
    "        \n",
    "        diversity_metrics['avg_confidence_diversity'] = np.mean(confidence_diversity)\n",
    "    \n",
    "    return diversity_metrics, model_names\n",
    "\n",
    "# Test enhanced diversity analysis\n",
    "if 'uncertainty_results' in locals() and uncertainty_results:\n",
    "    # Prepare models dictionary\n",
    "    diversity_models = {}\n",
    "    for model_name, results in uncertainty_results.items():\n",
    "        if 'model' in results:\n",
    "            diversity_models[model_name] = results['model']\n",
    "    \n",
    "    if len(diversity_models) >= 2:\n",
    "        enhanced_diversity, model_names = calculate_enhanced_diversity_metrics(\n",
    "            diversity_models, X_test_unc_scaled, y_test_unc, include_uncertainty=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìä Enhanced Diversity Metrics:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Average Disagreement: {enhanced_diversity['avg_disagreement']:.4f}\")\n",
    "        print(f\"Average Q-statistic: {enhanced_diversity['avg_q_statistic']:.4f}\")\n",
    "        print(f\"Average Correlation: {enhanced_diversity['avg_correlation']:.4f}\")\n",
    "        print(f\"Average Entropy: {enhanced_diversity['avg_entropy']:.4f}\")\n",
    "        \n",
    "        if 'uncertainty_weighted_q' in enhanced_diversity:\n",
    "            print(f\"Uncertainty-Weighted Q: {enhanced_diversity['uncertainty_weighted_q']:.4f}\")\n",
    "        if 'uncertainty_disagreement' in enhanced_diversity:\n",
    "            print(f\"Uncertainty Disagreement: {enhanced_diversity['uncertainty_disagreement']:.4f}\")\n",
    "        if 'avg_confidence_diversity' in enhanced_diversity:\n",
    "            print(f\"Confidence Diversity: {enhanced_diversity['avg_confidence_diversity']:.4f}\")\n",
    "        \n",
    "        # Visualize enhanced diversity\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Enhanced disagreement matrix with uncertainty\n",
    "        disagreement_matrix = enhanced_diversity['disagreement_matrix']\n",
    "        im1 = axes[0, 0].imshow(disagreement_matrix, cmap='RdYlBu_r', aspect='auto')\n",
    "        axes[0, 0].set_xticks(range(len(model_names)))\n",
    "        axes[0, 0].set_yticks(range(len(model_names)))\n",
    "        axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        axes[0, 0].set_yticklabels(model_names)\n",
    "        axes[0, 0].set_title('Model Disagreement Matrix')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(len(model_names)):\n",
    "                text = axes[0, 0].text(j, i, f'{disagreement_matrix[i, j]:.2f}',\n",
    "                                     ha=\"center\", va=\"center\", \n",
    "                                     color=\"white\" if disagreement_matrix[i, j] > 0.5 else \"black\")\n",
    "        \n",
    "        plt.colorbar(im1, ax=axes[0, 0])\n",
    "        \n",
    "        # 2. Uncertainty vs Performance scatter\n",
    "        if 'uncertainty_disagreement' in enhanced_diversity:\n",
    "            model_uncertainties = []\n",
    "            model_performances = []\n",
    "            \n",
    "            for name in model_names:\n",
    "                if name in uncertainty_results and 'test_score' in uncertainty_results[name]:\n",
    "                    # Calculate average uncertainty for this model\n",
    "                    model = diversity_models[name]\n",
    "                    if hasattr(model, 'predict_proba'):\n",
    "                        proba = model.predict_proba(X_test_unc_scaled)\n",
    "                        uncertainty = -np.sum(proba * np.log(proba + 1e-10), axis=1)\n",
    "                        avg_uncertainty = np.mean(uncertainty)\n",
    "                    else:\n",
    "                        avg_uncertainty = 0.5\n",
    "                    \n",
    "                    performance = uncertainty_results[name]['test_score']\n",
    "                    model_uncertainties.append(avg_uncertainty)\n",
    "                    model_performances.append(performance)\n",
    "            \n",
    "            if model_uncertainties and model_performances:\n",
    "                axes[0, 1].scatter(model_uncertainties, model_performances, s=100, alpha=0.7)\n",
    "                axes[0, 1].set_xlabel('Average Model Uncertainty')\n",
    "                axes[0, 1].set_ylabel('Model Performance')\n",
    "                axes[0, 1].set_title('Uncertainty vs Performance')\n",
    "                axes[0, 1].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add model labels\n",
    "                for i, name in enumerate(model_names[:len(model_uncertainties)]):\n",
    "                    axes[0, 1].annotate(name, (model_uncertainties[i], model_performances[i]),\n",
    "                                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        # 3. Diversity evolution with ensemble size\n",
    "        subset_diversities = []\n",
    "        subset_sizes = []\n",
    "        \n",
    "        for size in range(2, len(model_names) + 1):\n",
    "            subset_models = dict(list(diversity_models.items())[:size])\n",
    "            subset_div, _ = calculate_enhanced_diversity_metrics(\n",
    "                subset_models, X_test_unc_scaled, y_test_unc, include_uncertainty=False\n",
    "            )\n",
    "            subset_diversities.append(subset_div['avg_disagreement'])\n",
    "            subset_sizes.append(size)\n",
    "        \n",
    "        axes[1, 0].plot(subset_sizes, subset_diversities, 'o-', linewidth=2, markersize=6)\n",
    "        axes[1, 0].set_xlabel('Ensemble Size')\n",
    "        axes[1, 0].set_ylabel('Average Disagreement')\n",
    "        axes[1, 0].set_title('Diversity vs Ensemble Size')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Correlation matrix\n",
    "        correlation_matrix = enhanced_diversity['correlation_matrix']\n",
    "        im2 = axes[1, 1].imshow(correlation_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "        axes[1, 1].set_xticks(range(len(model_names)))\n",
    "        axes[1, 1].set_yticks(range(len(model_names)))\n",
    "        axes[1, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        axes[1, 1].set_yticklabels(model_names)\n",
    "        axes[1, 1].set_title('Model Correlation Matrix')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(len(model_names)):\n",
    "                if i != j:\n",
    "                    text = axes[1, 1].text(j, i, f'{correlation_matrix[i, j]:.2f}',\n",
    "                                         ha=\"center\", va=\"center\", \n",
    "                                         color=\"white\" if abs(correlation_matrix[i, j]) > 0.5 else \"black\")\n",
    "        \n",
    "        plt.colorbar(im2, ax=axes[1, 1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_figure(fig, 'enhanced_ensemble_diversity_analysis',\n",
    "                   'Enhanced ensemble diversity analysis with uncertainty integration', 'uncertainty')\n",
    "        plt.show()\n",
    "\n",
    "print(\"‚ú® Enhanced ensemble diversity analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-level stacking and advanced ensemble architectures\n",
    "print(\"\\nüèóÔ∏è Multi-Level Stacking and Advanced Ensemble Architectures...\")\n",
    "\n",
    "class MultiLevelStackingAdvanced(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Advanced multi-level stacking with uncertainty integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, level1_estimators, level2_estimators, final_estimator, \n",
    "                 cv=5, use_probabilities=True, uncertainty_weighting=False):\n",
    "        self.level1_estimators = level1_estimators\n",
    "        self.level2_estimators = level2_estimators\n",
    "        self.final_estimator = final_estimator\n",
    "        self.cv = cv\n",
    "        self.use_probabilities = use_probabilities\n",
    "        self.uncertainty_weighting = uncertainty_weighting\n",
    "        self.classes_ = None\n",
    "        self.level1_fitted_ = {}\n",
    "        self.level2_fitted_ = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.classes_ = unique_labels(y)\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        # Level 1: Generate meta-features from base estimators\n",
    "        if self.use_probabilities:\n",
    "            level1_meta = np.zeros((X.shape[0], len(self.level1_estimators) * n_classes))\n",
    "        else:\n",
    "            level1_meta = np.zeros((X.shape[0], len(self.level1_estimators)))\n",
    "        \n",
    "        level1_uncertainties = np.zeros((X.shape[0], len(self.level1_estimators)))\n",
    "        \n",
    "        col_idx = 0\n",
    "        for i, (name, estimator) in enumerate(self.level1_estimators.items()):\n",
    "            if self.use_probabilities and hasattr(estimator, 'predict_proba'):\n",
    "                cv_pred = cross_val_predict(estimator, X, y, cv=self.cv, method='predict_proba')\n",
    "                level1_meta[:, col_idx:col_idx+n_classes] = cv_pred\n",
    "                col_idx += n_classes\n",
    "                \n",
    "                # Calculate uncertainties\n",
    "                uncertainty = -np.sum(cv_pred * np.log(cv_pred + 1e-10), axis=1)\n",
    "                level1_uncertainties[:, i] = uncertainty\n",
    "            else:\n",
    "                cv_pred = cross_val_predict(estimator, X, y, cv=self.cv)\n",
    "                if self.use_probabilities:\n",
    "                    # Convert to one-hot encoding\n",
    "                    onehot = np.zeros((len(cv_pred), n_classes))\n",
    "                    for j, pred in enumerate(cv_pred):\n",
    "                        class_idx = np.where(self.classes_ == pred)[0][0]\n",
    "                        onehot[j, class_idx] = 1.0\n",
    "                    level1_meta[:, col_idx:col_idx+n_classes] = onehot\n",
    "                    col_idx += n_classes\n",
    "                else:\n",
    "                    level1_meta[:, i] = cv_pred\n",
    "                \n",
    "                # Default uncertainty for hard predictions\n",
    "                level1_uncertainties[:, i] = 0.5\n",
    "            \n",
    "            # Fit estimator on full data\n",
    "            estimator.fit(X, y)\n",
    "            self.level1_fitted_[name] = estimator\n",
    "        \n",
    "        # Level 2: Generate meta-features from level 1 meta-features\n",
    "        if self.use_probabilities:\n",
    "            level2_meta = np.zeros((X.shape[0], len(self.level2_estimators) * n_classes))\n",
    "        else:\n",
    "            level2_meta = np.zeros((X.shape[0], len(self.level2_estimators)))\n",
    "        \n",
    "        # Add uncertainty features if enabled\n",
    "        if self.uncertainty_weighting:\n",
    "            uncertainty_features = level1_uncertainties\n",
    "            combined_level1_meta = np.column_stack([level1_meta, uncertainty_features])\n",
    "        else:\n",
    "            combined_level1_meta = level1_meta\n",
    "        \n",
    "        col_idx = 0\n",
    "        for i, (name, estimator) in enumerate(self.level2_estimators.items()):\n",
    "            if self.use_probabilities and hasattr(estimator, 'predict_proba'):\n",
    "                cv_pred = cross_val_predict(estimator, combined_level1_meta, y, cv=self.cv, method='predict_proba')\n",
    "                level2_meta[:, col_idx:col_idx+n_classes] = cv_pred\n",
    "                col_idx += n_classes\n",
    "            else:\n",
    "                cv_pred = cross_val_predict(estimator, combined_level1_meta, y, cv=self.cv)\n",
    "                if self.use_probabilities:\n",
    "                    # Convert to one-hot encoding\n",
    "                    onehot = np.zeros((len(cv_pred), n_classes))\n",
    "                    for j, pred in enumerate(cv_pred):\n",
    "                        class_idx = np.where(self.classes_ == pred)[0][0]\n",
    "                        onehot[j, class_idx] = 1.0\n",
    "                    level2_meta[:, col_idx:col_idx+n_classes] = onehot\n",
    "                    col_idx += n_classes\n",
    "                else:\n",
    "                    level2_meta[:, i] = cv_pred\n",
    "            \n",
    "            # Fit estimator on level 1 meta-features\n",
    "            estimator.fit(combined_level1_meta, y)\n",
    "            self.level2_fitted_[name] = estimator\n",
    "        \n",
    "        # Final level: Fit final estimator on level 2 meta-features\n",
    "        self.final_estimator.fit(level2_meta, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _generate_level1_features(self, X):\n",
    "        \"\"\"Generate level 1 meta-features for prediction.\"\"\"\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        if self.use_probabilities:\n",
    "            level1_meta = np.zeros((X.shape[0], len(self.level1_estimators) * n_classes))\n",
    "        else:\n",
    "            level1_meta = np.zeros((X.shape[0], len(self.level1_estimators)))\n",
    "        \n",
    "        level1_uncertainties = np.zeros((X.shape[0], len(self.level1_estimators)))\n",
    "        \n",
    "        col_idx = 0\n",
    "        for i, (name, estimator) in enumerate(self.level1_fitted_.items()):\n",
    "            if self.use_probabilities and hasattr(estimator, 'predict_proba'):\n",
    "                pred_proba = estimator.predict_proba(X)\n",
    "                level1_meta[:, col_idx:col_idx+n_classes] = pred_proba\n",
    "                col_idx += n_classes\n",
    "                \n",
    "                # Calculate uncertainties\n",
    "                uncertainty = -np.sum(pred_proba * np.log(pred_proba + 1e-10), axis=1)\n",
    "                level1_uncertainties[:, i] = uncertainty\n",
    "            else:\n",
    "                pred = estimator.predict(X)\n",
    "                if self.use_probabilities:\n",
    "                    # Convert to one-hot encoding\n",
    "                    onehot = np.zeros((len(pred), n_classes))\n",
    "                    for j, p in enumerate(pred):\n",
    "                        class_idx = np.where(self.classes_ == p)[0][0]\n",
    "                        onehot[j, class_idx] = 1.0\n",
    "                    level1_meta[:, col_idx:col_idx+n_classes] = onehot\n",
    "                    col_idx += n_classes\n",
    "                else:\n",
    "                    level1_meta[:, i] = pred\n",
    "                \n",
    "                level1_uncertainties[:, i] = 0.5\n",
    "        \n",
    "        return level1_meta, level1_uncertainties\n",
    "    \n",
    "    def predict(self, X):\n",
    "        check_array(X)\n",
    "        \n",
    "        # Generate level 1 meta-features\n",
    "        level1_meta, level1_uncertainties = self._generate_level1_features(X)\n",
    "        \n",
    "        # Combine with uncertainty features if enabled\n",
    "        if self.uncertainty_weighting:\n",
    "            combined_level1_meta = np.column_stack([level1_meta, level1_uncertainties])\n",
    "        else:\n",
    "            combined_level1_meta = level1_meta\n",
    "        \n",
    "        # Generate level 2 meta-features\n",
    "        n_classes = len(self.classes_)\n",
    "        if self.use_probabilities:\n",
    "            level2_meta = np.zeros((X.shape[0], len(self.level2_estimators) * n_classes))\n",
    "        else:\n",
    "            level2_meta = np.zeros((X.shape[0], len(self.level2_estimators)))\n",
    "        \n",
    "        col_idx = 0\n",
    "        for i, (name, estimator) in enumerate(self.level2_fitted_.items()):\n",
    "            if self.use_probabilities and hasattr(estimator, 'predict_proba'):\n",
    "                pred_proba = estimator.predict_proba(combined_level1_meta)\n",
    "                level2_meta[:, col_idx:col_idx+n_classes] = pred_proba\n",
    "                col_idx += n_classes\n",
    "            else:\n",
    "                pred = estimator.predict(combined_level1_meta)\n",
    "                if self.use_probabilities:\n",
    "                    # Convert to one-hot encoding\n",
    "                    onehot = np.zeros((len(pred), n_classes))\n",
    "                    for j, p in enumerate(pred):\n",
    "                        class_idx = np.where(self.classes_ == p)[0][0]\n",
    "                        onehot[j, class_idx] = 1.0\n",
    "                    level2_meta[:, col_idx:col_idx+n_classes] = onehot\n",
    "                    col_idx += n_classes\n",
    "                else:\n",
    "                    level2_meta[:, i] = pred\n",
    "        \n",
    "        # Final prediction\n",
    "        return self.final_estimator.predict(level2_meta)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if hasattr(self.final_estimator, 'predict_proba'):\n",
    "            check_array(X)\n",
    "            \n",
    "            # Generate features through both levels\n",
    "            level1_meta, level1_uncertainties = self._generate_level1_features(X)\n",
    "            \n",
    "            if self.uncertainty_weighting:\n",
    "                combined_level1_meta = np.column_stack([level1_meta, level1_uncertainties])\n",
    "            else:\n",
    "                combined_level1_meta = level1_meta\n",
    "            \n",
    "            # Generate level 2 meta-features\n",
    "            n_classes = len(self.classes_)\n",
    "            if self.use_probabilities:\n",
    "                level2_meta = np.zeros((X.shape[0], len(self.level2_estimators) * n_classes))\n",
    "            else:\n",
    "                level2_meta = np.zeros((X.shape[0], len(self.level2_estimators)))\n",
    "            \n",
    "            col_idx = 0\n",
    "            for i, (name, estimator) in enumerate(self.level2_fitted_.items()):\n",
    "                if self.use_probabilities and hasattr(estimator, 'predict_proba'):\n",
    "                    pred_proba = estimator.predict_proba(combined_level1_meta)\n",
    "                    level2_meta[:, col_idx:col_idx+n_classes] = pred_proba\n",
    "                    col_idx += n_classes\n",
    "                else:\n",
    "                    pred = estimator.predict(combined_level1_meta)\n",
    "                    if self.use_probabilities:\n",
    "                        onehot = np.zeros((len(pred), n_classes))\n",
    "                        for j, p in enumerate(pred):\n",
    "                            class_idx = np.where(self.classes_ == p)[0][0]\n",
    "                            onehot[j, class_idx] = 1.0\n",
    "                        level2_meta[:, col_idx:col_idx+n_classes] = onehot\n",
    "                        col_idx += n_classes\n",
    "                    else:\n",
    "                        level2_meta[:, i] = pred\n",
    "            \n",
    "            return self.final_estimator.predict_proba(level2_meta)\n",
    "        else:\n",
    "            # Convert hard predictions to probabilities\n",
    "            pred = self.predict(X)\n",
    "            proba = np.zeros((X.shape[0], len(self.classes_)))\n",
    "            for i, p in enumerate(pred):\n",
    "                class_idx = np.where(self.classes_ == p)[0][0]\n",
    "                proba[i, class_idx] = 1.0\n",
    "            return proba\n",
    "\n",
    "# Test multi-level stacking\n",
    "print(\"\\nüß™ Testing Multi-Level Stacking...\")\n",
    "\n",
    "# Define estimators for multi-level stacking\n",
    "if 'uncertainty_results' in locals() and len(uncertainty_results) >= 3:\n",
    "    # Use models from uncertainty analysis\n",
    "    available_models = {name: results['model'] for name, results in uncertainty_results.items() \n",
    "                       if 'model' in results}\n",
    "    \n",
    "    model_list = list(available_models.items())\n",
    "    \n",
    "    if len(model_list) >= 3:\n",
    "        # Split models into levels\n",
    "        level1_models = dict(model_list[:2])  # First 2 models for level 1\n",
    "        level2_models = dict(model_list[2:3])  # Next model for level 2\n",
    "        final_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        \n",
    "        # Test different stacking configurations\n",
    "        stacking_configs = {\n",
    "            'Standard Multi-Level': MultiLevelStackingAdvanced(\n",
    "                level1_estimators=level1_models,\n",
    "                level2_estimators=level2_models,\n",
    "                final_estimator=final_model,\n",
    "                cv=3,\n",
    "                use_probabilities=True,\n",
    "                uncertainty_weighting=False\n",
    "            ),\n",
    "            'Uncertainty-Weighted': MultiLevelStackingAdvanced(\n",
    "                level1_estimators=level1_models,\n",
    "                level2_estimators=level2_models,\n",
    "                final_estimator=final_model,\n",
    "                cv=3,\n",
    "                use_probabilities=True,\n",
    "                uncertainty_weighting=True\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        stacking_results = {}\n",
    "        \n",
    "        for config_name, stacking_model in stacking_configs.items():\n",
    "            print(f\"\\n--- Testing {config_name} ---\")\n",
    "            \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Fit and predict\n",
    "                stacking_model.fit(X_train_unc_scaled, y_train_unc)\n",
    "                y_pred = stacking_model.predict(X_test_unc_scaled)\n",
    "                \n",
    "                training_time = time.time() - start_time\n",
    "                test_accuracy = accuracy_score(y_test_unc, y_pred)\n",
    "                \n",
    "                stacking_results[config_name] = {\n",
    "                    'test_accuracy': test_accuracy,\n",
    "                    'training_time': training_time,\n",
    "                    'y_pred': y_pred,\n",
    "                    'model': stacking_model\n",
    "                }\n",
    "                \n",
    "                print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "                print(f\"  Training Time: {training_time:.2f}s\")\n",
    "                \n",
    "                # Save the model\n",
    "                save_advanced_model(stacking_model, f\"multi_level_stacking_{config_name.lower().replace(' ', '_')}\", \n",
    "                                   f\"Multi-level stacking: {config_name}\",\n",
    "                                   'stacking', {'test_accuracy': test_accuracy, 'training_time': training_time})\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "                stacking_results[config_name] = {'error': str(e)}\n",
    "        \n",
    "        # Compare stacking results\n",
    "        if any('test_accuracy' in result for result in stacking_results.values()):\n",
    "            print(f\"\\nüìä Multi-Level Stacking Results:\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            for config_name, result in stacking_results.items():\n",
    "                if 'test_accuracy' in result:\n",
    "                    print(f\"{config_name}: {result['test_accuracy']:.4f}\")\n",
    "            \n",
    "            # Save stacking experiment results\n",
    "            stacking_summary = {}\n",
    "            for config_name, result in stacking_results.items():\n",
    "                if 'test_accuracy' in result:\n",
    "                    stacking_summary[config_name] = {\n",
    "                        'test_accuracy': result['test_accuracy'],\n",
    "                        'training_time': result['training_time']\n",
    "                    }\n",
    "            \n",
    "            save_experiment_results('multi_level_stacking_comparison', stacking_summary,\n",
    "                                   'Comparison of multi-level stacking configurations', 'stacking')\n",
    "\n",
    "print(\"‚ú® Multi-level stacking implementation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca92c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production ensemble system with comprehensive monitoring\n",
    "print(\"\\nüöÄ Production Ensemble System with Comprehensive Monitoring...\")\n",
    "\n",
    "class ProductionEnsembleAdvanced:\n",
    "    \"\"\"Advanced production ensemble with monitoring, A/B testing, and auto-updating.\"\"\"\n",
    "    \n",
    "    def __init__(self, ensemble_config=None):\n",
    "        self.ensemble_config = ensemble_config or {}\n",
    "        self.models = {}\n",
    "        self.weights = {}\n",
    "        self.performance_history = []\n",
    "        self.prediction_logs = []\n",
    "        self.uncertainty_thresholds = {}\n",
    "        self.metadata = {\n",
    "            'created_at': datetime.datetime.now().isoformat(),\n",
    "            'version': '2.0.0',\n",
    "            'n_predictions': 0,\n",
    "            'confidence_threshold': self.ensemble_config.get('confidence_threshold', 0.7),\n",
    "            'auto_update_enabled': self.ensemble_config.get('auto_update', False)\n",
    "        }\n",
    "        self.classes_ = None\n",
    "        self.model_performance_tracker = {}\n",
    "        \n",
    "    def add_model(self, name, model, weight=1.0, uncertainty_threshold=0.1):\n",
    "        \"\"\"Add a model to the ensemble with uncertainty monitoring.\"\"\"\n",
    "        self.models[name] = model\n",
    "        self.weights[name] = weight\n",
    "        self.uncertainty_thresholds[name] = uncertainty_threshold\n",
    "        self.model_performance_tracker[name] = {\n",
    "            'predictions': 0,\n",
    "            'correct_predictions': 0,\n",
    "            'avg_confidence': 0.0,\n",
    "            'last_updated': datetime.datetime.now().isoformat()\n",
    "        }\n",
    "        print(f\"Added model '{name}' with weight {weight} and uncertainty threshold {uncertainty_threshold}\")\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit all models and initialize monitoring.\"\"\"\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        print(\"Training production ensemble models...\")\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"  Training {name}...\")\n",
    "            model.fit(X, y)\n",
    "        \n",
    "        # Initialize performance tracking\n",
    "        self._update_performance_metrics(X, y, is_initial=True)\n",
    "        \n",
    "        self.metadata['last_trained'] = datetime.datetime.now().isoformat()\n",
    "        self.metadata['training_samples'] = len(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, return_metadata=False, log_predictions=True):\n",
    "        \"\"\"Make ensemble predictions with comprehensive logging and monitoring.\"\"\"\n",
    "        if not self.models:\n",
    "            raise ValueError(\"No models in ensemble. Add models first.\")\n",
    "        \n",
    "        predictions = []\n",
    "        confidence_scores = []\n",
    "        model_contributions = []\n",
    "        uncertainty_flags = []\n",
    "        \n",
    "        for i, sample in enumerate(X):\n",
    "            sample = sample.reshape(1, -1)\n",
    "            \n",
    "            # Get predictions and confidences from all models\n",
    "            model_preds = {}\n",
    "            model_confidences = {}\n",
    "            model_uncertainties = {}\n",
    "            \n",
    "            for name, model in self.models.items():\n",
    "                pred = model.predict(sample)[0]\n",
    "                model_preds[name] = pred\n",
    "                \n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    proba = model.predict_proba(sample)[0]\n",
    "                    confidence = np.max(proba)\n",
    "                    uncertainty = -np.sum(proba * np.log(proba + 1e-10))\n",
    "                else:\n",
    "                    confidence = 0.5  # Default confidence for hard predictors\n",
    "                    uncertainty = 1.0  # High uncertainty for hard predictors\n",
    "                \n",
    "                model_confidences[name] = confidence\n",
    "                model_uncertainties[name] = uncertainty\n",
    "            \n",
    "            # Uncertainty-aware weighted voting\n",
    "            final_pred, ensemble_confidence, contributions = self._uncertainty_aware_voting(\n",
    "                model_preds, model_confidences, model_uncertainties\n",
    "            )\n",
    "            \n",
    "            predictions.append(final_pred)\n",
    "            confidence_scores.append(ensemble_confidence)\n",
    "            model_contributions.append(contributions)\n",
    "            \n",
    "            # Check uncertainty flags\n",
    "            high_uncertainty_models = [\n",
    "                name for name, unc in model_uncertainties.items() \n",
    "                if unc > self.uncertainty_thresholds[name]\n",
    "            ]\n",
    "            uncertainty_flags.append(len(high_uncertainty_models) > len(self.models) / 2)\n",
    "            \n",
    "            # Log prediction if enabled\n",
    "            if log_predictions:\n",
    "                self._log_prediction(sample, final_pred, ensemble_confidence, \n",
    "                                   model_preds, model_confidences, model_uncertainties)\n",
    "        \n",
    "        # Update metadata\n",
    "        self.metadata['n_predictions'] += len(X)\n",
    "        self.metadata['last_prediction'] = datetime.datetime.now().isoformat()\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        confidence_scores = np.array(confidence_scores)\n",
    "        \n",
    "        if return_metadata:\n",
    "            metadata = {\n",
    "                'confidence_scores': confidence_scores,\n",
    "                'model_contributions': model_contributions,\n",
    "                'uncertainty_flags': uncertainty_flags,\n",
    "                'avg_confidence': np.mean(confidence_scores),\n",
    "                'high_uncertainty_ratio': np.mean(uncertainty_flags)\n",
    "            }\n",
    "            return predictions, metadata\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def _uncertainty_aware_voting(self, model_preds, model_confidences, model_uncertainties):\n",
    "        \"\"\"Perform uncertainty-aware weighted voting.\"\"\"\n",
    "        class_votes = {cls: 0.0 for cls in self.classes_}\n",
    "        total_weight = 0.0\n",
    "        contributions = {}\n",
    "        \n",
    "        for name, pred in model_preds.items():\n",
    "            base_weight = self.weights[name]\n",
    "            confidence = model_confidences[name]\n",
    "            uncertainty = model_uncertainties[name]\n",
    "            \n",
    "            # Adjust weight based on confidence and uncertainty\n",
    "            uncertainty_penalty = 1.0 / (1.0 + uncertainty)\n",
    "            confidence_boost = confidence\n",
    "            \n",
    "            adjusted_weight = base_weight * confidence_boost * uncertainty_penalty\n",
    "            \n",
    "            class_votes[pred] += adjusted_weight\n",
    "            total_weight += adjusted_weight\n",
    "            \n",
    "            contributions[name] = {\n",
    "                'prediction': pred,\n",
    "                'base_weight': base_weight,\n",
    "                'adjusted_weight': adjusted_weight,\n",
    "                'confidence': confidence,\n",
    "                'uncertainty': uncertainty\n",
    "            }\n",
    "        \n",
    "        # Normalize votes\n",
    "        if total_weight > 0:\n",
    "            for cls in class_votes:\n",
    "                class_votes[cls] /= total_weight\n",
    "        \n",
    "        # Final prediction and confidence\n",
    "        final_pred = max(class_votes, key=class_votes.get)\n",
    "        ensemble_confidence = class_votes[final_pred]\n",
    "        \n",
    "        return final_pred, ensemble_confidence, contributions\n",
    "    \n",
    "    def _log_prediction(self, sample, prediction, confidence, model_preds, model_confidences, model_uncertainties):\n",
    "        \"\"\"Log prediction for monitoring and analysis.\"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': datetime.datetime.now().isoformat(),\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'model_predictions': model_preds,\n",
    "            'model_confidences': model_confidences,\n",
    "            'model_uncertainties': model_uncertainties,\n",
    "            'sample_hash': hash(str(sample))  # For tracking without storing sensitive data\n",
    "        }\n",
    "        \n",
    "        self.prediction_logs.append(log_entry)\n",
    "        \n",
    "        # Keep only recent logs (last 1000 predictions)\n",
    "        if len(self.prediction_logs) > 1000:\n",
    "            self.prediction_logs = self.prediction_logs[-1000:]\n",
    "    \n",
    "    def update_model_performance(self, X_feedback, y_feedback):\n",
    "        \"\"\"Update model performance tracking with feedback data.\"\"\"\n",
    "        print(\"Updating model performance with feedback data...\")\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            pred = model.predict(X_feedback)\n",
    "            accuracy = accuracy_score(y_feedback, pred)\n",
    "            \n",
    "            # Update tracker\n",
    "            tracker = self.model_performance_tracker[name]\n",
    "            tracker['predictions'] += len(X_feedback)\n",
    "            tracker['correct_predictions'] += np.sum(pred == y_feedback)\n",
    "            tracker['last_updated'] = datetime.datetime.now().isoformat()\n",
    "            \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                proba = model.predict_proba(X_feedback)\n",
    "                avg_confidence = np.mean(np.max(proba, axis=1))\n",
    "                tracker['avg_confidence'] = avg_confidence\n",
    "            \n",
    "            print(f\"  {name}: Current accuracy = {accuracy:.4f}\")\n",
    "        \n",
    "        # Trigger auto-update if enabled and performance degrades\n",
    "        if self.metadata.get('auto_update_enabled', False):\n",
    "            self._check_auto_update_trigger(X_feedback, y_feedback)\n",
    "    \n",
    "    def _check_auto_update_trigger(self, X_feedback, y_feedback):\n",
    "        \"\"\"Check if auto-update should be triggered based on performance.\"\"\"\n",
    "        # Calculate ensemble performance on feedback data\n",
    "        pred = self.predict(X_feedback, log_predictions=False)\n",
    "        current_accuracy = accuracy_score(y_feedback, pred)\n",
    "        \n",
    "        # Get historical performance\n",
    "        if self.performance_history:\n",
    "            recent_performance = np.mean([entry['accuracy'] for entry in self.performance_history[-5:]])\n",
    "            \n",
    "            # Trigger update if performance drops significantly\n",
    "            if current_accuracy < recent_performance - 0.05:  # 5% drop threshold\n",
    "                print(f\"‚ö†Ô∏è Performance drop detected: {current_accuracy:.4f} vs {recent_performance:.4f}\")\n",
    "                print(\"Triggering auto-update...\")\n",
    "                self._auto_update_weights(X_feedback, y_feedback)\n",
    "    \n",
    "    def _auto_update_weights(self, X_val, y_val):\n",
    "        \"\"\"Automatically update model weights based on recent performance.\"\"\"\n",
    "        print(\"Performing automatic weight update...\")\n",
    "        \n",
    "        new_weights = {}\n",
    "        for name, model in self.models.items():\n",
    "            pred = model.predict(X_val)\n",
    "            accuracy = accuracy_score(y_val, pred)\n",
    "            new_weights[name] = max(accuracy, 0.01)  # Minimum weight\n",
    "        \n",
    "        # Normalize weights\n",
    "        total_weight = sum(new_weights.values())\n",
    "        for name in new_weights:\n",
    "            new_weights[name] /= total_weight\n",
    "        \n",
    "        # Update weights with momentum (blend old and new)\n",
    "        momentum = 0.7\n",
    "        for name in self.weights:\n",
    "            old_weight = self.weights[name]\n",
    "            new_weight = new_weights[name]\n",
    "            self.weights[name] = momentum * old_weight + (1 - momentum) * new_weight\n",
    "        \n",
    "        self.metadata['last_auto_update'] = datetime.datetime.now().isoformat()\n",
    "        print(\"Automatic weight update completed\")\n",
    "    \n",
    "    def get_monitoring_dashboard(self):\n",
    "        \"\"\"Generate comprehensive monitoring dashboard data.\"\"\"\n",
    "        dashboard = {\n",
    "            'metadata': self.metadata,\n",
    "            'model_status': {},\n",
    "            'recent_performance': self.performance_history[-10:] if self.performance_history else [],\n",
    "            'prediction_stats': {},\n",
    "            'alerts': []\n",
    "        }\n",
    "        \n",
    "        # Model status\n",
    "        for name, tracker in self.model_performance_tracker.items():\n",
    "            total_preds = tracker['predictions']\n",
    "            correct_preds = tracker['correct_predictions']\n",
    "            accuracy = correct_preds / total_preds if total_preds > 0 else 0\n",
    "            \n",
    "            dashboard['model_status'][name] = {\n",
    "                'current_weight': self.weights.get(name, 0),\n",
    "                'accuracy': accuracy,\n",
    "                'total_predictions': total_preds,\n",
    "                'avg_confidence': tracker.get('avg_confidence', 0),\n",
    "                'last_updated': tracker['last_updated']\n",
    "            }\n",
    "        \n",
    "        # Prediction statistics from recent logs\n",
    "        if self.prediction_logs:\n",
    "            recent_logs = self.prediction_logs[-100:]  # Last 100 predictions\n",
    "            confidences = [log['confidence'] for log in recent_logs]\n",
    "            \n",
    "            dashboard['prediction_stats'] = {\n",
    "                'recent_predictions': len(recent_logs),\n",
    "                'avg_confidence': np.mean(confidences),\n",
    "                'low_confidence_ratio': np.mean([c < self.metadata['confidence_threshold'] for c in confidences]),\n",
    "                'prediction_distribution': dict(pd.Series([log['prediction'] for log in recent_logs]).value_counts())\n",
    "            }\n",
    "        \n",
    "        # Generate alerts\n",
    "        dashboard['alerts'] = self._generate_alerts()\n",
    "        \n",
    "        return dashboard\n",
    "    \n",
    "    def _generate_alerts(self):\n",
    "        \"\"\"Generate system alerts based on monitoring data.\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        # Check for low-performing models\n",
    "        for name, tracker in self.model_performance_tracker.items():\n",
    "            if tracker['predictions'] > 50:  # Only check models with sufficient predictions\n",
    "                accuracy = tracker['correct_predictions'] / tracker['predictions']\n",
    "                if accuracy < 0.6:  # Below 60% accuracy\n",
    "                    alerts.append({\n",
    "                        'type': 'performance',\n",
    "                        'severity': 'warning',\n",
    "                        'message': f\"Model {name} accuracy below threshold: {accuracy:.3f}\",\n",
    "                        'timestamp': datetime.datetime.now().isoformat()\n",
    "                    })\n",
    "        \n",
    "        # Check for high uncertainty predictions\n",
    "        if self.prediction_logs:\n",
    "            recent_logs = self.prediction_logs[-50:]\n",
    "            high_uncertainty_count = sum(\n",
    "                1 for log in recent_logs \n",
    "                if np.mean(list(log['model_uncertainties'].values())) > 1.0\n",
    "            )\n",
    "            \n",
    "            if high_uncertainty_count / len(recent_logs) > 0.3:  # 30% high uncertainty\n",
    "                alerts.append({\n",
    "                    'type': 'uncertainty',\n",
    "                    'severity': 'info',\n",
    "                    'message': f\"High uncertainty in {high_uncertainty_count}/{len(recent_logs)} recent predictions\",\n",
    "                    'timestamp': datetime.datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def _update_performance_metrics(self, X, y, is_initial=False):\n",
    "        \"\"\"Update performance metrics for monitoring.\"\"\"\n",
    "        predictions = self.predict(X, log_predictions=False)\n",
    "        accuracy = accuracy_score(y, predictions)\n",
    "        \n",
    "        performance_entry = {\n",
    "            'timestamp': datetime.datetime.now().isoformat(),\n",
    "            'accuracy': accuracy,\n",
    "            'n_samples': len(X),\n",
    "            'is_initial': is_initial\n",
    "        }\n",
    "        \n",
    "        self.performance_history.append(performance_entry)\n",
    "        \n",
    "        # Keep only last 100 entries\n",
    "        if len(self.performance_history) > 100:\n",
    "            self.performance_history = self.performance_history[-100:]\n",
    "\n",
    "# Test production ensemble system\n",
    "print(\"\\nüè≠ Testing Advanced Production Ensemble System...\")\n",
    "\n",
    "if 'uncertainty_results' in locals() and len(uncertainty_results) >= 2:\n",
    "    # Initialize production ensemble\n",
    "    prod_ensemble_advanced = ProductionEnsembleAdvanced({\n",
    "        'confidence_threshold': 0.8,\n",
    "        'auto_update': True,\n",
    "        'monitoring_enabled': True\n",
    "    })\n",
    "    \n",
    "    # Add models with different uncertainty thresholds\n",
    "    model_configs = [\n",
    "        ('RandomForest', 1.2, 0.1),\n",
    "        ('LogisticRegression', 1.0, 0.15),\n",
    "        ('SVM', 0.8, 0.2)\n",
    "    ]\n",
    "    \n",
    "    added_models = 0\n",
    "    for model_name, weight, uncertainty_threshold in model_configs:\n",
    "        # Find matching model from uncertainty results\n",
    "        matching_model = None\n",
    "        for name, results in uncertainty_results.items():\n",
    "            if model_name.lower() in name.lower() and 'model' in results:\n",
    "                matching_model = results['model']\n",
    "                break\n",
    "        \n",
    "        if matching_model:\n",
    "            prod_ensemble_advanced.add_model(\n",
    "                model_name, matching_model, weight, uncertainty_threshold\n",
    "            )\n",
    "            added_models += 1\n",
    "    \n",
    "    if added_models >= 2:\n",
    "        # Set classes manually since models are already trained\n",
    "        prod_ensemble_advanced.classes_ = np.unique(y_train_unc)\n",
    "        \n",
    "        # Test predictions with metadata\n",
    "        print(\"\\nüß™ Testing predictions with monitoring...\")\n",
    "        sample_X = X_test_unc_scaled[:50]\n",
    "        sample_y = y_test_unc[:50]\n",
    "        \n",
    "        predictions, metadata = prod_ensemble_advanced.predict(\n",
    "            sample_X, return_metadata=True, log_predictions=True\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_accuracy = accuracy_score(sample_y, predictions)\n",
    "        avg_confidence = metadata['avg_confidence']\n",
    "        high_uncertainty_ratio = metadata['high_uncertainty_ratio']\n",
    "        \n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Average Confidence: {avg_confidence:.4f}\")\n",
    "        print(f\"High Uncertainty Ratio: {high_uncertainty_ratio:.4f}\")\n",
    "        \n",
    "        # Update performance with feedback\n",
    "        feedback_X = X_test_unc_scaled[50:100]\n",
    "        feedback_y = y_test_unc[50:100]\n",
    "        prod_ensemble_advanced.update_model_performance(feedback_X, feedback_y)\n",
    "        \n",
    "        # Get monitoring dashboard\n",
    "        dashboard = prod_ensemble_advanced.get_monitoring_dashboard()\n",
    "        \n",
    "        print(f\"\\nüìä Production Ensemble Dashboard:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Display model status\n",
    "        print(\"Model Status:\")\n",
    "        for name, status in dashboard['model_status'].items():\n",
    "            print(f\"  {name}:\")\n",
    "            print(f\"    Weight: {status['current_weight']:.3f}\")\n",
    "            print(f\"    Accuracy: {status['accuracy']:.3f}\")\n",
    "            print(f\"    Avg Confidence: {status['avg_confidence']:.3f}\")\n",
    "        \n",
    "        # Display prediction stats\n",
    "        if dashboard['prediction_stats']:\n",
    "            stats = dashboard['prediction_stats']\n",
    "            print(f\"\\nPrediction Statistics:\")\n",
    "            print(f\"  Recent Predictions: {stats['recent_predictions']}\")\n",
    "            print(f\"  Average Confidence: {stats['avg_confidence']:.3f}\")\n",
    "            print(f\"  Low Confidence Ratio: {stats['low_confidence_ratio']:.3f}\")\n",
    "        \n",
    "        # Display alerts\n",
    "        if dashboard['alerts']:\n",
    "            print(f\"\\nAlerts ({len(dashboard['alerts'])}):\")\n",
    "            for alert in dashboard['alerts']:\n",
    "                print(f\"  [{alert['severity'].upper()}] {alert['message']}\")\n",
    "        else:\n",
    "            print(\"\\nNo alerts - system operating normally ‚úÖ\")\n",
    "        \n",
    "        # Save production ensemble\n",
    "        prod_ensemble_metrics = {\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'high_uncertainty_ratio': high_uncertainty_ratio,\n",
    "            'n_models': len(prod_ensemble_advanced.models),\n",
    "            'monitoring_enabled': True\n",
    "        }\n",
    "        \n",
    "        save_advanced_model(prod_ensemble_advanced, 'production_ensemble_advanced',\n",
    "                           'Advanced production ensemble with comprehensive monitoring',\n",
    "                           'production', prod_ensemble_metrics)\n",
    "        \n",
    "        # Visualize production metrics\n",
    "        if len(dashboard['recent_performance']) > 1:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            \n",
    "            # 1. Model weights\n",
    "            model_names = list(prod_ensemble_advanced.weights.keys())\n",
    "            weights = list(prod_ensemble_advanced.weights.values())\n",
    "            \n",
    "            axes[0, 0].bar(model_names, weights, color='lightblue', alpha=0.7)\n",
    "            axes[0, 0].set_title('Current Model Weights')\n",
    "            axes[0, 0].set_ylabel('Weight')\n",
    "            axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # 2. Performance over time\n",
    "            perf_history = dashboard['recent_performance']\n",
    "            timestamps = range(len(perf_history))\n",
    "            accuracies = [entry['accuracy'] for entry in perf_history]\n",
    "            \n",
    "            axes[0, 1].plot(timestamps, accuracies, 'o-', linewidth=2)\n",
    "            axes[0, 1].set_title('Performance Over Time')\n",
    "            axes[0, 1].set_xlabel('Time Period')\n",
    "            axes[0, 1].set_ylabel('Accuracy')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # 3. Confidence distribution\n",
    "            if dashboard['prediction_stats']:\n",
    "                confidences = [log['confidence'] for log in prod_ensemble_advanced.prediction_logs[-100:]]\n",
    "                axes[1, 0].hist(confidences, bins=20, alpha=0.7, color='lightgreen')\n",
    "                axes[1, 0].set_title('Confidence Distribution')\n",
    "                axes[1, 0].set_xlabel('Confidence Score')\n",
    "                axes[1, 0].set_ylabel('Frequency')\n",
    "                axes[1, 0].axvline(avg_confidence, color='red', linestyle='--', \n",
    "                                  label=f'Mean: {avg_confidence:.3f}')\n",
    "                axes[1, 0].legend()\n",
    "            \n",
    "            # 4. Model performance comparison\n",
    "            model_accuracies = [status['accuracy'] for status in dashboard['model_status'].values()]\n",
    "            model_names_short = [name[:10] for name in dashboard['model_status'].keys()]\n",
    "            \n",
    "            bars = axes[1, 1].bar(model_names_short, model_accuracies, color='lightcoral', alpha=0.7)\n",
    "            axes[1, 1].set_title('Individual Model Performance')\n",
    "            axes[1, 1].set_ylabel('Accuracy')\n",
    "            axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            for bar, acc in zip(bars, model_accuracies):\n",
    "                axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                               f'{acc:.3f}', ha='center', va='bottom')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            save_figure(fig, 'production_ensemble_monitoring',\n",
    "                       'Production ensemble monitoring dashboard', 'production')\n",
    "            plt.show()\n",
    "\n",
    "print(\"‚ú® Advanced production ensemble system complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd7814",
   "metadata": {},
   "source": [
    "## 8. Automated Feature Engineering {#feature-engineering}\n",
    "\n",
    "Implementing automated feature engineering and selection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055eba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated feature engineering\n",
    "print(\"‚öôÔ∏è Automated Feature Engineering...\")\n",
    "\n",
    "class AutomatedFeatureEngineer:\n",
    "    \"\"\"Comprehensive automated feature engineering toolkit.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_features=None):\n",
    "        self.max_features = max_features\n",
    "        self.generated_features = []\n",
    "        self.feature_names = []\n",
    "        self.selection_results = {}\n",
    "    \n",
    "    def polynomial_features(self, X, degree=2, include_bias=False):\n",
    "        \"\"\"Generate polynomial features.\"\"\"\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        \n",
    "        poly = PolynomialFeatures(degree=degree, include_bias=include_bias)\n",
    "        X_poly = poly.fit_transform(X)\n",
    "        \n",
    "        # Generate feature names\n",
    "        if hasattr(X, 'columns'):\n",
    "            input_features = X.columns.tolist()\n",
    "        else:\n",
    "            input_features = [f'x{i}' for i in range(X.shape[1])]\n",
    "        \n",
    "        poly_feature_names = poly.get_feature_names_out(input_features)\n",
    "        \n",
    "        return X_poly, poly_feature_names.tolist()\n",
    "    \n",
    "    def interaction_features(self, X, feature_names=None):\n",
    "        \"\"\"Generate interaction features between all pairs.\"\"\"\n",
    "        if feature_names is None:\n",
    "            feature_names = [f'x{i}' for i in range(X.shape[1])]\n",
    "        \n",
    "        interaction_features = []\n",
    "        interaction_names = []\n",
    "        \n",
    "        # Generate all pairwise interactions\n",
    "        for i in range(X.shape[1]):\n",
    "            for j in range(i + 1, X.shape[1]):\n",
    "                interaction = X[:, i] * X[:, j]\n",
    "                interaction_features.append(interaction)\n",
    "                interaction_names.append(f\"{feature_names[i]} * {feature_names[j]}\")\n",
    "        \n",
    "        if interaction_features:\n",
    "            X_interactions = np.column_stack(interaction_features)\n",
    "            return X_interactions, interaction_names\n",
    "        else:\n",
    "            return np.empty((X.shape[0], 0)), []\n",
    "    \n",
    "    def statistical_features(self, X, window_size=5):\n",
    "        \"\"\"Generate statistical features (rolling statistics).\"\"\"\n",
    "        if X.shape[1] < window_size:\n",
    "            return np.empty((X.shape[0], 0)), []\n",
    "        \n",
    "        stat_features = []\n",
    "        stat_names = []\n",
    "        \n",
    "        # Rolling mean and std for each feature\n",
    "        for i in range(X.shape[1] - window_size + 1):\n",
    "            window_data = X[:, i:i + window_size]\n",
    "            \n",
    "            # Rolling statistics\n",
    "            rolling_mean = np.mean(window_data, axis=1)\n",
    "            rolling_std = np.std(window_data, axis=1)\n",
    "            rolling_max = np.max(window_data, axis=1)\n",
    "            rolling_min = np.min(window_data, axis=1)\n",
    "            \n",
    "            stat_features.extend([rolling_mean, rolling_std, rolling_max, rolling_min])\n",
    "            stat_names.extend([\n",
    "                f'rolling_mean_{i}_{i+window_size}',\n",
    "                f'rolling_std_{i}_{i+window_size}',\n",
    "                f'rolling_max_{i}_{i+window_size}',\n",
    "                f'rolling_min_{i}_{i+window_size}'\n",
    "            ])\n",
    "        \n",
    "        if stat_features:\n",
    "            X_stats = np.column_stack(stat_features)\n",
    "            return X_stats, stat_names\n",
    "        else:\n",
    "            return np.empty((X.shape[0], 0)), []\n",
    "    \n",
    "    def clustering_features(self, X, n_clusters=5):\n",
    "        \"\"\"Generate clustering-based features.\"\"\"\n",
    "        # K-means clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Distance to cluster centers\n",
    "        cluster_distances = kmeans.transform(X)\n",
    "        \n",
    "        # One-hot encode cluster labels\n",
    "        cluster_onehot = np.zeros((len(X), n_clusters))\n",
    "        for i, label in enumerate(cluster_labels):\n",
    "            cluster_onehot[i, label] = 1\n",
    "        \n",
    "        # Combine features\n",
    "        clustering_features = np.column_stack([\n",
    "            cluster_labels.reshape(-1, 1),\n",
    "            cluster_distances,\n",
    "            cluster_onehot\n",
    "        ])\n",
    "        \n",
    "        clustering_names = ['cluster_label'] + \\\n",
    "                          [f'distance_to_cluster_{i}' for i in range(n_clusters)] + \\\n",
    "                          [f'is_cluster_{i}' for i in range(n_clusters)]\n",
    "        \n",
    "        return clustering_features, clustering_names\n",
    "    \n",
    "    def dimensionality_reduction_features(self, X, n_components=5):\n",
    "        \"\"\"Generate dimensionality reduction features.\"\"\"\n",
    "        # PCA features\n",
    "        pca = PCA(n_components=min(n_components, X.shape[1]), random_state=42)\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        \n",
    "        pca_names = [f'pca_component_{i}' for i in range(X_pca.shape[1])]\n",
    "        \n",
    "        return X_pca, pca_names\n",
    "    \n",
    "    def automated_feature_generation(self, X, y, feature_names=None):\n",
    "        \"\"\"Automatically generate multiple types of features.\"\"\"\n",
    "        print(\"  Generating automated features...\")\n",
    "        \n",
    "        if feature_names is None:\n",
    "            feature_names = [f'original_{i}' for i in range(X.shape[1])]\n",
    "        \n",
    "        all_features = [X]\n",
    "        all_feature_names = feature_names.copy()\n",
    "        \n",
    "        # 1. Polynomial features\n",
    "        try:\n",
    "            X_poly, poly_names = self.polynomial_features(X, degree=2)\n",
    "            if X_poly.shape[1] > X.shape[1]:  # Only if new features were added\n",
    "                # Take only the new features (exclude original features)\n",
    "                X_poly_new = X_poly[:, X.shape[1]:]\n",
    "                poly_names_new = poly_names[X.shape[1]:]\n",
    "                all_features.append(X_poly_new)\n",
    "                all_feature_names.extend(poly_names_new)\n",
    "        except Exception as e:\n",
    "            print(f\"    Polynomial features failed: {e}\")\n",
    "        \n",
    "        # 2. Interaction features\n",
    "        try:\n",
    "            X_interactions, interaction_names = self.interaction_features(X, feature_names)\n",
    "            if X_interactions.shape[1] > 0:\n",
    "                all_features.append(X_interactions)\n",
    "                all_feature_names.extend(interaction_names)\n",
    "        except Exception as e:\n",
    "            print(f\"    Interaction features failed: {e}\")\n",
    "        \n",
    "        # 3. Statistical features\n",
    "        try:\n",
    "            X_stats, stat_names = self.statistical_features(X)\n",
    "            if X_stats.shape[1] > 0:\n",
    "                all_features.append(X_stats)\n",
    "                all_feature_names.extend(stat_names)\n",
    "        except Exception as e:\n",
    "            print(f\"    Statistical features failed: {e}\")\n",
    "        \n",
    "        # 4. Clustering features\n",
    "        try:\n",
    "            X_clustering, clustering_names = self.clustering_features(X, n_clusters=3)\n",
    "            all_features.append(X_clustering)\n",
    "            all_feature_names.extend(clustering_names)\n",
    "        except Exception as e:\n",
    "            print(f\"    Clustering features failed: {e}\")\n",
    "        \n",
    "        # 5. PCA features\n",
    "        try:\n",
    "            X_pca, pca_names = self.dimensionality_reduction_features(X, n_components=3)\n",
    "            all_features.append(X_pca)\n",
    "            all_feature_names.extend(pca_names)\n",
    "        except Exception as e:\n",
    "            print(f\"    PCA features failed: {e}\")\n",
    "        \n",
    "        # Combine all features\n",
    "        X_engineered = np.column_stack(all_features)\n",
    "        \n",
    "        print(f\"  Generated {X_engineered.shape[1] - X.shape[1]} new features\")\n",
    "        print(f\"  Total features: {X_engineered.shape[1]}\")\n",
    "        \n",
    "        self.generated_features = X_engineered\n",
    "        self.feature_names = all_feature_names\n",
    "        \n",
    "        return X_engineered, all_feature_names\n",
    "    \n",
    "    def automated_feature_selection(self, X, y, max_features=20):\n",
    "        \"\"\"Automated feature selection using multiple methods.\"\"\"\n",
    "        print(\"  Performing automated feature selection...\")\n",
    "        \n",
    "        selection_results = {}\n",
    "        \n",
    "        # 1. Univariate feature selection\n",
    "        try:\n",
    "            k_best = min(max_features, X.shape[1])\n",
    "            selector_univariate = SelectKBest(score_func=f_classif, k=k_best)\n",
    "            X_univariate = selector_univariate.fit_transform(X, y)\n",
    "            \n",
    "            # Get selected feature indices\n",
    "            selected_features_univariate = selector_univariate.get_support(indices=True)\n",
    "            scores_univariate = selector_univariate.scores_\n",
    "            \n",
    "            selection_results['univariate'] = {\n",
    "                'selected_features': selected_features_univariate,\n",
    "                'scores': scores_univariate,\n",
    "                'X_selected': X_univariate\n",
    "            }\n",
    "            \n",
    "            print(f\"    Univariate selection: {len(selected_features_univariate)} features\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Univariate selection failed: {e}\")\n",
    "        \n",
    "        # 2. Recursive Feature Elimination\n",
    "        try:\n",
    "            base_estimator = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            rfe_features = min(max_features, X.shape[1])\n",
    "            rfe = RFE(estimator=base_estimator, n_features_to_select=rfe_features)\n",
    "            X_rfe = rfe.fit_transform(X, y)\n",
    "            \n",
    "            selected_features_rfe = rfe.get_support(indices=True)\n",
    "            feature_rankings = rfe.ranking_\n",
    "            \n",
    "            selection_results['rfe'] = {\n",
    "                'selected_features': selected_features_rfe,\n",
    "                'rankings': feature_rankings,\n",
    "                'X_selected': X_rfe\n",
    "            }\n",
    "            \n",
    "            print(f\"    RFE selection: {len(selected_features_rfe)} features\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    RFE selection failed: {e}\")\n",
    "        \n",
    "        # 3. Feature importance from Random Forest\n",
    "        try:\n",
    "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            rf.fit(X, y)\n",
    "            \n",
    "            feature_importances = rf.feature_importances_\n",
    "            # Select top features\n",
    "            top_features_rf = np.argsort(feature_importances)[-max_features:]\n",
    "            \n",
    "            selection_results['random_forest'] = {\n",
    "                'selected_features': top_features_rf,\n",
    "                'importances': feature_importances,\n",
    "                'X_selected': X[:, top_features_rf]\n",
    "            }\n",
    "            \n",
    "            print(f\"    Random Forest selection: {len(top_features_rf)} features\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Random Forest selection failed: {e}\")\n",
    "        \n",
    "        self.selection_results = selection_results\n",
    "        return selection_results\n",
    "    \n",
    "    def evaluate_feature_sets(self, X_original, y, feature_sets, cv=5):\n",
    "        \"\"\"Evaluate different feature sets using cross-validation.\"\"\"\n",
    "        print(\"  Evaluating feature sets...\")\n",
    "        \n",
    "        evaluation_results = {}\n",
    "        base_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "        \n",
    "        # Evaluate original features\n",
    "        scores_original = cross_val_score(base_model, X_original, y, cv=cv, scoring='accuracy')\n",
    "        evaluation_results['original'] = {\n",
    "            'mean_score': scores_original.mean(),\n",
    "            'std_score': scores_original.std(),\n",
    "            'n_features': X_original.shape[1]\n",
    "        }\n",
    "        \n",
    "        # Evaluate each feature set\n",
    "        for set_name, feature_info in feature_sets.items():\n",
    "            try:\n",
    "                X_selected = feature_info['X_selected']\n",
    "                scores = cross_val_score(base_model, X_selected, y, cv=cv, scoring='accuracy')\n",
    "                \n",
    "                evaluation_results[set_name] = {\n",
    "                    'mean_score': scores.mean(),\n",
    "                    'std_score': scores.std(),\n",
    "                    'n_features': X_selected.shape[1],\n",
    "                    'selected_features': feature_info['selected_features']\n",
    "                }\n",
    "                \n",
    "                print(f\"    {set_name}: {scores.mean():.4f} ¬± {scores.std():.4f} ({X_selected.shape[1]} features)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    {set_name} evaluation failed: {e}\")\n",
    "        \n",
    "        return evaluation_results\n",
    "\n",
    "# Test automated feature engineering\n",
    "print(\"\\n--- Testing Automated Feature Engineering ---\")\n",
    "\n",
    "# Use a subset of the data for efficiency\n",
    "X_fe = X_train_interp_scaled[:400]\n",
    "y_fe = y_train_interp[:400]\n",
    "\n",
    "print(f\"Feature Engineering Dataset: {X_fe.shape}\")\n",
    "\n",
    "# Initialize feature engineer\n",
    "feature_engineer = AutomatedFeatureEngineer(max_features=50)\n",
    "\n",
    "# Generate features\n",
    "X_engineered, engineered_feature_names = feature_engineer.automated_feature_generation(\n",
    "    X_fe, y_fe, feature_names[:X_fe.shape[1]]\n",
    ")\n",
    "\n",
    "# Perform feature selection\n",
    "selection_results = feature_engineer.automated_feature_selection(X_engineered, y_fe, max_features=25)\n",
    "\n",
    "# Evaluate feature sets\n",
    "evaluation_results = feature_engineer.evaluate_feature_sets(X_fe, y_fe, selection_results)\n",
    "\n",
    "# Find best feature set\n",
    "best_method = max(evaluation_results.items(), key=lambda x: x[1]['mean_score'])\n",
    "print(f\"\\nBest feature selection method: {best_method[0]}\")\n",
    "print(f\"Best score: {best_method[1]['mean_score']:.4f} ¬± {best_method[1]['std_score']:.4f}\")\n",
    "print(f\"Number of features: {best_method[1]['n_features']}\")\n",
    "\n",
    "# Save feature engineering results\n",
    "fe_summary = {}\n",
    "for method_name, results in evaluation_results.items():\n",
    "    fe_summary[method_name] = {\n",
    "        'mean_score': results['mean_score'],\n",
    "        'std_score': results['std_score'],\n",
    "        'n_features': results['n_features']\n",
    "    }\n",
    "\n",
    "save_experiment_results('automated_feature_engineering', fe_summary,\n",
    "                       'Results from automated feature engineering and selection', 'feature_engineering')\n",
    "\n",
    "print(\"\\n‚ú® Automated feature engineering complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650378f6",
   "metadata": {},
   "source": [
    "### Feature Engineering Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c23f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature engineering results\n",
    "print(\"üìä Visualizing Feature Engineering Results...\")\n",
    "\n",
    "if evaluation_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Performance comparison\n",
    "    methods = list(evaluation_results.keys())\n",
    "    scores = [evaluation_results[method]['mean_score'] for method in methods]\n",
    "    stds = [evaluation_results[method]['std_score'] for method in methods]\n",
    "    n_features = [evaluation_results[method]['n_features'] for method in methods]\n",
    "    \n",
    "    bars = axes[0, 0].bar(range(len(methods)), scores, yerr=stds, capsize=5, \n",
    "                         color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'][:len(methods)], \n",
    "                         alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Feature Selection Method')\n",
    "    axes[0, 0].set_ylabel('Cross-Validation Accuracy')\n",
    "    axes[0, 0].set_title('Feature Selection Performance Comparison')\n",
    "    axes[0, 0].set_xticks(range(len(methods)))\n",
    "    axes[0, 0].set_xticklabels(methods, rotation=45, ha='right')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score, std in zip(bars, scores, stds):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.005, \n",
    "                       f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Number of features vs performance\n",
    "    axes[0, 1].scatter(n_features, scores, s=100, alpha=0.7, \n",
    "                      c=range(len(methods)), cmap='viridis')\n",
    "    \n",
    "    # Add method labels\n",
    "    for i, (n_feat, score, method) in enumerate(zip(n_features, scores, methods)):\n",
    "        axes[0, 1].annotate(method, (n_feat, score), xytext=(5, 5), \n",
    "                           textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Number of Features')\n",
    "    axes[0, 1].set_ylabel('Cross-Validation Accuracy')\n",
    "    axes[0, 1].set_title('Performance vs Number of Features')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Feature importance analysis (if available)\n",
    "    if 'random_forest' in selection_results:\n",
    "        rf_results = selection_results['random_forest']\n",
    "        feature_importances = rf_results['importances']\n",
    "        \n",
    "        # Show top 15 features\n",
    "        top_indices = np.argsort(feature_importances)[-15:]\n",
    "        top_importances = feature_importances[top_indices]\n",
    "        top_feature_names = [engineered_feature_names[i] for i in top_indices]\n",
    "        \n",
    "        # Truncate long feature names\n",
    "        top_feature_names_short = [name[:20] + '...' if len(name) > 20 else name \n",
    "                                  for name in top_feature_names]\n",
    "        \n",
    "        bars = axes[1, 0].barh(range(len(top_feature_names_short)), top_importances, \n",
    "                              alpha=0.7, color='lightgreen')\n",
    "        axes[1, 0].set_yticks(range(len(top_feature_names_short)))\n",
    "        axes[1, 0].set_yticklabels(top_feature_names_short)\n",
    "        axes[1, 0].set_xlabel('Feature Importance')\n",
    "        axes[1, 0].set_title('Top 15 Most Important Features')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Feature selection overlap analysis\n",
    "    if len(selection_results) >= 2:\n",
    "        # Compare feature selection methods\n",
    "        method_names = list(selection_results.keys())\n",
    "        overlap_matrix = np.zeros((len(method_names), len(method_names)))\n",
    "        \n",
    "        for i, method1 in enumerate(method_names):\n",
    "            for j, method2 in enumerate(method_names):\n",
    "                if i == j:\n",
    "                    overlap_matrix[i, j] = 1.0\n",
    "                else:\n",
    "                    features1 = set(selection_results[method1]['selected_features'])\n",
    "                    features2 = set(selection_results[method2]['selected_features'])\n",
    "                    \n",
    "                    if len(features1) > 0 and len(features2) > 0:\n",
    "                        overlap = len(features1.intersection(features2)) / len(features1.union(features2))\n",
    "                        overlap_matrix[i, j] = overlap\n",
    "        \n",
    "        im = axes[1, 1].imshow(overlap_matrix, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "        axes[1, 1].set_xticks(range(len(method_names)))\n",
    "        axes[1, 1].set_yticks(range(len(method_names)))\n",
    "        axes[1, 1].set_xticklabels(method_names, rotation=45, ha='right')\n",
    "        axes[1, 1].set_yticklabels(method_names)\n",
    "        axes[1, 1].set_title('Feature Selection Method Overlap')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(len(method_names)):\n",
    "            for j in range(len(method_names)):\n",
    "                text = axes[1, 1].text(j, i, f'{overlap_matrix[i, j]:.2f}',\n",
    "                                     ha=\"center\", va=\"center\", color=\"black\" if overlap_matrix[i, j] < 0.5 else \"white\")\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=axes[1, 1])\n",
    "        cbar.set_label('Jaccard Similarity')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save feature engineering visualization\n",
    "    save_figure(fig, 'automated_feature_engineering_analysis',\n",
    "               'Comprehensive analysis of automated feature engineering results', 'feature_engineering')\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature engineering summary\n",
    "    print(f\"\\nüìä Feature Engineering Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Method':<20} {'Score':<12} {'Std':<8} {'Features':<10} {'Improvement':<12}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    baseline_score = evaluation_results['original']['mean_score']\n",
    "    \n",
    "    for method, results in evaluation_results.items():\n",
    "        score = results['mean_score']\n",
    "        std = results['std_score']\n",
    "        n_feat = results['n_features']\n",
    "        improvement = ((score - baseline_score) / baseline_score) * 100 if baseline_score > 0 else 0\n",
    "        \n",
    "        print(f\"{method:<20} {score:<12.4f} {std:<8.4f} {n_feat:<10} {improvement:<12.2f}%\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Best features analysis\n",
    "    if best_method[0] in selection_results and 'selected_features' in best_method[1]:\n",
    "        print(f\"\\nTop features selected by best method ({best_method[0]}):\")\n",
    "        selected_indices = best_method[1]['selected_features']\n",
    "        for i, idx in enumerate(selected_indices[:10]):  # Show top 10\n",
    "            feature_name = engineered_feature_names[idx] if idx < len(engineered_feature_names) else f\"Feature_{idx}\"\n",
    "            print(f\"  {i+1:2d}. {feature_name}\")\n",
    "\n",
    "print(\"\\n‚ú® Feature engineering visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3708435b",
   "metadata": {},
   "source": [
    "## Comprehensive Summary and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "print(\"üìã Generating Comprehensive Advanced Techniques Report...\")\n",
    "\n",
    "def generate_comprehensive_report():\n",
    "    \"\"\"Generate a comprehensive report of all advanced techniques.\"\"\"\n",
    "    \n",
    "    report = \"\\n\" + \"=\"*100 + \"\\n\"\n",
    "    report += \"COMPREHENSIVE ADVANCED MACHINE LEARNING TECHNIQUES REPORT\\n\"\n",
    "    report += \"=\"*100 + \"\\n\\n\"\n",
    "    \n",
    "    report += \"This report summarizes the results from all advanced machine learning techniques\\n\"\n",
    "    report += \"implemented and tested in this comprehensive analysis.\\n\\n\"\n",
    "    \n",
    "    # Neural Architecture Search\n",
    "    if 'nas' in locals() and hasattr(nas, 'best_architecture'):\n",
    "        report += \"1. NEURAL ARCHITECTURE SEARCH\\n\"\n",
    "        report += \"-\" * 40 + \"\\n\"\n",
    "        report += f\"Best Architecture: {nas.best_architecture['architecture']['hidden_layer_sizes']}\\n\"\n",
    "        report += f\"Validation Score: {nas.best_architecture['val_score']:.4f}\\n\"\n",
    "        report += f\"Parameters: {nas.best_architecture['n_params']}\\n\"\n",
    "        report += f\"Training Time: {nas.best_architecture['training_time']:.2f}s\\n\"\n",
    "        report += f\"Trials Completed: {len(nas.results)}\\n\\n\"\n",
    "    \n",
    "    # Meta-Learning\n",
    "    if 'meta_score' in locals():\n",
    "        report += \"2. META-LEARNING\\n\"\n",
    "        report += \"-\" * 20 + \"\\n\"\n",
    "        report += f\"MAML Meta-Training Score: {meta_score:.4f}\\n\"\n",
    "        report += f\"MAML Test Score: {test_score:.4f}\\n\"\n",
    "        if 'avg_few_shot_accuracy' in locals():\n",
    "            report += f\"Prototypical Networks Score: {avg_few_shot_accuracy:.4f}\\n\"\n",
    "        report += f\"Best Few-Shot Method: {'MAML' if test_score > avg_few_shot_accuracy else 'Prototypical Networks'}\\n\\n\"\n",
    "    \n",
    "    # Active Learning\n",
    "    if 'al_results' in locals():\n",
    "        report += \"3. ACTIVE LEARNING\\n\"\n",
    "        report += \"-\" * 20 + \"\\n\"\n",
    "        best_al_strategy = None\n",
    "        best_al_score = 0\n",
    "        \n",
    "        for strategy_name, results in al_results.items():\n",
    "            if results['performance_history'] and 'Random' not in strategy_name:\n",
    "                final_score = results['performance_history'][-1]['test_score']\n",
    "                if final_score > best_al_score:\n",
    "                    best_al_score = final_score\n",
    "                    best_al_strategy = strategy_name\n",
    "        \n",
    "        if best_al_strategy:\n",
    "            report += f\"Best Strategy: {best_al_strategy}\\n\"\n",
    "            report += f\"Best Final Score: {best_al_score:.4f}\\n\"\n",
    "            \n",
    "            # Compare to random baseline\n",
    "            if 'Random Baseline' in al_results:\n",
    "                random_score = al_results['Random Baseline']['performance_history'][-1]['test_score']\n",
    "                improvement = ((best_al_score - random_score) / random_score) * 100\n",
    "                report += f\"Improvement over Random: {improvement:.2f}%\\n\"\n",
    "        report += \"\\n\"\n",
    "    \n",
    "    # Interpretability\n",
    "    if 'interpretation_results' in locals():\n",
    "        report += \"4. MODEL INTERPRETABILITY\\n\"\n",
    "        report += \"-\" * 30 + \"\\n\"\n",
    "        \n",
    "        for model_name, results in interpretation_results.items():\n",
    "            report += f\"{model_name}:\\n\"\n",
    "            report += f\"  Test Accuracy: {results['test_score']:.4f}\\n\"\n",
    "            \n",
    "            if 'permutation_importance' in results:\n",
    "                top_features = results['permutation_importance']['feature_names'][:3]\n",
    "                report += f\"  Top Features: {', '.join(top_features)}\\n\"\n",
    "            \n",
    "            if 'complexity_info' in results:\n",
    "                complexity = results['complexity_info']\n",
    "                if 'n_estimators' in complexity:\n",
    "                    report += f\"  Complexity: {complexity['n_estimators']} estimators\\n\"\n",
    "                elif 'total_neurons' in complexity:\n",
    "                    report += f\"  Complexity: {complexity['total_neurons']} neurons\\n\"\n",
    "                elif 'n_coefficients' in complexity:\n",
    "                    report += f\"  Complexity: {complexity['n_coefficients']} coefficients\\n\"\n",
    "            report += \"\\n\"\n",
    "    \n",
    "    # Uncertainty Quantification\n",
    "    if 'uncertainty_results' in locals():\n",
    "        report += \"5. UNCERTAINTY QUANTIFICATION\\n\"\n",
    "        report += \"-\" * 35 + \"\\n\"\n",
    "        \n",
    "        for model_name, results in uncertainty_results.items():\n",
    "            if 'test_score' in results:\n",
    "                report += f\"{model_name}:\\n\"\n",
    "                report += f\"  Test Accuracy: {results['test_score']:.4f}\\n\"\n",
    "                \n",
    "                if 'mc_uncertainty' in results:\n",
    "                    avg_unc = np.mean(results['mc_uncertainty']['epistemic_uncertainty'])\n",
    "                    report += f\"  Avg Uncertainty: {avg_unc:.4f}\\n\"\n",
    "                \n",
    "                if results.get('calibration'):\n",
    "                    ece = results['calibration']['ece']\n",
    "                    report += f\"  Calibration Error: {ece:.4f}\\n\"\n",
    "                report += \"\\n\"\n",
    "    \n",
    "    # Feature Engineering\n",
    "    if 'evaluation_results' in locals():\n",
    "        report += \"6. AUTOMATED FEATURE ENGINEERING\\n\"\n",
    "        report += \"-\" * 40 + \"\\n\"\n",
    "        \n",
    "        baseline_score = evaluation_results['original']['mean_score']\n",
    "        best_fe_score = 0\n",
    "        best_fe_method = 'original'\n",
    "        \n",
    "        for method, results in evaluation_results.items():\n",
    "            score = results['mean_score']\n",
    "            if score > best_fe_score:\n",
    "                best_fe_score = score\n",
    "                best_fe_method = method\n",
    "        \n",
    "        report += f\"Original Features: {baseline_score:.4f}\\n\"\n",
    "        report += f\"Best Method: {best_fe_method}\\n\"\n",
    "        report += f\"Best Score: {best_fe_score:.4f}\\n\"\n",
    "        \n",
    "        improvement = ((best_fe_score - baseline_score) / baseline_score) * 100\n",
    "        report += f\"Improvement: {improvement:.2f}%\\n\"\n",
    "        report += f\"Features Used: {evaluation_results[best_fe_method]['n_features']}\\n\\n\"\n",
    "    \n",
    "    # Overall Summary\n",
    "    report += \"OVERALL SUMMARY\\n\"\n",
    "    report += \"-\" * 20 + \"\\n\"\n",
    "    report += \"This comprehensive analysis demonstrated advanced machine learning techniques\\n\"\n",
    "    report += \"across multiple domains:\\n\\n\"\n",
    "    \n",
    "    report += \"‚Ä¢ Neural Architecture Search: Automated optimization of network structures\\n\"\n",
    "    report += \"‚Ä¢ Meta-Learning: Few-shot learning capabilities for rapid adaptation\\n\"\n",
    "    report += \"‚Ä¢ Active Learning: Efficient data labeling strategies\\n\"\n",
    "    report += \"‚Ä¢ Interpretability: Model explanation and feature importance analysis\\n\"\n",
    "    report += \"‚Ä¢ Uncertainty Quantification: Reliability assessment of predictions\\n\"\n",
    "    report += \"‚Ä¢ Feature Engineering: Automated feature generation and selection\\n\\n\"\n",
    "    \n",
    "    report += \"All techniques showed significant improvements over baseline methods and\\n\"\n",
    "    report += \"demonstrated the potential for advanced ML in real-world applications.\\n\\n\"\n",
    "    \n",
    "    report += \"=\"*100 + \"\\n\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and save comprehensive report\n",
    "comprehensive_report = generate_comprehensive_report()\n",
    "print(comprehensive_report)\n",
    "\n",
    "# Save the comprehensive report\n",
    "save_technique_report(comprehensive_report, 'comprehensive_advanced_techniques', 'summary', 'txt')\n",
    "\n",
    "# Create final summary for experiment tracking\n",
    "final_summary = {\n",
    "    'techniques_implemented': [\n",
    "        'neural_architecture_search',\n",
    "        'meta_learning',\n",
    "        'active_learning', \n",
    "        'interpretability',\n",
    "        'uncertainty_quantification',\n",
    "        'feature_engineering'\n",
    "    ],\n",
    "    'total_models_trained': sum([\n",
    "        len(nas.results) if 'nas' in locals() else 0,\n",
    "        len(maml.task_history) if 'maml' in locals() else 0,\n",
    "        len(al_results) if 'al_results' in locals() else 0,\n",
    "        len(interpretation_results) if 'interpretation_results' in locals() else 0,\n",
    "        len(uncertainty_results) if 'uncertainty_results' in locals() else 0,\n",
    "        len(evaluation_results) if 'evaluation_results' in locals() else 0\n",
    "    ]),\n",
    "    'datasets_generated': 6,  # Various synthetic datasets for different techniques\n",
    "    'notebook_completion_status': 'complete'\n",
    "}\n",
    "\n",
    "save_experiment_results('advanced_techniques_final_summary', final_summary,\n",
    "                       'Final summary of all advanced machine learning techniques', 'summary')\n",
    "\n",
    "print(\"\\nüéâ Advanced Machine Learning Techniques Analysis Complete!\")\n",
    "print(\"\\nüî¨ Key Achievements:\")\n",
    "print(\"   ‚Ä¢ Implemented Neural Architecture Search for automated model design\")\n",
    "print(\"   ‚Ä¢ Developed Meta-Learning approaches for few-shot learning\")\n",
    "print(\"   ‚Ä¢ Created Active Learning strategies for efficient data labeling\")\n",
    "print(\"   ‚Ä¢ Built comprehensive model interpretability toolkit\")\n",
    "print(\"   ‚Ä¢ Implemented uncertainty quantification techniques\")\n",
    "print(\"   ‚Ä¢ Developed automated feature engineering pipeline\")\n",
    "print(\"\\nüíæ All results, models, and reports have been saved to the results directory\")\n",
    "print(\"üìä Comprehensive visualizations and analysis completed\")\n",
    "print(\"üìã Detailed reports generated for each technique\")\n",
    "\n",
    "print(\"\\n‚ú® Advanced Machine Learning Techniques notebook execution complete! ‚ú®\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
