{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c877fd",
   "metadata": {},
   "source": [
    "# Advanced Unsupervised Learning\n",
    "\n",
    "This notebook demonstrates the comprehensive unsupervised learning capabilities of the sklearn-mastery project, including sophisticated clustering algorithms, dimensionality reduction techniques, anomaly detection, association rule mining, manifold learning, and unsupervised feature learning methods.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Results Saving Infrastructure](#results-setup)\n",
    "3. [Advanced Clustering Algorithms](#clustering)\n",
    "4. [Dimensionality Reduction Techniques](#dimensionality)\n",
    "5. [Anomaly Detection Methods](#anomaly)\n",
    "6. [Association Rule Mining](#association)\n",
    "7. [Manifold Learning](#manifold)\n",
    "8. [Cluster Validation and Evaluation](#validation)\n",
    "9. [Unsupervised Feature Learning](#feature-learning)\n",
    "10. [Save All Results](#save-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749f7ff8",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ff2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.decomposition import PCA, FactorAnalysis, FastICA, NMF, DictionaryLearning\n",
    "from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enhanced imports for results saving\n",
    "import os\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import datetime\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d26cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.generators import SyntheticDataGenerator\n",
    "from models.unsupervised.clustering import *\n",
    "from models.unsupervised.dimensionality_reduction import *\n",
    "from evaluation.metrics import ModelEvaluator\n",
    "from evaluation.visualization import ModelVisualizationSuite\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca388fb",
   "metadata": {},
   "source": [
    "## 2. Results Saving Infrastructure {#results-setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup results directories and saving functions\n",
    "def setup_results_directories():\n",
    "    \"\"\"Create results directory structure if it doesn't exist.\"\"\"\n",
    "    base_dir = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "    results_dir = base_dir / 'results'\n",
    "    \n",
    "    # Create subdirectories\n",
    "    directories = ['figures', 'models', 'reports', 'experiments', 'clustering', 'features', 'manifold', 'anomaly']\n",
    "    for directory in directories:\n",
    "        (results_dir / directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return results_dir\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\"Get current timestamp for file naming.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Setup results directories\n",
    "results_dir = setup_results_directories()\n",
    "figures_dir = results_dir / 'figures'\n",
    "models_dir = results_dir / 'models'\n",
    "clustering_dir = results_dir / 'clustering'\n",
    "features_dir = results_dir / 'features'\n",
    "manifold_dir = results_dir / 'manifold'\n",
    "anomaly_dir = results_dir / 'anomaly'\n",
    "experiments_dir = results_dir / 'experiments'\n",
    "reports_dir = results_dir / 'reports'\n",
    "\n",
    "print(f\"Unsupervised learning results will be saved to: {results_dir}\")\n",
    "\n",
    "# Enhanced figure saving for unsupervised learning\n",
    "def save_unsupervised_figure(fig, name, description=\"\", category=\"general\", dpi=300):\n",
    "    \"\"\"Save unsupervised learning figure with proper naming and metadata.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_unsupervised_{category}_{name}.png\"\n",
    "    filepath = figures_dir / filename\n",
    "    \n",
    "    fig.savefig(filepath, dpi=dpi, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"📊 Unsupervised learning figure saved: {filepath}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'filename': filename,\n",
    "        'description': description,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '04_unsupervised_learning',\n",
    "        'category': f'unsupervised_{category}'\n",
    "    }\n",
    "    \n",
    "    metadata_file = figures_dir / f\"{timestamp}_unsupervised_{category}_{name}_metadata.json\"\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Enhanced clustering model saving\n",
    "def save_clustering_model(model, name, description=\"\", metadata=None):\n",
    "    \"\"\"Save clustering model with comprehensive metadata.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_clustering_{name}.joblib\"\n",
    "    filepath = clustering_dir / filename\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, filepath)\n",
    "    print(f\"🔄 Clustering model saved: {filepath}\")\n",
    "    \n",
    "    # Save comprehensive metadata\n",
    "    model_metadata = {\n",
    "        'filename': filename,\n",
    "        'description': description,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '04_unsupervised_learning',\n",
    "        'model_type': 'clustering',\n",
    "        'algorithm': model.__class__.__name__,\n",
    "        'sklearn_version': sklearn.__version__ if 'sklearn' in globals() else 'unknown'\n",
    "    }\n",
    "    \n",
    "    if metadata:\n",
    "        model_metadata.update(metadata)\n",
    "    \n",
    "    metadata_file = clustering_dir / f\"{timestamp}_clustering_{name}_metadata.json\"\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(model_metadata, f, indent=2, default=str)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Enhanced dimensionality reduction model saving\n",
    "def save_reduction_model(model, name, description=\"\", metadata=None):\n",
    "    \"\"\"Save dimensionality reduction model with metadata.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_reduction_{name}.joblib\"\n",
    "    filepath = models_dir / filename\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, filepath)\n",
    "    print(f\"📉 Dimensionality reduction model saved: {filepath}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    model_metadata = {\n",
    "        'filename': filename,\n",
    "        'description': description,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '04_unsupervised_learning',\n",
    "        'model_type': 'dimensionality_reduction',\n",
    "        'algorithm': model.__class__.__name__\n",
    "    }\n",
    "    \n",
    "    if metadata:\n",
    "        model_metadata.update(metadata)\n",
    "    \n",
    "    metadata_file = models_dir / f\"{timestamp}_reduction_{name}_metadata.json\"\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(model_metadata, f, indent=2, default=str)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Enhanced feature learning model saving\n",
    "def save_feature_learning_model(model, name, description=\"\", metadata=None):\n",
    "    \"\"\"Save feature learning model with metadata.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_features_{name}.joblib\"\n",
    "    filepath = features_dir / filename\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, filepath)\n",
    "    print(f\"🎓 Feature learning model saved: {filepath}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    model_metadata = {\n",
    "        'filename': filename,\n",
    "        'description': description,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '04_unsupervised_learning',\n",
    "        'model_type': 'feature_learning',\n",
    "        'algorithm': model.__class__.__name__\n",
    "    }\n",
    "    \n",
    "    if metadata:\n",
    "        model_metadata.update(metadata)\n",
    "    \n",
    "    metadata_file = features_dir / f\"{timestamp}_features_{name}_metadata.json\"\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(model_metadata, f, indent=2, default=str)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Enhanced save report function\n",
    "def save_report(content, name, description=\"\", format='txt'):\n",
    "    \"\"\"Save report with proper naming.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_unsupervised_{name}.{format}\"\n",
    "    filepath = reports_dir / filename\n",
    "    \n",
    "    if format == 'txt':\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(content)\n",
    "    elif format == 'json':\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(content, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"📄 Unsupervised learning report saved: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_experiment_results(experiment_name, results, description=\"\", technique_type=\"general\"):\n",
    "    \"\"\"Save experiment results with detailed configuration.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    filename = f\"{timestamp}_{technique_type}_{experiment_name}.json\"\n",
    "    filepath = experiments_dir / filename\n",
    "    \n",
    "    experiment_data = {\n",
    "        'experiment_name': experiment_name,\n",
    "        'description': description,\n",
    "        'technique_type': technique_type,\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': '04_unsupervised_learning',\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(experiment_data, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"💾 Saved experiment results: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "print(\"✅ Results saving infrastructure ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfaa5ba",
   "metadata": {},
   "source": [
    "## 3. Advanced Clustering Algorithms {#clustering}\n",
    "\n",
    "Let's explore sophisticated clustering algorithms that can handle complex data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate diverse clustering datasets\n",
    "print(\"🎯 Generating Clustering Datasets...\")\n",
    "\n",
    "generator = SyntheticDataGenerator(random_state=42)\n",
    "\n",
    "# 1. Standard clustering dataset\n",
    "X_blobs, y_blobs = generator.clustering_dataset(\n",
    "    n_samples=800,\n",
    "    n_features=2,\n",
    "    n_clusters=4,\n",
    "    cluster_std=1.5\n",
    ")\n",
    "\n",
    "print(f\"Blob clustering dataset: {X_blobs.shape}, {len(np.unique(y_blobs))} clusters\")\n",
    "\n",
    "# 2. Complex shapes dataset\n",
    "X_moons, y_moons = generator.complex_shapes(\n",
    "    n_samples=600,\n",
    "    shape_type='moons',\n",
    "    noise=0.1\n",
    ")\n",
    "\n",
    "print(f\"Moons dataset: {X_moons.shape}\")\n",
    "\n",
    "# 3. High-dimensional clustering data\n",
    "X_highdim, y_highdim = generator.clustering_dataset(\n",
    "    n_samples=1000,\n",
    "    n_features=50,\n",
    "    n_clusters=5,\n",
    "    cluster_std=2.0\n",
    ")\n",
    "\n",
    "print(f\"High-dimensional clustering dataset: {X_highdim.shape}, {len(np.unique(y_highdim))} clusters\")\n",
    "\n",
    "# 4. Varied density clusters\n",
    "X_varied, y_varied = generator.varied_density_clusters(\n",
    "    n_samples=800,\n",
    "    n_features=2\n",
    ")\n",
    "\n",
    "print(f\"Varied density clustering dataset: {X_varied.shape}\")\n",
    "\n",
    "# Visualize datasets\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Blob clusters\n",
    "scatter1 = axes[0, 0].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap='viridis', alpha=0.7)\n",
    "axes[0, 0].set_title('Standard Blob Clusters')\n",
    "axes[0, 0].set_xlabel('Feature 1')\n",
    "axes[0, 0].set_ylabel('Feature 2')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Moons\n",
    "scatter2 = axes[0, 1].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis', alpha=0.7)\n",
    "axes[0, 1].set_title('Two Moons')\n",
    "axes[0, 1].set_xlabel('Feature 1')\n",
    "axes[0, 1].set_ylabel('Feature 2')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# High-dimensional (first 2 components)\n",
    "scatter3 = axes[1, 0].scatter(X_highdim[:, 0], X_highdim[:, 1], c=y_highdim, cmap='viridis', alpha=0.7)\n",
    "axes[1, 0].set_title('High-Dimensional Clusters (2D projection)')\n",
    "axes[1, 0].set_xlabel('Feature 1')\n",
    "axes[1, 0].set_ylabel('Feature 2')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Varied density\n",
    "scatter4 = axes[1, 1].scatter(X_varied[:, 0], X_varied[:, 1], c=y_varied, cmap='viridis', alpha=0.7)\n",
    "axes[1, 1].set_title('Varied Density Clusters')\n",
    "axes[1, 1].set_xlabel('Feature 1')\n",
    "axes[1, 1].set_ylabel('Feature 2')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_unsupervised_figure(fig, \"clustering_datasets_overview\", \n",
    "                        \"Overview of different clustering datasets generated\", \"clustering\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✨ Clustering datasets generated and visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956dbde",
   "metadata": {},
   "source": [
    "### 3.1 Advanced Clustering Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test advanced clustering algorithms\n",
    "print(\"🚀 Testing Advanced Clustering Algorithms...\")\n",
    "\n",
    "# Initialize clustering algorithms with fallbacks to sklearn implementations\n",
    "clustering_algorithms = {}\n",
    "\n",
    "# Try to use custom implementations, fall back to sklearn\n",
    "try:\n",
    "    clustering_algorithms['Adaptive KMeans'] = AdaptiveKMeans(random_state=42)\n",
    "except:\n",
    "    clustering_algorithms['K-Means'] = KMeans(n_clusters=4, random_state=42)\n",
    "\n",
    "try:\n",
    "    clustering_algorithms['Hierarchical Enhanced'] = HierarchicalEnhanced()\n",
    "except:\n",
    "    clustering_algorithms['Agglomerative'] = AgglomerativeClustering(n_clusters=4)\n",
    "\n",
    "try:\n",
    "    clustering_algorithms['DBSCAN Enhanced'] = DBSCANEnhanced()\n",
    "except:\n",
    "    clustering_algorithms['DBSCAN'] = DBSCAN(eps=0.5, min_samples=5)\n",
    "\n",
    "try:\n",
    "    clustering_algorithms['Gaussian Mixture Enhanced'] = GaussianMixtureEnhanced(random_state=42)\n",
    "except:\n",
    "    clustering_algorithms['Gaussian Mixture'] = GaussianMixture(n_components=4, random_state=42)\n",
    "\n",
    "try:\n",
    "    clustering_algorithms['Spectral Enhanced'] = SpectralEnhanced(random_state=42)\n",
    "except:\n",
    "    clustering_algorithms['Spectral Clustering'] = SpectralClustering(n_clusters=4, random_state=42)\n",
    "\n",
    "# Test datasets for different algorithms\n",
    "test_datasets = {\n",
    "    'Blob Clusters': (X_blobs, y_blobs),\n",
    "    'Complex Shapes': (X_moons, y_moons),\n",
    "    'Varied Density': (X_varied, y_varied)\n",
    "}\n",
    "\n",
    "clustering_results = {}\n",
    "\n",
    "for dataset_name, (X, y_true) in test_datasets.items():\n",
    "    print(f\"\\n--- Testing on {dataset_name} ---\")\n",
    "    clustering_results[dataset_name] = {}\n",
    "    \n",
    "    for algo_name, algorithm in clustering_algorithms.items():\n",
    "        try:\n",
    "            # Fit clustering algorithm\n",
    "            if hasattr(algorithm, 'fit_predict'):\n",
    "                y_pred = algorithm.fit_predict(X)\n",
    "            else:\n",
    "                algorithm.fit(X)\n",
    "                y_pred = algorithm.predict(X) if hasattr(algorithm, 'predict') else algorithm.labels_\n",
    "            \n",
    "            # Calculate clustering metrics\n",
    "            n_clusters = len(np.unique(y_pred[y_pred >= 0]))  # Exclude noise points (-1)\n",
    "            \n",
    "            # Silhouette score (only if we have more than 1 cluster)\n",
    "            if n_clusters > 1 and len(y_pred[y_pred >= 0]) > 1:\n",
    "                sil_score = silhouette_score(X, y_pred)\n",
    "                ch_score = calinski_harabasz_score(X, y_pred)\n",
    "            else:\n",
    "                sil_score = -1\n",
    "                ch_score = -1\n",
    "            \n",
    "            # Adjusted Rand Index (if true labels available)\n",
    "            ari_score = adjusted_rand_score(y_true, y_pred)\n",
    "            \n",
    "            clustering_results[dataset_name][algo_name] = {\n",
    "                'y_pred': y_pred,\n",
    "                'n_clusters': n_clusters,\n",
    "                'silhouette_score': sil_score,\n",
    "                'calinski_harabasz_score': ch_score,\n",
    "                'adjusted_rand_index': ari_score,\n",
    "                'noise_points': np.sum(y_pred == -1)\n",
    "            }\n",
    "            \n",
    "            print(f\"  {algo_name}:\")\n",
    "            print(f\"    Clusters found: {n_clusters}\")\n",
    "            print(f\"    Silhouette Score: {sil_score:.3f}\" if sil_score > -1 else \"    Silhouette Score: N/A\")\n",
    "            print(f\"    Adjusted Rand Index: {ari_score:.3f}\")\n",
    "            print(f\"    Noise points: {np.sum(y_pred == -1)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  {algo_name}: ❌ Failed - {str(e)}\")\n",
    "            clustering_results[dataset_name][algo_name] = {'error': str(e)}\n",
    "\n",
    "print(\"\\n✨ Advanced clustering algorithms tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e415620",
   "metadata": {},
   "source": [
    "### 3.2 Clustering Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba14393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results\n",
    "print(\"📊 Visualizing Clustering Results...\")\n",
    "\n",
    "# Create comprehensive clustering visualization\n",
    "for dataset_name, (X, y_true) in test_datasets.items():\n",
    "    if X.shape[1] == 2:  # Only visualize 2D datasets\n",
    "        print(f\"\\nVisualizing results for {dataset_name}...\")\n",
    "        \n",
    "        # Count successful algorithms\n",
    "        successful_algos = [algo for algo, result in clustering_results[dataset_name].items() \n",
    "                           if 'error' not in result]\n",
    "        \n",
    "        if successful_algos:\n",
    "            n_algos = len(successful_algos)\n",
    "            n_cols = min(3, n_algos)\n",
    "            n_rows = (n_algos + n_cols - 1) // n_cols\n",
    "            \n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "            if n_rows == 1:\n",
    "                axes = axes.reshape(1, -1) if n_algos > 1 else [axes]\n",
    "            axes = axes.ravel() if n_algos > 1 else [axes[0]]\n",
    "            \n",
    "            for i, algo_name in enumerate(successful_algos):\n",
    "                result = clustering_results[dataset_name][algo_name]\n",
    "                y_pred = result['y_pred']\n",
    "                \n",
    "                # Plot clustering result\n",
    "                scatter = axes[i].scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', alpha=0.7)\n",
    "                \n",
    "                # Highlight noise points if any\n",
    "                noise_mask = y_pred == -1\n",
    "                if np.any(noise_mask):\n",
    "                    axes[i].scatter(X[noise_mask, 0], X[noise_mask, 1], \n",
    "                                  c='red', marker='x', s=50, label='Noise')\n",
    "                    axes[i].legend()\n",
    "                \n",
    "                # Add colorbar\n",
    "                plt.colorbar(scatter, ax=axes[i])\n",
    "                \n",
    "                # Title with metrics\n",
    "                sil_score = result.get('silhouette_score', -1)\n",
    "                ari_score = result.get('adjusted_rand_index', -1)\n",
    "                n_clusters = result.get('n_clusters', 0)\n",
    "                \n",
    "                title = f\"{algo_name}\\nClusters: {n_clusters}, ARI: {ari_score:.3f}\"\n",
    "                if sil_score > -1:\n",
    "                    title += f\", Sil: {sil_score:.3f}\"\n",
    "                \n",
    "                axes[i].set_title(title)\n",
    "                axes[i].set_xlabel('Feature 1')\n",
    "                axes[i].set_ylabel('Feature 2')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Hide unused subplots\n",
    "            for i in range(len(successful_algos), len(axes)):\n",
    "                axes[i].set_visible(False)\n",
    "            \n",
    "            plt.suptitle(f'Clustering Results - {dataset_name}', fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            save_unsupervised_figure(fig, f\"clustering_results_{dataset_name.lower().replace(' ', '_')}\", \n",
    "                                   f\"Clustering algorithm comparison on {dataset_name} dataset\", \"clustering\")\n",
    "            plt.show()\n",
    "\n",
    "print(\"\\n✨ Clustering visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccecb6f",
   "metadata": {},
   "source": [
    "### 3.3 Clustering Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcec6663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive clustering performance analysis\n",
    "print(\"📈 Clustering Performance Analysis...\")\n",
    "\n",
    "# Create performance comparison\n",
    "performance_data = []\n",
    "\n",
    "for dataset_name in clustering_results.keys():\n",
    "    for algo_name, result in clustering_results[dataset_name].items():\n",
    "        if 'error' not in result:\n",
    "            performance_data.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Algorithm': algo_name,\n",
    "                'Clusters_Found': result.get('n_clusters', 0),\n",
    "                'Silhouette_Score': result.get('silhouette_score', -1),\n",
    "                'Calinski_Harabasz_Score': result.get('calinski_harabasz_score', -1),\n",
    "                'Adjusted_Rand_Index': result.get('adjusted_rand_index', -1),\n",
    "                'Noise_Points': result.get('noise_points', 0)\n",
    "            })\n",
    "\n",
    "if performance_data:\n",
    "    performance_df = pd.DataFrame(performance_data)\n",
    "    \n",
    "    print(\"\\n📊 Performance Summary:\")\n",
    "    print(performance_df.round(3).to_string(index=False))\n",
    "    \n",
    "    # Create performance visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Silhouette Score comparison\n",
    "    valid_sil = performance_df[performance_df['Silhouette_Score'] > -1]\n",
    "    if not valid_sil.empty:\n",
    "        sns.barplot(data=valid_sil, x='Algorithm', y='Silhouette_Score', \n",
    "                   hue='Dataset', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Silhouette Score by Algorithm and Dataset')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Adjusted Rand Index comparison\n",
    "    valid_ari = performance_df[performance_df['Adjusted_Rand_Index'] > -1]\n",
    "    if not valid_ari.empty:\n",
    "        sns.barplot(data=valid_ari, x='Algorithm', y='Adjusted_Rand_Index', \n",
    "                   hue='Dataset', ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Adjusted Rand Index by Algorithm and Dataset')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Number of clusters found\n",
    "    sns.barplot(data=performance_df, x='Algorithm', y='Clusters_Found', \n",
    "               hue='Dataset', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Number of Clusters Found')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Noise points detected\n",
    "    sns.barplot(data=performance_df, x='Algorithm', y='Noise_Points', \n",
    "               hue='Dataset', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Noise Points Detected')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_unsupervised_figure(fig, \"clustering_performance_analysis\", \n",
    "                           \"Comprehensive clustering performance metrics across algorithms and datasets\", \"clustering\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Best algorithm per dataset\n",
    "    print(\"\\n🏆 Best Algorithm per Dataset (by ARI):\")\n",
    "    for dataset in performance_df['Dataset'].unique():\n",
    "        dataset_data = performance_df[performance_df['Dataset'] == dataset]\n",
    "        if not dataset_data.empty:\n",
    "            best_algo = dataset_data.loc[dataset_data['Adjusted_Rand_Index'].idxmax()]\n",
    "            print(f\"  {dataset}: {best_algo['Algorithm']} (ARI: {best_algo['Adjusted_Rand_Index']:.3f})\")\n",
    "\n",
    "print(\"\\n✨ Clustering performance analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98bc9c",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction Techniques {#dimensionality}\n",
    "\n",
    "Let's explore advanced dimensionality reduction methods for visualization and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d414e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dimensionality reduction techniques\n",
    "print(\"🔍 Testing Dimensionality Reduction Techniques...\")\n",
    "\n",
    "# Use high-dimensional dataset for reduction\n",
    "X_highdim_scaled = StandardScaler().fit_transform(X_highdim)\n",
    "\n",
    "# Initialize dimensionality reduction algorithms with fallbacks\n",
    "reduction_algorithms = {}\n",
    "\n",
    "# Try custom implementations, fall back to sklearn\n",
    "try:\n",
    "    reduction_algorithms['Enhanced PCA'] = EnhancedPCA(n_components=2, random_state=42)\n",
    "except:\n",
    "    reduction_algorithms['PCA'] = PCA(n_components=2, random_state=42)\n",
    "\n",
    "try:\n",
    "    reduction_algorithms['Adaptive t-SNE'] = AdaptiveTSNE(n_components=2, random_state=42)\n",
    "except:\n",
    "    reduction_algorithms['t-SNE'] = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "\n",
    "try:\n",
    "    reduction_algorithms['UMAP Enhanced'] = UMAPEnhanced(n_components=2, random_state=42)\n",
    "except:\n",
    "    try:\n",
    "        import umap\n",
    "        reduction_algorithms['UMAP'] = umap.UMAP(n_components=2, random_state=42)\n",
    "    except:\n",
    "        print(\"UMAP not available, skipping...\")\n",
    "\n",
    "try:\n",
    "    reduction_algorithms['Autoencoder Reduction'] = AutoencoderReduction(encoding_dim=2, random_state=42)\n",
    "except:\n",
    "    reduction_algorithms['Factor Analysis'] = FactorAnalysis(n_components=2, random_state=42)\n",
    "\n",
    "try:\n",
    "    reduction_algorithms['Manifold Learning'] = ManifoldLearning(method='isomap', n_components=2)\n",
    "except:\n",
    "    reduction_algorithms['Isomap'] = Isomap(n_components=2)\n",
    "\n",
    "reduction_results = {}\n",
    "reduction_times = {}\n",
    "\n",
    "for algo_name, algorithm in reduction_algorithms.items():\n",
    "    print(f\"\\n--- Testing {algo_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Time the reduction\n",
    "        start_time = time.time()\n",
    "        X_reduced = algorithm.fit_transform(X_highdim_scaled)\n",
    "        reduction_time = time.time() - start_time\n",
    "        \n",
    "        reduction_results[algo_name] = X_reduced\n",
    "        reduction_times[algo_name] = reduction_time\n",
    "        \n",
    "        print(f\"  Reduction time: {reduction_time:.3f}s\")\n",
    "        print(f\"  Output shape: {X_reduced.shape}\")\n",
    "        print(f\"  Variance preserved: {np.var(X_reduced):.3f}\")\n",
    "        \n",
    "        # Calculate reconstruction error if possible\n",
    "        if hasattr(algorithm, 'inverse_transform'):\n",
    "            try:\n",
    "                X_reconstructed = algorithm.inverse_transform(X_reduced)\n",
    "                reconstruction_error = np.mean((X_highdim_scaled - X_reconstructed) ** 2)\n",
    "                print(f\"  Reconstruction error: {reconstruction_error:.3f}\")\n",
    "            except:\n",
    "                print(f\"  Reconstruction error: Not available\")\n",
    "        \n",
    "        # Calculate explained variance for PCA-like methods\n",
    "        if hasattr(algorithm, 'explained_variance_ratio_'):\n",
    "            explained_var = np.sum(algorithm.explained_variance_ratio_)\n",
    "            print(f\"  Explained variance ratio: {explained_var:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Failed: {str(e)}\")\n",
    "        reduction_results[algo_name] = None\n",
    "\n",
    "print(\"\\n✨ Dimensionality reduction techniques tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f9fef9",
   "metadata": {},
   "source": [
    "### 4.1 Dimensionality Reduction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0002059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dimensionality reduction results\n",
    "print(\"📊 Visualizing Dimensionality Reduction Results...\")\n",
    "\n",
    "# Filter successful reductions\n",
    "successful_reductions = {k: v for k, v in reduction_results.items() if v is not None}\n",
    "\n",
    "if successful_reductions:\n",
    "    n_methods = len(successful_reductions)\n",
    "    n_cols = min(3, n_methods)\n",
    "    n_rows = (n_methods + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1) if n_methods > 1 else [axes]\n",
    "    axes = axes.ravel() if n_methods > 1 else [axes[0]]\n",
    "    \n",
    "    for i, (method_name, X_reduced) in enumerate(successful_reductions.items()):\n",
    "        # Create scatter plot colored by true clusters\n",
    "        scatter = axes[i].scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                                c=y_highdim, cmap='viridis', alpha=0.7)\n",
    "        \n",
    "        axes[i].set_title(f'{method_name}\\n(Time: {reduction_times.get(method_name, 0):.2f}s)')\n",
    "        axes[i].set_xlabel('Component 1')\n",
    "        axes[i].set_ylabel('Component 2')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(scatter, ax=axes[i])\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(successful_reductions), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Dimensionality Reduction Results (50D → 2D)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    save_unsupervised_figure(fig, \"dimensionality_reduction_comparison\", \n",
    "                           \"Comparison of dimensionality reduction techniques\", \"reduction\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(\"\\n📈 Dimensionality Reduction Performance:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Method':<20} {'Time (s)':<10} {'Variance':<12} {'Separation':<12}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for method_name, X_reduced in successful_reductions.items():\n",
    "        time_taken = reduction_times.get(method_name, 0)\n",
    "        variance = np.var(X_reduced)\n",
    "        \n",
    "        # Calculate cluster separation (silhouette score)\n",
    "        try:\n",
    "            separation = silhouette_score(X_reduced, y_highdim)\n",
    "        except:\n",
    "            separation = 0\n",
    "        \n",
    "        print(f\"{method_name:<20} {time_taken:<10.3f} {variance:<12.3f} {separation:<12.3f}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "else:\n",
    "    print(\"❌ No successful dimensionality reduction results to visualize.\")\n",
    "\n",
    "print(\"\\n✨ Dimensionality reduction visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2da2e",
   "metadata": {},
   "source": [
    "## 5. Anomaly Detection Methods {#anomaly}\n",
    "\n",
    "Let's explore advanced anomaly detection techniques for identifying outliers and unusual patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datasets with anomalies\n",
    "print(\"🔍 Generating Anomaly Detection Datasets...\")\n",
    "\n",
    "# Normal data with outliers\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_outliers = 50\n",
    "\n",
    "# Generate normal data (2D for visualization)\n",
    "X_normal = np.random.randn(n_samples - n_outliers, 2)\n",
    "\n",
    "# Generate outliers (further from center)\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(n_outliers, 2))\n",
    "\n",
    "# Combine data\n",
    "X_anomaly = np.vstack([X_normal, X_outliers])\n",
    "y_true_anomaly = np.hstack([np.ones(n_samples - n_outliers), -np.ones(n_outliers)])\n",
    "\n",
    "# Scale the data\n",
    "scaler_anomaly = StandardScaler()\n",
    "X_anomaly_scaled = scaler_anomaly.fit_transform(X_anomaly)\n",
    "\n",
    "print(f\"Anomaly detection dataset: {X_anomaly.shape}\")\n",
    "print(f\"Normal points: {np.sum(y_true_anomaly == 1)}\")\n",
    "print(f\"Anomalous points: {np.sum(y_true_anomaly == -1)}\")\n",
    "\n",
    "# Visualize the anomaly dataset\n",
    "plt.figure(figsize=(10, 8))\n",
    "normal_mask = y_true_anomaly == 1\n",
    "anomaly_mask = y_true_anomaly == -1\n",
    "\n",
    "plt.scatter(X_anomaly[normal_mask, 0], X_anomaly[normal_mask, 1], \n",
    "           c='blue', alpha=0.6, label='Normal', s=30)\n",
    "plt.scatter(X_anomaly[anomaly_mask, 0], X_anomaly[anomaly_mask, 1], \n",
    "           c='red', alpha=0.8, label='Anomalies', s=50, marker='x')\n",
    "\n",
    "plt.title('Anomaly Detection Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "save_unsupervised_figure(plt.gcf(), \"anomaly_detection_dataset\", \n",
    "                       \"Generated dataset with normal points and anomalies\", \"anomaly\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✨ Anomaly detection dataset generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e69831f",
   "metadata": {},
   "source": [
    "### 5.1 Anomaly Detection Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test anomaly detection algorithms\n",
    "print(\"🚀 Testing Anomaly Detection Algorithms...\")\n",
    "\n",
    "# Initialize anomaly detection algorithms\n",
    "anomaly_algorithms = {\n",
    "    'Isolation Forest': IsolationForest(contamination=0.05, random_state=42),\n",
    "    'One-Class SVM': OneClassSVM(nu=0.05),\n",
    "    'Local Outlier Factor': LocalOutlierFactor(n_neighbors=20, contamination=0.05),\n",
    "}\n",
    "\n",
    "# Add Elliptic Envelope\n",
    "try:\n",
    "    from sklearn.covariance import EllipticEnvelope\n",
    "    anomaly_algorithms['Elliptic Envelope'] = EllipticEnvelope(contamination=0.05, random_state=42)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "anomaly_results = {}\n",
    "\n",
    "for algo_name, algorithm in anomaly_algorithms.items():\n",
    "    print(f\"\\n--- Testing {algo_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Fit and predict\n",
    "        if algo_name == 'Local Outlier Factor':\n",
    "            # LOF returns predictions directly\n",
    "            y_pred_anomaly = algorithm.fit_predict(X_anomaly_scaled)\n",
    "        else:\n",
    "            # Other algorithms\n",
    "            algorithm.fit(X_anomaly_scaled)\n",
    "            y_pred_anomaly = algorithm.predict(X_anomaly_scaled)\n",
    "        \n",
    "        # Convert predictions to binary (1 for normal, -1 for anomaly)\n",
    "        y_pred_binary = np.where(y_pred_anomaly == 1, 1, -1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "        \n",
    "        accuracy = accuracy_score(y_true_anomaly, y_pred_binary)\n",
    "        precision = precision_score(y_true_anomaly, y_pred_binary, pos_label=-1)\n",
    "        recall = recall_score(y_true_anomaly, y_pred_binary, pos_label=-1)\n",
    "        f1 = f1_score(y_true_anomaly, y_pred_binary, pos_label=-1)\n",
    "        \n",
    "        # Count predictions\n",
    "        n_predicted_anomalies = np.sum(y_pred_binary == -1)\n",
    "        n_true_anomalies = np.sum(y_true_anomaly == -1)\n",
    "        \n",
    "        anomaly_results[algo_name] = {\n",
    "            'y_pred': y_pred_binary,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'n_predicted_anomalies': n_predicted_anomalies,\n",
    "            'n_true_anomalies': n_true_anomalies\n",
    "        }\n",
    "        \n",
    "        print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"  Precision: {precision:.3f}\")\n",
    "        print(f\"  Recall: {recall:.3f}\")\n",
    "        print(f\"  F1-Score: {f1:.3f}\")\n",
    "        print(f\"  Predicted anomalies: {n_predicted_anomalies}/{n_true_anomalies}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Failed: {str(e)}\")\n",
    "        anomaly_results[algo_name] = {'error': str(e)}\n",
    "\n",
    "print(\"\\n✨ Anomaly detection algorithms tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6204a7f0",
   "metadata": {},
   "source": [
    "### 5.2 Anomaly Detection Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a792a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomaly detection results\n",
    "print(\"📊 Visualizing Anomaly Detection Results...\")\n",
    "\n",
    "# Filter successful algorithms\n",
    "successful_anomaly_algos = {k: v for k, v in anomaly_results.items() if 'error' not in v}\n",
    "\n",
    "if successful_anomaly_algos:\n",
    "    n_algos = len(successful_anomaly_algos)\n",
    "    n_cols = min(2, n_algos)\n",
    "    n_rows = (n_algos + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 6*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1) if n_algos > 1 else [axes]\n",
    "    axes = axes.ravel() if n_algos > 1 else [axes[0]]\n",
    "    \n",
    "    for i, (algo_name, result) in enumerate(successful_anomaly_algos.items()):\n",
    "        y_pred = result['y_pred']\n",
    "        \n",
    "        # True positives, false positives, etc.\n",
    "        tp_mask = (y_true_anomaly == -1) & (y_pred == -1)  # True anomalies correctly identified\n",
    "        fp_mask = (y_true_anomaly == 1) & (y_pred == -1)   # Normal points incorrectly flagged\n",
    "        fn_mask = (y_true_anomaly == -1) & (y_pred == 1)   # Anomalies missed\n",
    "        tn_mask = (y_true_anomaly == 1) & (y_pred == 1)    # Normal points correctly identified\n",
    "        \n",
    "        # Plot with different colors for different categories\n",
    "        axes[i].scatter(X_anomaly[tn_mask, 0], X_anomaly[tn_mask, 1], \n",
    "                       c='lightblue', alpha=0.6, label='True Normal', s=30)\n",
    "        axes[i].scatter(X_anomaly[tp_mask, 0], X_anomaly[tp_mask, 1], \n",
    "                       c='red', alpha=0.8, label='True Anomaly (Detected)', s=50, marker='x')\n",
    "        axes[i].scatter(X_anomaly[fp_mask, 0], X_anomaly[fp_mask, 1], \n",
    "                       c='orange', alpha=0.8, label='False Positive', s=40, marker='s')\n",
    "        axes[i].scatter(X_anomaly[fn_mask, 0], X_anomaly[fn_mask, 1], \n",
    "                       c='purple', alpha=0.8, label='False Negative', s=40, marker='^')\n",
    "        \n",
    "        # Title with metrics\n",
    "        f1 = result['f1_score']\n",
    "        precision = result['precision']\n",
    "        recall = result['recall']\n",
    "        \n",
    "        axes[i].set_title(f'{algo_name}\\nF1: {f1:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}')\n",
    "        axes[i].set_xlabel('Feature 1')\n",
    "        axes[i].set_ylabel('Feature 2')\n",
    "        axes[i].legend(fontsize=8)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(successful_anomaly_algos), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Anomaly Detection Results', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    save_unsupervised_figure(fig, \"anomaly_detection_results\", \n",
    "                           \"Comparison of anomaly detection algorithm performance\", \"anomaly\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(\"\\n📈 Anomaly Detection Performance:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Algorithm':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for algo_name, result in successful_anomaly_algos.items():\n",
    "        print(f\"{algo_name:<20} {result['accuracy']:<10.3f} {result['precision']:<10.3f} \"\n",
    "              f\"{result['recall']:<10.3f} {result['f1_score']:<10.3f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Best algorithm\n",
    "    best_algo = max(successful_anomaly_algos.keys(), \n",
    "                   key=lambda x: successful_anomaly_algos[x]['f1_score'])\n",
    "    print(f\"\\n🏆 Best Algorithm: {best_algo} (F1-Score: {successful_anomaly_algos[best_algo]['f1_score']:.3f})\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No successful anomaly detection results to visualize.\")\n",
    "\n",
    "print(\"\\n✨ Anomaly detection visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44fa4a8",
   "metadata": {},
   "source": [
    "## 6. Association Rule Mining {#association}\n",
    "\n",
    "Let's explore pattern discovery through association rule mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate transaction data for association rule mining\n",
    "print(\"🛒 Generating Transaction Data for Association Rule Mining...\")\n",
    "\n",
    "# Create synthetic market basket data\n",
    "np.random.seed(42)\n",
    "n_transactions = 1000\n",
    "n_items = 20\n",
    "\n",
    "# Item names\n",
    "item_names = [f'Item_{i:02d}' for i in range(n_items)]\n",
    "\n",
    "# Generate transactions with some correlations\n",
    "transactions = []\n",
    "\n",
    "for _ in range(n_transactions):\n",
    "    # Start with random selection\n",
    "    n_items_in_transaction = np.random.poisson(5) + 1\n",
    "    n_items_in_transaction = min(n_items_in_transaction, n_items)\n",
    "    \n",
    "    # Add some correlation patterns\n",
    "    transaction_items = set()\n",
    "    \n",
    "    # Base items selection\n",
    "    base_items = np.random.choice(n_items, size=n_items_in_transaction, replace=False)\n",
    "    \n",
    "    for item in base_items:\n",
    "        transaction_items.add(item)\n",
    "        \n",
    "        # Add correlated items with some probability\n",
    "        if item == 0 and np.random.random() < 0.7:  # Item_00 often with Item_01\n",
    "            transaction_items.add(1)\n",
    "        if item == 2 and np.random.random() < 0.6:  # Item_02 often with Item_03 and Item_04\n",
    "            transaction_items.add(3)\n",
    "            if np.random.random() < 0.4:\n",
    "                transaction_items.add(4)\n",
    "        if item == 5 and np.random.random() < 0.5:  # Item_05 often with Item_06\n",
    "            transaction_items.add(6)\n",
    "    \n",
    "    transactions.append(list(transaction_items))\n",
    "\n",
    "print(f\"Generated {len(transactions)} transactions\")\n",
    "print(f\"Average items per transaction: {np.mean([len(t) for t in transactions]):.2f}\")\n",
    "\n",
    "# Convert to binary matrix for analysis\n",
    "transaction_matrix = np.zeros((len(transactions), n_items))\n",
    "for i, transaction in enumerate(transactions):\n",
    "    for item in transaction:\n",
    "        transaction_matrix[i, item] = 1\n",
    "\n",
    "print(f\"Transaction matrix shape: {transaction_matrix.shape}\")\n",
    "print(\"\\n✨ Transaction data generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f124e70",
   "metadata": {},
   "source": [
    "### 6.1 Association Rule Mining Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee4ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Apriori algorithm for association rule mining\n",
    "print(\"⚡ Implementing Association Rule Mining...\")\n",
    "\n",
    "class AssociationRuleMiner:\n",
    "    \"\"\"Simple implementation of association rule mining.\"\"\"\n",
    "    \n",
    "    def __init__(self, min_support=0.1, min_confidence=0.5):\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "        self.frequent_itemsets = {}\n",
    "        self.rules = []\n",
    "    \n",
    "    def calculate_support(self, itemset, transactions):\n",
    "        \"\"\"Calculate support for an itemset.\"\"\"\n",
    "        count = 0\n",
    "        for transaction in transactions:\n",
    "            if set(itemset).issubset(set(transaction)):\n",
    "                count += 1\n",
    "        return count / len(transactions)\n",
    "    \n",
    "    def find_frequent_itemsets(self, transactions):\n",
    "        \"\"\"Find frequent itemsets using Apriori algorithm.\"\"\"\n",
    "        n_items = max([max(t) for t in transactions if t]) + 1\n",
    "        \n",
    "        # Find frequent 1-itemsets\n",
    "        frequent_1_itemsets = []\n",
    "        for item in range(n_items):\n",
    "            support = self.calculate_support([item], transactions)\n",
    "            if support >= self.min_support:\n",
    "                frequent_1_itemsets.append(([item], support))\n",
    "        \n",
    "        self.frequent_itemsets[1] = frequent_1_itemsets\n",
    "        print(f\"Found {len(frequent_1_itemsets)} frequent 1-itemsets\")\n",
    "        \n",
    "        # Find frequent k-itemsets for k > 1\n",
    "        k = 2\n",
    "        while True:\n",
    "            candidates = self.generate_candidates(k)\n",
    "            frequent_k_itemsets = []\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                support = self.calculate_support(candidate, transactions)\n",
    "                if support >= self.min_support:\n",
    "                    frequent_k_itemsets.append((candidate, support))\n",
    "            \n",
    "            if not frequent_k_itemsets:\n",
    "                break\n",
    "            \n",
    "            self.frequent_itemsets[k] = frequent_k_itemsets\n",
    "            print(f\"Found {len(frequent_k_itemsets)} frequent {k}-itemsets\")\n",
    "            k += 1\n",
    "            \n",
    "            if k > 4:  # Limit to prevent excessive computation\n",
    "                break\n",
    "    \n",
    "    def generate_candidates(self, k):\n",
    "        \"\"\"Generate candidate itemsets of size k.\"\"\"\n",
    "        if k not in self.frequent_itemsets or k-1 not in self.frequent_itemsets:\n",
    "            return []\n",
    "        \n",
    "        prev_itemsets = [itemset for itemset, _ in self.frequent_itemsets[k-1]]\n",
    "        candidates = []\n",
    "        \n",
    "        for i in range(len(prev_itemsets)):\n",
    "            for j in range(i+1, len(prev_itemsets)):\n",
    "                itemset1 = sorted(prev_itemsets[i])\n",
    "                itemset2 = sorted(prev_itemsets[j])\n",
    "                \n",
    "                # Join if first k-2 items are the same\n",
    "                if itemset1[:-1] == itemset2[:-1]:\n",
    "                    candidate = sorted(list(set(itemset1 + itemset2)))\n",
    "                    if len(candidate) == k:\n",
    "                        candidates.append(candidate)\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    def generate_rules(self, transactions):\n",
    "        \"\"\"Generate association rules from frequent itemsets.\"\"\"\n",
    "        self.rules = []\n",
    "        \n",
    "        for k in range(2, max(self.frequent_itemsets.keys()) + 1):\n",
    "            for itemset, support in self.frequent_itemsets[k]:\n",
    "                # Generate all possible rules from this itemset\n",
    "                for i in range(1, len(itemset)):\n",
    "                    for antecedent in self.combinations(itemset, i):\n",
    "                        consequent = [item for item in itemset if item not in antecedent]\n",
    "                        \n",
    "                        # Calculate confidence\n",
    "                        antecedent_support = self.calculate_support(antecedent, transactions)\n",
    "                        if antecedent_support > 0:\n",
    "                            confidence = support / antecedent_support\n",
    "                            \n",
    "                            if confidence >= self.min_confidence:\n",
    "                                # Calculate lift\n",
    "                                consequent_support = self.calculate_support(consequent, transactions)\n",
    "                                lift = confidence / consequent_support if consequent_support > 0 else 0\n",
    "                                \n",
    "                                self.rules.append({\n",
    "                                    'antecedent': antecedent,\n",
    "                                    'consequent': consequent,\n",
    "                                    'support': support,\n",
    "                                    'confidence': confidence,\n",
    "                                    'lift': lift\n",
    "                                })\n",
    "        \n",
    "        # Sort rules by confidence\n",
    "        self.rules.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        print(f\"Generated {len(self.rules)} association rules\")\n",
    "    \n",
    "    def combinations(self, items, r):\n",
    "        \"\"\"Generate all combinations of r items from items list.\"\"\"\n",
    "        from itertools import combinations\n",
    "        return list(combinations(items, r))\n",
    "    \n",
    "    def print_rules(self, n_rules=10):\n",
    "        \"\"\"Print top n association rules.\"\"\"\n",
    "        print(f\"\\nTop {min(n_rules, len(self.rules))} Association Rules:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Antecedent':<20} {'Consequent':<15} {'Support':<10} {'Confidence':<12} {'Lift':<8}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, rule in enumerate(self.rules[:n_rules]):\n",
    "            antecedent_str = ', '.join([item_names[item] for item in rule['antecedent']])\n",
    "            consequent_str = ', '.join([item_names[item] for item in rule['consequent']])\n",
    "            \n",
    "            print(f\"{antecedent_str:<20} {consequent_str:<15} {rule['support']:<10.3f} \"\n",
    "                  f\"{rule['confidence']:<12.3f} {rule['lift']:<8.3f}\")\n",
    "\n",
    "# Apply association rule mining\n",
    "miner = AssociationRuleMiner(min_support=0.05, min_confidence=0.4)\n",
    "\n",
    "print(\"\\n--- Finding Frequent Itemsets ---\")\n",
    "miner.find_frequent_itemsets(transactions)\n",
    "\n",
    "print(\"\\n--- Generating Association Rules ---\")\n",
    "miner.generate_rules(transactions)\n",
    "\n",
    "# Display results\n",
    "miner.print_rules(15)\n",
    "\n",
    "print(\"\\n✨ Association rule mining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ff035",
   "metadata": {},
   "source": [
    "### 6.2 Association Rules Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize association rule mining results\n",
    "print(\"📊 Visualizing Association Rule Mining Results...\")\n",
    "\n",
    "if miner.rules:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Support vs Confidence scatter plot\n",
    "    supports = [rule['support'] for rule in miner.rules]\n",
    "    confidences = [rule['confidence'] for rule in miner.rules]\n",
    "    lifts = [rule['lift'] for rule in miner.rules]\n",
    "    \n",
    "    scatter = axes[0, 0].scatter(supports, confidences, c=lifts, cmap='viridis', alpha=0.7, s=50)\n",
    "    axes[0, 0].set_xlabel('Support')\n",
    "    axes[0, 0].set_ylabel('Confidence')\n",
    "    axes[0, 0].set_title('Association Rules: Support vs Confidence')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=axes[0, 0])\n",
    "    cbar.set_label('Lift')\n",
    "    \n",
    "    # 2. Lift distribution\n",
    "    axes[0, 1].hist(lifts, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 1].axvline(np.mean(lifts), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(lifts):.2f}')\n",
    "    axes[0, 1].axvline(1.0, color='green', linestyle='--', \n",
    "                      label='Lift = 1 (Independence)')\n",
    "    axes[0, 1].set_xlabel('Lift')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Distribution of Lift Values')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Top rules visualization\n",
    "    top_rules = miner.rules[:10]\n",
    "    rule_labels = []\n",
    "    rule_confidences = []\n",
    "    \n",
    "    for rule in top_rules:\n",
    "        antecedent_str = ', '.join([item_names[item] for item in rule['antecedent']])\n",
    "        consequent_str = ', '.join([item_names[item] for item in rule['consequent']])\n",
    "        rule_label = f\"{antecedent_str} → {consequent_str}\"\n",
    "        \n",
    "        # Truncate long labels\n",
    "        if len(rule_label) > 25:\n",
    "            rule_label = rule_label[:22] + \"...\"\n",
    "        \n",
    "        rule_labels.append(rule_label)\n",
    "        rule_confidences.append(rule['confidence'])\n",
    "    \n",
    "    bars = axes[1, 0].barh(range(len(rule_labels)), rule_confidences, alpha=0.7, color='lightgreen')\n",
    "    axes[1, 0].set_yticks(range(len(rule_labels)))\n",
    "    axes[1, 0].set_yticklabels(rule_labels)\n",
    "    axes[1, 0].set_xlabel('Confidence')\n",
    "    axes[1, 0].set_title('Top 10 Association Rules by Confidence')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Item frequency analysis\n",
    "    item_frequencies = np.sum(transaction_matrix, axis=0)\n",
    "    item_indices = np.argsort(item_frequencies)[-15:]  # Top 15 items\n",
    "    \n",
    "    top_item_names = [item_names[i] for i in item_indices]\n",
    "    top_item_freqs = item_frequencies[item_indices]\n",
    "    \n",
    "    bars = axes[1, 1].bar(range(len(top_item_names)), top_item_freqs, alpha=0.7, color='lightcoral')\n",
    "    axes[1, 1].set_xticks(range(len(top_item_names)))\n",
    "    axes[1, 1].set_xticklabels(top_item_names, rotation=45, ha='right')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Top 15 Most Frequent Items')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_unsupervised_figure(fig, \"association_rule_mining_analysis\", \n",
    "                           \"Comprehensive analysis of association rule mining results\", \"association\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n📊 Association Rule Mining Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total transactions: {len(transactions)}\")\n",
    "    print(f\"Total items: {n_items}\")\n",
    "    print(f\"Average transaction size: {np.mean([len(t) for t in transactions]):.2f}\")\n",
    "    print(f\"Frequent itemsets found:\")\n",
    "    for k, itemsets in miner.frequent_itemsets.items():\n",
    "        print(f\"  {k}-itemsets: {len(itemsets)}\")\n",
    "    print(f\"Association rules generated: {len(miner.rules)}\")\n",
    "    print(f\"Average confidence: {np.mean(confidences):.3f}\")\n",
    "    print(f\"Average lift: {np.mean(lifts):.3f}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n✨ Association rule mining visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9764ceb8",
   "metadata": {},
   "source": [
    "## 7. Manifold Learning {#manifold}\n",
    "\n",
    "Explore advanced manifold learning techniques for non-linear dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate manifold datasets\n",
    "print(\"🌀 Generating Manifold Learning Datasets...\")\n",
    "\n",
    "def generate_swiss_roll_3d(n_samples=1000, noise=0.1):\n",
    "    \"\"\"Generate 3D Swiss Roll dataset.\"\"\"\n",
    "    t = 1.5 * np.pi * (1 + 2 * np.random.rand(n_samples))\n",
    "    height = 21 * np.random.rand(n_samples)\n",
    "    \n",
    "    X = np.zeros((n_samples, 3))\n",
    "    X[:, 0] = t * np.cos(t)\n",
    "    X[:, 1] = height\n",
    "    X[:, 2] = t * np.sin(t)\n",
    "    \n",
    "    X += noise * np.random.randn(n_samples, 3)\n",
    "    \n",
    "    return X, t\n",
    "\n",
    "def generate_s_curve_3d(n_samples=1000, noise=0.1):\n",
    "    \"\"\"Generate 3D S-curve dataset.\"\"\"\n",
    "    t = 3 * np.pi * (np.random.rand(n_samples) - 0.5)\n",
    "    height = 2 * np.random.rand(n_samples)\n",
    "    \n",
    "    X = np.zeros((n_samples, 3))\n",
    "    X[:, 0] = np.sin(t)\n",
    "    X[:, 1] = height\n",
    "    X[:, 2] = np.sign(t) * (np.cos(t) - 1)\n",
    "    \n",
    "    X += noise * np.random.randn(n_samples, 3)\n",
    "    \n",
    "    return X, t\n",
    "\n",
    "# Generate manifold datasets\n",
    "X_swiss, t_swiss = generate_swiss_roll_3d(n_samples=800, noise=0.1)\n",
    "X_scurve, t_scurve = generate_s_curve_3d(n_samples=800, noise=0.1)\n",
    "\n",
    "# Create a sphere dataset\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "X_sklearn_swiss, color_sklearn = make_swiss_roll(n_samples=800, noise=0.1, random_state=42)\n",
    "\n",
    "manifold_datasets = {\n",
    "    'Swiss Roll': (X_swiss, t_swiss),\n",
    "    'S-Curve': (X_scurve, t_scurve),\n",
    "    'sklearn Swiss Roll': (X_sklearn_swiss, color_sklearn)\n",
    "}\n",
    "\n",
    "print(f\"Generated {len(manifold_datasets)} manifold datasets\")\n",
    "\n",
    "# Visualize 3D manifold datasets\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "for i, (name, (X, color)) in enumerate(manifold_datasets.items()):\n",
    "    ax = fig.add_subplot(1, 3, i+1, projection='3d')\n",
    "    \n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap='viridis', alpha=0.7)\n",
    "    ax.set_title(f'{name} (3D)')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    \n",
    "    plt.colorbar(scatter, ax=ax, shrink=0.8)\n",
    "\n",
    "plt.suptitle('3D Manifold Datasets', fontsize=16)\n",
    "plt.tight_layout()\n",
    "save_unsupervised_figure(fig, \"manifold_datasets_3d\", \n",
    "                       \"3D visualization of manifold learning datasets\", \"manifold\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✨ Manifold datasets generated and visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4333d058",
   "metadata": {},
   "source": [
    "### 7.1 Manifold Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test manifold learning algorithms\n",
    "print(\"🔄 Testing Manifold Learning Algorithms...\")\n",
    "\n",
    "# Initialize manifold learning algorithms\n",
    "manifold_algorithms = {\n",
    "    'Isomap': Isomap(n_components=2, n_neighbors=10),\n",
    "    'Locally Linear Embedding': LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42),\n",
    "    't-SNE': TSNE(n_components=2, random_state=42, perplexity=30),\n",
    "    'PCA': PCA(n_components=2, random_state=42)\n",
    "}\n",
    "\n",
    "# Add spectral embedding if available\n",
    "try:\n",
    "    from sklearn.manifold import SpectralEmbedding\n",
    "    manifold_algorithms['Spectral Embedding'] = SpectralEmbedding(n_components=2, random_state=42)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Add MDS if available\n",
    "try:\n",
    "    from sklearn.manifold import MDS\n",
    "    manifold_algorithms['MDS'] = MDS(n_components=2, random_state=42)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def calculate_trustworthiness(X_original, X_embedded, n_neighbors=5):\n",
    "    \"\"\"Calculate trustworthiness score for manifold learning.\"\"\"\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    \n",
    "    # Find neighbors in original space\n",
    "    nbrs_original = NearestNeighbors(n_neighbors=n_neighbors + 1).fit(X_original)\n",
    "    _, indices_original = nbrs_original.kneighbors(X_original)\n",
    "    \n",
    "    # Find neighbors in embedded space\n",
    "    nbrs_embedded = NearestNeighbors(n_neighbors=n_neighbors + 1).fit(X_embedded)\n",
    "    _, indices_embedded = nbrs_embedded.kneighbors(X_embedded)\n",
    "    \n",
    "    # Calculate trustworthiness\n",
    "    n_samples = X_original.shape[0]\n",
    "    trustworthiness = 0.0\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Get k nearest neighbors (excluding the point itself)\n",
    "        original_neighbors = set(indices_original[i, 1:])\n",
    "        embedded_neighbors = set(indices_embedded[i, 1:])\n",
    "        \n",
    "        # Count how many embedded neighbors are also original neighbors\n",
    "        intersection = len(original_neighbors.intersection(embedded_neighbors))\n",
    "        trustworthiness += intersection / n_neighbors\n",
    "    \n",
    "    return trustworthiness / n_samples\n",
    "\n",
    "manifold_results = {}\n",
    "\n",
    "for dataset_name, (X, color) in manifold_datasets.items():\n",
    "    print(f\"\\n--- Testing on {dataset_name} ---\")\n",
    "    manifold_results[dataset_name] = {}\n",
    "    \n",
    "    # Standardize the data\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    for algo_name, algorithm in manifold_algorithms.items():\n",
    "        try:\n",
    "            print(f\"  Running {algo_name}...\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            X_embedded = algorithm.fit_transform(X_scaled)\n",
    "            embedding_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate trustworthiness\n",
    "            trustworthiness = calculate_trustworthiness(X_scaled, X_embedded, n_neighbors=5)\n",
    "            \n",
    "            manifold_results[dataset_name][algo_name] = {\n",
    "                'X_embedded': X_embedded,\n",
    "                'embedding_time': embedding_time,\n",
    "                'trustworthiness': trustworthiness,\n",
    "                'color': color\n",
    "            }\n",
    "            \n",
    "            print(f\"    Time: {embedding_time:.3f}s, Trustworthiness: {trustworthiness:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ❌ Failed: {str(e)}\")\n",
    "            manifold_results[dataset_name][algo_name] = {'error': str(e)}\n",
    "\n",
    "print(\"\\n✨ Manifold learning algorithms tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f2417",
   "metadata": {},
   "source": [
    "### 7.2 Manifold Learning Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad51d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize manifold learning results\n",
    "print(\"📊 Visualizing Manifold Learning Results...\")\n",
    "\n",
    "for dataset_name, results in manifold_results.items():\n",
    "    print(f\"\\nVisualizing results for {dataset_name}...\")\n",
    "    \n",
    "    # Count successful algorithms\n",
    "    successful_algos = [(algo, result) for algo, result in results.items() \n",
    "                       if 'error' not in result]\n",
    "    \n",
    "    if successful_algos:\n",
    "        n_algos = len(successful_algos)\n",
    "        n_cols = min(3, n_algos)\n",
    "        n_rows = (n_algos + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1) if n_algos > 1 else [axes]\n",
    "        axes = axes.ravel() if n_algos > 1 else [axes[0]]\n",
    "        \n",
    "        for i, (algo_name, result) in enumerate(successful_algos):\n",
    "            X_embedded = result['X_embedded']\n",
    "            color = result['color']\n",
    "            \n",
    "            scatter = axes[i].scatter(X_embedded[:, 0], X_embedded[:, 1], \n",
    "                                    c=color, cmap='viridis', alpha=0.7)\n",
    "            \n",
    "            # Title with metrics\n",
    "            trustworthiness = result['trustworthiness']\n",
    "            embedding_time = result['embedding_time']\n",
    "            \n",
    "            axes[i].set_title(f'{algo_name}\\nTrustworthiness: {trustworthiness:.3f}\\nTime: {embedding_time:.2f}s')\n",
    "            axes[i].set_xlabel('Component 1')\n",
    "            axes[i].set_ylabel('Component 2')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.colorbar(scatter, ax=axes[i])\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(successful_algos), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle(f'Manifold Learning Results - {dataset_name}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        save_unsupervised_figure(fig, f\"manifold_learning_results_{dataset_name.lower().replace(' ', '_')}\", \n",
    "                               f\"Manifold learning algorithm comparison on {dataset_name} dataset\", \"manifold\")\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n✨ Manifold learning visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9291e76c",
   "metadata": {},
   "source": [
    "### 7.3 Manifold Learning Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive manifold learning performance analysis\n",
    "print(\"📈 Manifold Learning Performance Analysis...\")\n",
    "\n",
    "# Create performance comparison\n",
    "performance_data = []\n",
    "\n",
    "for dataset_name in manifold_results.keys():\n",
    "    for algo_name, result in manifold_results[dataset_name].items():\n",
    "        if 'error' not in result:\n",
    "            performance_data.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Algorithm': algo_name,\n",
    "                'Trustworthiness': result['trustworthiness'],\n",
    "                'Embedding_Time': result['embedding_time']\n",
    "            })\n",
    "\n",
    "if performance_data:\n",
    "    performance_df = pd.DataFrame(performance_data)\n",
    "    \n",
    "    print(\"\\n📊 Manifold Learning Performance Summary:\")\n",
    "    print(performance_df.round(3).to_string(index=False))\n",
    "    \n",
    "    # Create performance visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Trustworthiness comparison\n",
    "    sns.barplot(data=performance_df, x='Algorithm', y='Trustworthiness', \n",
    "               hue='Dataset', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Trustworthiness by Algorithm and Dataset')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Embedding time comparison\n",
    "    sns.barplot(data=performance_df, x='Algorithm', y='Embedding_Time', \n",
    "               hue='Dataset', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Embedding Time by Algorithm and Dataset')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].set_ylabel('Time (seconds)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Trustworthiness vs Time scatter\n",
    "    algorithms = performance_df['Algorithm'].unique()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(algorithms)))\n",
    "    \n",
    "    for i, algo in enumerate(algorithms):\n",
    "        algo_data = performance_df[performance_df['Algorithm'] == algo]\n",
    "        axes[1, 0].scatter(algo_data['Embedding_Time'], algo_data['Trustworthiness'], \n",
    "                          color=colors[i], label=algo, s=100, alpha=0.7)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Embedding Time (seconds)')\n",
    "    axes[1, 0].set_ylabel('Trustworthiness')\n",
    "    axes[1, 0].set_title('Trustworthiness vs Embedding Time')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Average performance by algorithm\n",
    "    avg_performance = performance_df.groupby('Algorithm').agg({\n",
    "        'Trustworthiness': 'mean',\n",
    "        'Embedding_Time': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Normalize both metrics to 0-1 scale for comparison\n",
    "    avg_performance['Trustworthiness_norm'] = (avg_performance['Trustworthiness'] - \n",
    "                                              avg_performance['Trustworthiness'].min()) / \\\n",
    "                                             (avg_performance['Trustworthiness'].max() - \n",
    "                                              avg_performance['Trustworthiness'].min())\n",
    "    \n",
    "    avg_performance['Speed_norm'] = 1 - ((avg_performance['Embedding_Time'] - \n",
    "                                         avg_performance['Embedding_Time'].min()) / \\\n",
    "                                        (avg_performance['Embedding_Time'].max() - \n",
    "                                         avg_performance['Embedding_Time'].min()))\n",
    "    \n",
    "    # Combined score (quality + speed)\n",
    "    avg_performance['Combined_Score'] = (avg_performance['Trustworthiness_norm'] + \n",
    "                                        avg_performance['Speed_norm']) / 2\n",
    "    \n",
    "    bars = axes[1, 1].bar(range(len(avg_performance)), avg_performance['Combined_Score'], \n",
    "                         color='lightblue', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Algorithm')\n",
    "    axes[1, 1].set_ylabel('Combined Score (Quality + Speed)')\n",
    "    axes[1, 1].set_title('Overall Algorithm Performance')\n",
    "    axes[1, 1].set_xticks(range(len(avg_performance)))\n",
    "    axes[1, 1].set_xticklabels(avg_performance['Algorithm'], rotation=45, ha='right')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                       f'{avg_performance.iloc[i][\"Combined_Score\"]:.3f}',\n",
    "                       ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_unsupervised_figure(fig, \"manifold_learning_performance_analysis\", \n",
    "                           \"Comprehensive manifold learning performance metrics\", \"manifold\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Best algorithm per dataset\n",
    "    print(\"\\n🏆 Best Algorithm per Dataset (by Trustworthiness):\")\n",
    "    for dataset in performance_df['Dataset'].unique():\n",
    "        dataset_data = performance_df[performance_df['Dataset'] == dataset]\n",
    "        if not dataset_data.empty:\n",
    "            best_algo = dataset_data.loc[dataset_data['Trustworthiness'].idxmax()]\n",
    "            print(f\"  {dataset}: {best_algo['Algorithm']} (Trustworthiness: {best_algo['Trustworthiness']:.3f})\")\n",
    "\n",
    "print(\"\\n✨ Manifold learning performance analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd746abc",
   "metadata": {},
   "source": [
    "## 8. Cluster Validation and Evaluation {#validation}\n",
    "\n",
    "Comprehensive cluster validation and evaluation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bd750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive cluster validation\n",
    "print(\"✅ Comprehensive Cluster Validation and Evaluation...\")\n",
    "\n",
    "class ClusterValidator:\n",
    "    \"\"\"Comprehensive cluster validation toolkit.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_results = {}\n",
    "    \n",
    "    def internal_validation(self, X, labels, algorithm_name=\"Unknown\"):\n",
    "        \"\"\"Internal validation metrics (no ground truth required).\"\"\"\n",
    "        \n",
    "        # Remove noise points for validation\n",
    "        valid_mask = labels >= 0\n",
    "        X_valid = X[valid_mask]\n",
    "        labels_valid = labels[valid_mask]\n",
    "        \n",
    "        if len(np.unique(labels_valid)) < 2:\n",
    "            return {\n",
    "                'silhouette_score': -1,\n",
    "                'calinski_harabasz_score': -1,\n",
    "                'davies_bouldin_score': -1,\n",
    "                'error': 'Less than 2 clusters found'\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Silhouette Score\n",
    "            sil_score = silhouette_score(X_valid, labels_valid)\n",
    "            \n",
    "            # Calinski-Harabasz Index\n",
    "            ch_score = calinski_harabasz_score(X_valid, labels_valid)\n",
    "            \n",
    "            # Davies-Bouldin Index\n",
    "            from sklearn.metrics import davies_bouldin_score\n",
    "            db_score = davies_bouldin_score(X_valid, labels_valid)\n",
    "            \n",
    "            return {\n",
    "                'silhouette_score': sil_score,\n",
    "                'calinski_harabasz_score': ch_score,\n",
    "                'davies_bouldin_score': db_score,\n",
    "                'n_clusters': len(np.unique(labels_valid)),\n",
    "                'n_noise_points': np.sum(labels == -1)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def external_validation(self, y_true, y_pred):\n",
    "        \"\"\"External validation metrics (requires ground truth).\"\"\"\n",
    "        try:\n",
    "            from sklearn.metrics import (adjusted_rand_score, adjusted_mutual_info_score, \n",
    "                                       homogeneity_score, completeness_score, v_measure_score)\n",
    "            \n",
    "            # Adjusted Rand Index\n",
    "            ari = adjusted_rand_score(y_true, y_pred)\n",
    "            \n",
    "            # Adjusted Mutual Information\n",
    "            ami = adjusted_mutual_info_score(y_true, y_pred)\n",
    "            \n",
    "            # Homogeneity, Completeness, V-measure\n",
    "            homogeneity = homogeneity_score(y_true, y_pred)\n",
    "            completeness = completeness_score(y_true, y_pred)\n",
    "            v_measure = v_measure_score(y_true, y_pred)\n",
    "            \n",
    "            return {\n",
    "                'adjusted_rand_index': ari,\n",
    "                'adjusted_mutual_info': ami,\n",
    "                'homogeneity': homogeneity,\n",
    "                'completeness': completeness,\n",
    "                'v_measure': v_measure\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def stability_analysis(self, X, algorithm, n_runs=10, sample_ratio=0.8):\n",
    "        \"\"\"Analyze clustering stability through bootstrap sampling.\"\"\"\n",
    "        print(f\"  Performing stability analysis with {n_runs} runs...\")\n",
    "        \n",
    "        stability_scores = []\n",
    "        cluster_counts = []\n",
    "        \n",
    "        n_samples = int(len(X) * sample_ratio)\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            # Bootstrap sample\n",
    "            indices = np.random.choice(len(X), size=n_samples, replace=True)\n",
    "            X_sample = X[indices]\n",
    "            \n",
    "            try:\n",
    "                # Fit algorithm\n",
    "                if hasattr(algorithm, 'fit_predict'):\n",
    "                    labels = algorithm.fit_predict(X_sample)\n",
    "                else:\n",
    "                    algorithm.fit(X_sample)\n",
    "                    labels = algorithm.predict(X_sample) if hasattr(algorithm, 'predict') else algorithm.labels_\n",
    "                \n",
    "                # Calculate internal validation\n",
    "                validation = self.internal_validation(X_sample, labels)\n",
    "                \n",
    "                if 'silhouette_score' in validation and validation['silhouette_score'] > -1:\n",
    "                    stability_scores.append(validation['silhouette_score'])\n",
    "                    cluster_counts.append(validation['n_clusters'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if stability_scores:\n",
    "            return {\n",
    "                'mean_silhouette': np.mean(stability_scores),\n",
    "                'std_silhouette': np.std(stability_scores),\n",
    "                'mean_clusters': np.mean(cluster_counts),\n",
    "                'std_clusters': np.std(cluster_counts),\n",
    "                'stability_coefficient': 1 - (np.std(stability_scores) / np.mean(stability_scores)) if np.mean(stability_scores) > 0 else 0\n",
    "            }\n",
    "        else:\n",
    "            return {'error': 'No valid runs completed'}\n",
    "    \n",
    "    def elbow_analysis(self, X, algorithm_class, max_clusters=10, **kwargs):\n",
    "        \"\"\"Perform elbow analysis for optimal cluster number.\"\"\"\n",
    "        print(f\"  Performing elbow analysis up to {max_clusters} clusters...\")\n",
    "        \n",
    "        inertias = []\n",
    "        silhouette_scores = []\n",
    "        cluster_range = range(2, max_clusters + 1)\n",
    "        \n",
    "        for n_clusters in cluster_range:\n",
    "            try:\n",
    "                # Create algorithm instance\n",
    "                if 'n_clusters' in algorithm_class().get_params():\n",
    "                    alg = algorithm_class(n_clusters=n_clusters, **kwargs)\n",
    "                elif 'n_components' in algorithm_class().get_params():\n",
    "                    alg = algorithm_class(n_components=n_clusters, **kwargs)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Fit and predict\n",
    "                labels = alg.fit_predict(X)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                if hasattr(alg, 'inertia_'):\n",
    "                    inertias.append(alg.inertia_)\n",
    "                else:\n",
    "                    inertias.append(None)\n",
    "                \n",
    "                # Calculate silhouette score\n",
    "                if len(np.unique(labels)) > 1:\n",
    "                    sil_score = silhouette_score(X, labels)\n",
    "                    silhouette_scores.append(sil_score)\n",
    "                else:\n",
    "                    silhouette_scores.append(-1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                inertias.append(None)\n",
    "                silhouette_scores.append(-1)\n",
    "        \n",
    "        return {\n",
    "            'cluster_range': list(cluster_range),\n",
    "            'inertias': inertias,\n",
    "            'silhouette_scores': silhouette_scores\n",
    "        }\n",
    "\n",
    "# Test cluster validation methods\n",
    "validator = ClusterValidator()\n",
    "\n",
    "# Comprehensive validation of clustering results\n",
    "validation_summary = {}\n",
    "\n",
    "print(\"\\n--- Comprehensive Cluster Validation ---\")\n",
    "\n",
    "for dataset_name, (X, y_true) in test_datasets.items():\n",
    "    print(f\"\\n  Validating clustering results for {dataset_name}...\")\n",
    "    validation_summary[dataset_name] = {}\n",
    "    \n",
    "    for algo_name, result in clustering_results[dataset_name].items():\n",
    "        if 'error' not in result:\n",
    "            print(f\"    Validating {algo_name}...\")\n",
    "            \n",
    "            y_pred = result['y_pred']\n",
    "            \n",
    "            # Internal validation\n",
    "            internal_val = validator.internal_validation(X, y_pred, algo_name)\n",
    "            \n",
    "            # External validation\n",
    "            external_val = validator.external_validation(y_true, y_pred)\n",
    "            \n",
    "            # Combine validation results\n",
    "            validation_summary[dataset_name][algo_name] = {\n",
    "                'internal': internal_val,\n",
    "                'external': external_val\n",
    "            }\n",
    "            \n",
    "            # Print summary\n",
    "            if 'error' not in internal_val:\n",
    "                print(f\"      Silhouette: {internal_val['silhouette_score']:.3f}\")\n",
    "                print(f\"      ARI: {external_val['adjusted_rand_index']:.3f}\")\n",
    "\n",
    "# Stability analysis for selected algorithms\n",
    "print(\"\\n--- Stability Analysis ---\")\n",
    "\n",
    "stability_results = {}\n",
    "\n",
    "# Test stability on blob dataset with K-Means\n",
    "if 'K-Means' in clustering_algorithms:\n",
    "    print(\"  Testing K-Means stability on Blob Clusters...\")\n",
    "    kmeans_stability = validator.stability_analysis(\n",
    "        X_blobs, clustering_algorithms['K-Means'], n_runs=15\n",
    "    )\n",
    "    stability_results['K-Means_Blobs'] = kmeans_stability\n",
    "    \n",
    "    if 'error' not in kmeans_stability:\n",
    "        print(f\"    Stability coefficient: {kmeans_stability['stability_coefficient']:.3f}\")\n",
    "\n",
    "# Elbow analysis\n",
    "print(\"\\n--- Elbow Analysis ---\")\n",
    "\n",
    "elbow_results = {}\n",
    "\n",
    "# Elbow analysis for K-Means on blob dataset\n",
    "print(\"  K-Means elbow analysis on Blob Clusters...\")\n",
    "kmeans_elbow = validator.elbow_analysis(\n",
    "    X_blobs, KMeans, max_clusters=8, random_state=42\n",
    ")\n",
    "elbow_results['K-Means_Blobs'] = kmeans_elbow\n",
    "\n",
    "print(\"\\n✨ Cluster validation and evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40086a76",
   "metadata": {},
   "source": [
    "### 8.1 Cluster Validation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster validation results\n",
    "print(\"📊 Visualizing Cluster Validation Results...\")\n",
    "\n",
    "# Create comprehensive validation visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Internal vs External validation scatter plot\n",
    "internal_scores = []\n",
    "external_scores = []\n",
    "algorithm_names = []\n",
    "dataset_names = []\n",
    "\n",
    "for dataset_name, algorithms in validation_summary.items():\n",
    "    for algo_name, validation in algorithms.items():\n",
    "        if ('error' not in validation['internal'] and \n",
    "            'error' not in validation['external']):\n",
    "            \n",
    "            internal_scores.append(validation['internal']['silhouette_score'])\n",
    "            external_scores.append(validation['external']['adjusted_rand_index'])\n",
    "            algorithm_names.append(algo_name)\n",
    "            dataset_names.append(dataset_name)\n",
    "\n",
    "if internal_scores and external_scores:\n",
    "    # Create scatter plot with different colors for different datasets\n",
    "    datasets_unique = list(set(dataset_names))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(datasets_unique)))\n",
    "    \n",
    "    for i, dataset in enumerate(datasets_unique):\n",
    "        mask = [d == dataset for d in dataset_names]\n",
    "        x_vals = [internal_scores[j] for j, m in enumerate(mask) if m]\n",
    "        y_vals = [external_scores[j] for j, m in enumerate(mask) if m]\n",
    "        \n",
    "        axes[0, 0].scatter(x_vals, y_vals, color=colors[i], label=dataset, s=100, alpha=0.7)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Silhouette Score (Internal)')\n",
    "    axes[0, 0].set_ylabel('Adjusted Rand Index (External)')\n",
    "    axes[0, 0].set_title('Internal vs External Validation')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Validation metrics heatmap\n",
    "if validation_summary:\n",
    "    # Create heatmap data\n",
    "    metrics = ['silhouette_score', 'adjusted_rand_index', 'v_measure']\n",
    "    metric_labels = ['Silhouette Score', 'Adjusted Rand Index', 'V-Measure']\n",
    "    \n",
    "    heatmap_data = []\n",
    "    row_labels = []\n",
    "    \n",
    "    for dataset_name, algorithms in validation_summary.items():\n",
    "        for algo_name, validation in algorithms.items():\n",
    "            if ('error' not in validation['internal'] and \n",
    "                'error' not in validation['external']):\n",
    "                \n",
    "                row_data = [\n",
    "                    validation['internal']['silhouette_score'],\n",
    "                    validation['external']['adjusted_rand_index'],\n",
    "                    validation['external']['v_measure']\n",
    "                ]\n",
    "                heatmap_data.append(row_data)\n",
    "                row_labels.append(f\"{dataset_name}_{algo_name}\")\n",
    "    \n",
    "    if heatmap_data:\n",
    "        heatmap_array = np.array(heatmap_data)\n",
    "        \n",
    "        im = axes[0, 1].imshow(heatmap_array, cmap='RdYlBu_r', aspect='auto')\n",
    "        axes[0, 1].set_xticks(range(len(metric_labels)))\n",
    "        axes[0, 1].set_xticklabels(metric_labels, rotation=45, ha='right')\n",
    "        axes[0, 1].set_yticks(range(len(row_labels)))\n",
    "        axes[0, 1].set_yticklabels([label.replace('_', '\\n') for label in row_labels])\n",
    "        axes[0, 1].set_title('Validation Metrics Heatmap')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=axes[0, 1])\n",
    "        cbar.set_label('Score')\n",
    "\n",
    "# 3. Elbow curve\n",
    "if 'K-Means_Blobs' in elbow_results:\n",
    "    elbow_data = elbow_results['K-Means_Blobs']\n",
    "    cluster_range = elbow_data['cluster_range']\n",
    "    silhouette_scores = elbow_data['silhouette_scores']\n",
    "    \n",
    "    # Plot silhouette scores\n",
    "    valid_scores = [(k, s) for k, s in zip(cluster_range, silhouette_scores) if s > -1]\n",
    "    if valid_scores:\n",
    "        k_vals, sil_vals = zip(*valid_scores)\n",
    "        axes[1, 0].plot(k_vals, sil_vals, 'bo-', linewidth=2, markersize=8)\n",
    "        axes[1, 0].set_xlabel('Number of Clusters')\n",
    "        axes[1, 0].set_ylabel('Silhouette Score')\n",
    "        axes[1, 0].set_title('Elbow Analysis: Silhouette Score')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Mark optimal point\n",
    "        best_k = k_vals[np.argmax(sil_vals)]\n",
    "        best_score = max(sil_vals)\n",
    "        axes[1, 0].scatter([best_k], [best_score], color='red', s=200, marker='*', \n",
    "                          label=f'Optimal: k={best_k}')\n",
    "        axes[1, 0].legend()\n",
    "\n",
    "# 4. Stability analysis results\n",
    "if stability_results:\n",
    "    stability_names = []\n",
    "    stability_coeffs = []\n",
    "    \n",
    "    for name, result in stability_results.items():\n",
    "        if 'error' not in result:\n",
    "            stability_names.append(name.replace('_', '\\n'))\n",
    "            stability_coeffs.append(result['stability_coefficient'])\n",
    "    \n",
    "    if stability_coeffs:\n",
    "        bars = axes[1, 1].bar(range(len(stability_names)), stability_coeffs, \n",
    "                             color='lightgreen', alpha=0.7)\n",
    "        axes[1, 1].set_xlabel('Algorithm_Dataset')\n",
    "        axes[1, 1].set_ylabel('Stability Coefficient')\n",
    "        axes[1, 1].set_title('Clustering Stability Analysis')\n",
    "        axes[1, 1].set_xticks(range(len(stability_names)))\n",
    "        axes[1, 1].set_xticklabels(stability_names)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, coeff in zip(bars, stability_coeffs):\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                           f'{coeff:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_unsupervised_figure(fig, \"cluster_validation_comprehensive\", \n",
    "                       \"Comprehensive cluster validation analysis\", \"validation\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✨ Cluster validation visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6bc67",
   "metadata": {},
   "source": [
    "## 9. Unsupervised Feature Learning {#feature-learning}\n",
    "\n",
    "Explore advanced unsupervised feature learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcce016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised feature learning\n",
    "print(\"🎓 Unsupervised Feature Learning...\")\n",
    "\n",
    "class UnsupervisedFeatureLearner:\n",
    "    \"\"\"Comprehensive unsupervised feature learning toolkit.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_transformers = {}\n",
    "        self.learned_features = {}\n",
    "    \n",
    "    def dictionary_learning(self, X, n_components=10, alpha=1.0):\n",
    "        \"\"\"Dictionary learning for sparse coding.\"\"\"\n",
    "        print(\"  Learning dictionary for sparse coding...\")\n",
    "        \n",
    "        try:\n",
    "            dict_learner = DictionaryLearning(\n",
    "                n_components=n_components,\n",
    "                alpha=alpha,\n",
    "                random_state=42,\n",
    "                max_iter=100\n",
    "            )\n",
    "            \n",
    "            sparse_codes = dict_learner.fit_transform(X)\n",
    "            \n",
    "            # Calculate sparsity\n",
    "            sparsity = np.mean(sparse_codes == 0)\n",
    "            \n",
    "            self.feature_transformers['dictionary'] = dict_learner\n",
    "            \n",
    "            return {\n",
    "                'features': sparse_codes,\n",
    "                'dictionary': dict_learner.components_,\n",
    "                'sparsity': sparsity,\n",
    "                'reconstruction_error': np.mean((X - sparse_codes @ dict_learner.components_) ** 2)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Dictionary learning failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def independent_component_analysis(self, X, n_components=10):\n",
    "        \"\"\"Independent Component Analysis for feature learning.\"\"\"\n",
    "        print(\"  Performing Independent Component Analysis...\")\n",
    "        \n",
    "        try:\n",
    "            ica = FastICA(\n",
    "                n_components=n_components,\n",
    "                random_state=42,\n",
    "                max_iter=200\n",
    "            )\n",
    "            \n",
    "            ica_features = ica.fit_transform(X)\n",
    "            \n",
    "            # Calculate independence score (simplified)\n",
    "            independence_score = np.mean(np.abs(np.corrcoef(ica_features.T)))\n",
    "            \n",
    "            self.feature_transformers['ica'] = ica\n",
    "            \n",
    "            return {\n",
    "                'features': ica_features,\n",
    "                'mixing_matrix': ica.mixing_,\n",
    "                'components': ica.components_,\n",
    "                'independence_score': independence_score\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ICA failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def factor_analysis(self, X, n_components=10):\n",
    "        \"\"\"Factor Analysis for dimensionality reduction.\"\"\"\n",
    "        print(\"  Performing Factor Analysis...\")\n",
    "        \n",
    "        try:\n",
    "            fa = FactorAnalysis(\n",
    "                n_components=n_components,\n",
    "                random_state=42,\n",
    "                max_iter=100\n",
    "            )\n",
    "            \n",
    "            fa_features = fa.fit_transform(X)\n",
    "            \n",
    "            self.feature_transformers['factor_analysis'] = fa\n",
    "            \n",
    "            return {\n",
    "                'features': fa_features,\n",
    "                'loadings': fa.components_,\n",
    "                'noise_variance': fa.noise_variance_,\n",
    "                'log_likelihood': fa.score(X)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Factor Analysis failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def non_negative_matrix_factorization(self, X, n_components=10):\n",
    "        \"\"\"Non-negative Matrix Factorization.\"\"\"\n",
    "        print(\"  Performing Non-negative Matrix Factorization...\")\n",
    "        \n",
    "        try:\n",
    "            # Ensure non-negative data\n",
    "            X_nonneg = X - X.min() + 1e-10\n",
    "            \n",
    "            nmf = NMF(\n",
    "                n_components=n_components,\n",
    "                random_state=42,\n",
    "                max_iter=200\n",
    "            )\n",
    "            \n",
    "            nmf_features = nmf.fit_transform(X_nonneg)\n",
    "            \n",
    "            # Calculate reconstruction error\n",
    "            reconstruction = nmf_features @ nmf.components_\n",
    "            reconstruction_error = np.mean((X_nonneg - reconstruction) ** 2)\n",
    "            \n",
    "            self.feature_transformers['nmf'] = nmf\n",
    "            \n",
    "            return {\n",
    "                'features': nmf_features,\n",
    "                'components': nmf.components_,\n",
    "                'reconstruction_error': reconstruction_error\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    NMF failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def autoencoder_features(self, X, encoding_dim=10):\n",
    "        \"\"\"Autoencoder-like feature extraction using PCA.\"\"\"\n",
    "        print(\"  Generating autoencoder-like features with PCA...\")\n",
    "        \n",
    "        try:\n",
    "            # Use PCA as a linear autoencoder\n",
    "            pca = PCA(n_components=encoding_dim, random_state=42)\n",
    "            encoded_features = pca.fit_transform(X)\n",
    "            \n",
    "            # Reconstruction\n",
    "            reconstructed = pca.inverse_transform(encoded_features)\n",
    "            reconstruction_error = np.mean((X - reconstructed) ** 2)\n",
    "            \n",
    "            self.feature_transformers['autoencoder'] = pca\n",
    "            \n",
    "            return {\n",
    "                'features': encoded_features,\n",
    "                'components': pca.components_,\n",
    "                'explained_variance_ratio': pca.explained_variance_ratio_,\n",
    "                'reconstruction_error': reconstruction_error\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Autoencoder features failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def learn_all_features(self, X, n_components=10):\n",
    "        \"\"\"Learn features using all available methods.\"\"\"\n",
    "        print(f\"Learning features with {n_components} components...\")\n",
    "        \n",
    "        # Standardize input data\n",
    "        X_scaled = StandardScaler().fit_transform(X)\n",
    "        \n",
    "        # Dictionary Learning\n",
    "        dict_result = self.dictionary_learning(X_scaled, n_components)\n",
    "        if dict_result:\n",
    "            self.learned_features['dictionary'] = dict_result\n",
    "        \n",
    "        # Independent Component Analysis\n",
    "        ica_result = self.independent_component_analysis(X_scaled, n_components)\n",
    "        if ica_result:\n",
    "            self.learned_features['ica'] = ica_result\n",
    "        \n",
    "        # Factor Analysis\n",
    "        fa_result = self.factor_analysis(X_scaled, n_components)\n",
    "        if fa_result:\n",
    "            self.learned_features['factor_analysis'] = fa_result\n",
    "        \n",
    "        # Non-negative Matrix Factorization\n",
    "        nmf_result = self.non_negative_matrix_factorization(X_scaled, n_components)\n",
    "        if nmf_result:\n",
    "            self.learned_features['nmf'] = nmf_result\n",
    "        \n",
    "        # Autoencoder-like features\n",
    "        ae_result = self.autoencoder_features(X_scaled, n_components)\n",
    "        if ae_result:\n",
    "            self.learned_features['autoencoder'] = ae_result\n",
    "        \n",
    "        print(f\"Successfully learned {len(self.learned_features)} feature representations\")\n",
    "        \n",
    "        return self.learned_features\n",
    "\n",
    "# Test unsupervised feature learning\n",
    "print(\"\\n--- Testing Unsupervised Feature Learning ---\")\n",
    "\n",
    "# Use high-dimensional dataset for feature learning\n",
    "feature_learner = UnsupervisedFeatureLearner()\n",
    "\n",
    "# Learn features\n",
    "learned_features = feature_learner.learn_all_features(X_highdim, n_components=8)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n📊 Feature Learning Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for method_name, result in learned_features.items():\n",
    "    print(f\"\\n{method_name.upper()}:\")\n",
    "    print(f\"  Feature shape: {result['features'].shape}\")\n",
    "    print(f\"  Feature variance: {np.var(result['features']):.3f}\")\n",
    "    \n",
    "    # Method-specific metrics\n",
    "    if method_name == 'dictionary':\n",
    "        print(f\"  Sparsity: {result['sparsity']:.3f}\")\n",
    "        print(f\"  Reconstruction error: {result['reconstruction_error']:.3f}\")\n",
    "    elif method_name == 'ica':\n",
    "        print(f\"  Independence score: {result['independence_score']:.3f}\")\n",
    "    elif method_name == 'factor_analysis':\n",
    "        print(f\"  Log-likelihood: {result['log_likelihood']:.1f}\")\n",
    "        print(f\"  Mean noise variance: {np.mean(result['noise_variance']):.3f}\")\n",
    "    elif method_name == 'nmf':\n",
    "        print(f\"  Reconstruction error: {result['reconstruction_error']:.3f}\")\n",
    "    elif method_name == 'autoencoder':\n",
    "        print(f\"  Explained variance: {np.sum(result['explained_variance_ratio']):.3f}\")\n",
    "        print(f\"  Reconstruction error: {result['reconstruction_error']:.3f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✨ Unsupervised feature learning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cee9d3",
   "metadata": {},
   "source": [
    "### 9.1 Feature Learning Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13af4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize unsupervised feature learning results\n",
    "print(\"📊 Visualizing Feature Learning Results...\")\n",
    "\n",
    "if learned_features:\n",
    "    n_methods = len(learned_features)\n",
    "    \n",
    "    # 1. Feature comparison visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Visualize learned features (first 2 components) colored by original clusters\n",
    "    for i, (method_name, result) in enumerate(learned_features.items()):\n",
    "        if i < len(axes):\n",
    "            features = result['features']\n",
    "            \n",
    "            if features.shape[1] >= 2:\n",
    "                scatter = axes[i].scatter(features[:, 0], features[:, 1], \n",
    "                                        c=y_highdim, cmap='viridis', alpha=0.7)\n",
    "                axes[i].set_title(f'{method_name.replace(\"_\", \" \").title()}\\nFeatures')\n",
    "                axes[i].set_xlabel(f'Component 1')\n",
    "                axes[i].set_ylabel(f'Component 2')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.colorbar(scatter, ax=axes[i])\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(learned_features), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Learned Feature Representations', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    save_unsupervised_figure(fig, \"feature_learning_representations\", \n",
    "                           \"Visualization of learned feature representations\", \"features\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Feature quality comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Feature variance comparison\n",
    "    methods = list(learned_features.keys())\n",
    "    variances = [np.var(learned_features[method]['features']) for method in methods]\n",
    "    \n",
    "    bars = axes[0, 0].bar(range(len(methods)), variances, alpha=0.7, color='lightblue')\n",
    "    axes[0, 0].set_xlabel('Method')\n",
    "    axes[0, 0].set_ylabel('Feature Variance')\n",
    "    axes[0, 0].set_title('Feature Variance by Method')\n",
    "    axes[0, 0].set_xticks(range(len(methods)))\n",
    "    axes[0, 0].set_xticklabels([m.replace('_', '\\n') for m in methods], rotation=45, ha='right')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, var in zip(bars, variances):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(variances)*0.01, \n",
    "                       f'{var:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Reconstruction error comparison (where available)\n",
    "    recon_methods = []\n",
    "    recon_errors = []\n",
    "    \n",
    "    for method, result in learned_features.items():\n",
    "        if 'reconstruction_error' in result:\n",
    "            recon_methods.append(method)\n",
    "            recon_errors.append(result['reconstruction_error'])\n",
    "    \n",
    "    if recon_errors:\n",
    "        bars = axes[0, 1].bar(range(len(recon_methods)), recon_errors, alpha=0.7, color='lightcoral')\n",
    "        axes[0, 1].set_xlabel('Method')\n",
    "        axes[0, 1].set_ylabel('Reconstruction Error')\n",
    "        axes[0, 1].set_title('Reconstruction Error by Method')\n",
    "        axes[0, 1].set_xticks(range(len(recon_methods)))\n",
    "        axes[0, 1].set_xticklabels([m.replace('_', '\\n') for m in recon_methods], rotation=45, ha='right')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature distribution analysis\n",
    "    if 'dictionary' in learned_features:\n",
    "        dict_features = learned_features['dictionary']['features']\n",
    "        axes[1, 0].hist(dict_features.flatten(), bins=50, alpha=0.7, color='lightgreen')\n",
    "        axes[1, 0].set_xlabel('Feature Value')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title('Dictionary Learning Feature Distribution')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Component visualization (for methods with interpretable components)\n",
    "    if 'ica' in learned_features:\n",
    "        ica_components = learned_features['ica']['components']\n",
    "        \n",
    "        # Show first few components\n",
    "        n_show = min(4, ica_components.shape[0])\n",
    "        for i in range(n_show):\n",
    "            axes[1, 1].plot(ica_components[i], alpha=0.7, label=f'Component {i+1}')\n",
    "        \n",
    "        axes[1, 1].set_xlabel('Feature Index')\n",
    "        axes[1, 1].set_ylabel('Component Weight')\n",
    "        axes[1, 1].set_title('ICA Components')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_unsupervised_figure(fig, \"feature_learning_analysis\", \n",
    "                           \"Comprehensive analysis of feature learning methods\", \"features\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Feature learning summary table\n",
    "    print(\"\\n📋 Feature Learning Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Method':<20} {'Components':<12} {'Variance':<10} {'Special Metric':<25}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for method, result in learned_features.items():\n",
    "        components = result['features'].shape[1]\n",
    "        variance = np.var(result['features'])\n",
    "        \n",
    "        # Special metric for each method\n",
    "        special_metric = \"\"\n",
    "        if method == 'dictionary':\n",
    "            special_metric = f\"Sparsity: {result['sparsity']:.3f}\"\n",
    "        elif method == 'ica':\n",
    "            special_metric = f\"Independence: {result['independence_score']:.3f}\"\n",
    "        elif method == 'factor_analysis':\n",
    "            special_metric = f\"Log-likelihood: {result['log_likelihood']:.1f}\"\n",
    "        elif method == 'nmf':\n",
    "            special_metric = f\"Recon. Error: {result['reconstruction_error']:.3f}\"\n",
    "        elif method == 'autoencoder':\n",
    "            special_metric = f\"Explained Var: {np.sum(result['explained_variance_ratio']):.3f}\"\n",
    "        \n",
    "        print(f\"{method:<20} {components:<12} {variance:<10.3f} {special_metric:<25}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✨ Feature learning visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6abbf9e",
   "metadata": {},
   "source": [
    "## 10. Save All Results {#save-results}\n",
    "\n",
    "Comprehensive saving of all unsupervised learning results, models, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis saving functions\n",
    "def save_clustering_analysis():\n",
    "    \"\"\"Save all clustering analysis figures and results.\"\"\"\n",
    "    print(\"💾 Saving clustering analysis...\")\n",
    "    \n",
    "    # Save clustering results for each dataset\n",
    "    if 'clustering_results' in globals():\n",
    "        for dataset_name, (X, y_true) in test_datasets.items():\n",
    "            if X.shape[1] == 2:  # Only visualize 2D datasets\n",
    "                successful_algos = [algo for algo, result in clustering_results[dataset_name].items() \n",
    "                                   if 'error' not in result]\n",
    "                \n",
    "                if successful_algos:\n",
    "                    n_algos = len(successful_algos)\n",
    "                    n_cols = min(3, n_algos)\n",
    "                    n_rows = (n_algos + n_cols - 1) // n_cols\n",
    "                    \n",
    "                    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "                    if n_rows == 1:\n",
    "                        axes = axes.reshape(1, -1) if n_algos > 1 else [axes]\n",
    "                    axes = axes.ravel() if n_algos > 1 else [axes[0]]\n",
    "                    \n",
    "                    for i, algo_name in enumerate(successful_algos):\n",
    "                        result = clustering_results[dataset_name][algo_name]\n",
    "                        y_pred = result['y_pred']\n",
    "                        \n",
    "                        scatter = axes[i].scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', alpha=0.7)\n",
    "                        \n",
    "                        # Highlight noise points if any\n",
    "                        noise_mask = y_pred == -1\n",
    "                        if np.any(noise_mask):\n",
    "                            axes[i].scatter(X[noise_mask, 0], X[noise_mask, 1], \n",
    "                                          c='red', marker='x', s=50, label='Noise')\n",
    "                            axes[i].legend()\n",
    "                        \n",
    "                        plt.colorbar(scatter, ax=axes[i])\n",
    "                        \n",
    "                        # Title with metrics\n",
    "                        sil_score = result.get('silhouette_score', -1)\n",
    "                        ari_score = result.get('adjusted_rand_index', -1)\n",
    "                        n_clusters = result.get('n_clusters', 0)\n",
    "                        \n",
    "                        title = f\"{algo_name}\\nClusters: {n_clusters}, ARI: {ari_score:.3f}\"\n",
    "                        if sil_score > -1:\n",
    "                            title += f\", Sil: {sil_score:.3f}\"\n",
    "                        \n",
    "                        axes[i].set_title(title)\n",
    "                        axes[i].set_xlabel('Feature 1')\n",
    "                        axes[i].set_ylabel('Feature 2')\n",
    "                        axes[i].grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # Hide unused subplots\n",
    "                    for i in range(len(successful_algos), len(axes)):\n",
    "                        axes[i].set_visible(False)\n",
    "                    \n",
    "                    plt.suptitle(f'Clustering Results - {dataset_name}', fontsize=16)\n",
    "                    plt.tight_layout()\n",
    "                    save_unsupervised_figure(fig, f\"clustering_results_{dataset_name.lower().replace(' ', '_')}\", \n",
    "                                           f\"Clustering algorithm comparison on {dataset_name} dataset\", \"clustering\")\n",
    "                    plt.close(fig)\n",
    "\n",
    "def save_dimensionality_reduction_analysis():\n",
    "    \"\"\"Save all dimensionality reduction analysis.\"\"\"\n",
    "    print(\"💾 Saving dimensionality reduction analysis...\")\n",
    "    \n",
    "    # Save dimensionality reduction results\n",
    "    if 'successful_reductions' in globals():\n",
    "        n_methods = len(successful_reductions)\n",
    "        n_cols = min(3, n_methods)\n",
    "        n_rows = (n_methods + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1) if n_methods > 1 else [axes]\n",
    "        axes = axes.ravel() if n_methods > 1 else [axes[0]]\n",
    "        \n",
    "        for i, (method_name, X_reduced) in enumerate(successful_reductions.items()):\n",
    "            scatter = axes[i].scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                                    c=y_highdim, cmap='viridis', alpha=0.7)\n",
    "            \n",
    "            axes[i].set_title(f'{method_name}\\n(Time: {reduction_times.get(method_name, 0):.2f}s)')\n",
    "            axes[i].set_xlabel('Component 1')\n",
    "            axes[i].set_ylabel('Component 2')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.colorbar(scatter, ax=axes[i])\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(successful_reductions), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Dimensionality Reduction Results (50D → 2D)', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        save_unsupervised_figure(fig, \"dimensionality_reduction_final\", \n",
    "                               \"Final dimensionality reduction comparison\", \"reduction\")\n",
    "        plt.close(fig)\n",
    "\n",
    "def save_anomaly_detection_analysis():\n",
    "    \"\"\"Save all anomaly detection analysis.\"\"\"\n",
    "    print(\"💾 Saving anomaly detection analysis...\")\n",
    "    \n",
    "    if 'successful_anomaly_algos' in globals() and successful_anomaly_algos:\n",
    "        n_algos = len(successful_anomaly_algos)\n",
    "        n_cols = min(2, n_algos)\n",
    "        n_rows = (n_algos + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 6*n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1) if n_algos > 1 else [axes]\n",
    "        axes = axes.ravel() if n_algos > 1 else [axes[0]]\n",
    "        \n",
    "        for i, (algo_name, result) in enumerate(successful_anomaly_algos.items()):\n",
    "            y_pred = result['y_pred']\n",
    "            \n",
    "            # True positives, false positives, etc.\n",
    "            tp_mask = (y_true_anomaly == -1) & (y_pred == -1)\n",
    "            fp_mask = (y_true_anomaly == 1) & (y_pred == -1)\n",
    "            fn_mask = (y_true_anomaly == -1) & (y_pred == 1)\n",
    "            tn_mask = (y_true_anomaly == 1) & (y_pred == 1)\n",
    "            \n",
    "            axes[i].scatter(X_anomaly[tn_mask, 0], X_anomaly[tn_mask, 1], \n",
    "                           c='lightblue', alpha=0.6, label='True Normal', s=30)\n",
    "            axes[i].scatter(X_anomaly[tp_mask, 0], X_anomaly[tp_mask, 1], \n",
    "                           c='red', alpha=0.8, label='True Anomaly (Detected)', s=50, marker='x')\n",
    "            axes[i].scatter(X_anomaly[fp_mask, 0], X_anomaly[fp_mask, 1], \n",
    "                           c='orange', alpha=0.8, label='False Positive', s=40, marker='s')\n",
    "            axes[i].scatter(X_anomaly[fn_mask, 0], X_anomaly[fn_mask, 1], \n",
    "                           c='purple', alpha=0.8, label='False Negative', s=40, marker='^')\n",
    "            \n",
    "            f1 = result['f1_score']\n",
    "            precision = result['precision']\n",
    "            recall = result['recall']\n",
    "            \n",
    "            axes[i].set_title(f'{algo_name}\\nF1: {f1:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}')\n",
    "            axes[i].set_xlabel('Feature 1')\n",
    "            axes[i].set_ylabel('Feature 2')\n",
    "            axes[i].legend(fontsize=8)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(successful_anomaly_algos), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Final Anomaly Detection Results', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        save_unsupervised_figure(fig, \"anomaly_detection_final\", \n",
    "                               \"Final anomaly detection results\", \"anomaly\")\n",
    "        plt.close(fig)\n",
    "\n",
    "def save_manifold_learning_analysis():\n",
    "    \"\"\"Save all manifold learning analysis.\"\"\"\n",
    "    print(\"💾 Saving manifold learning analysis...\")\n",
    "    \n",
    "    if 'manifold_results' in globals():\n",
    "        for dataset_name, results in manifold_results.items():\n",
    "            successful_algos = [(algo, result) for algo, result in results.items() \n",
    "                               if 'error' not in result]\n",
    "            \n",
    "            if successful_algos:\n",
    "                n_algos = len(successful_algos)\n",
    "                n_cols = min(3, n_algos)\n",
    "                n_rows = (n_algos + n_cols - 1) // n_cols\n",
    "                \n",
    "                fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "                if n_rows == 1:\n",
    "                    axes = axes.reshape(1, -1) if n_algos > 1 else [axes]\n",
    "                axes = axes.ravel() if n_algos > 1 else [axes[0]]\n",
    "                \n",
    "                for i, (algo_name, result) in enumerate(successful_algos):\n",
    "                    X_embedded = result['X_embedded']\n",
    "                    color = result['color']\n",
    "                    \n",
    "                    scatter = axes[i].scatter(X_embedded[:, 0], X_embedded[:, 1], \n",
    "                                            c=color, cmap='viridis', alpha=0.7)\n",
    "                    \n",
    "                    trustworthiness = result['trustworthiness']\n",
    "                    embedding_time = result['embedding_time']\n",
    "                    \n",
    "                    axes[i].set_title(f'{algo_name}\\nTrustworthiness: {trustworthiness:.3f}\\nTime: {embedding_time:.2f}s')\n",
    "                    axes[i].set_xlabel('Component 1')\n",
    "                    axes[i].set_ylabel('Component 2')\n",
    "                    axes[i].grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.colorbar(scatter, ax=axes[i])\n",
    "                \n",
    "                # Hide unused subplots\n",
    "                for i in range(len(successful_algos), len(axes)):\n",
    "                    axes[i].set_visible(False)\n",
    "                \n",
    "                plt.suptitle(f'Final Manifold Learning Results - {dataset_name}', fontsize=16)\n",
    "                plt.tight_layout()\n",
    "                save_unsupervised_figure(fig, f\"manifold_learning_final_{dataset_name.lower().replace(' ', '_')}\", \n",
    "                                       f\"Final manifold learning results for {dataset_name}\", \"manifold\")\n",
    "                plt.close(fig)\n",
    "\n",
    "def save_feature_learning_analysis():\n",
    "    \"\"\"Save all feature learning analysis.\"\"\"\n",
    "    print(\"💾 Saving feature learning analysis...\")\n",
    "    \n",
    "    if 'learned_features' in globals() and learned_features:\n",
    "        n_methods = len(learned_features)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i, (method_name, result) in enumerate(learned_features.items()):\n",
    "            if i < len(axes):\n",
    "                features = result['features']\n",
    "                \n",
    "                if features.shape[1] >= 2:\n",
    "                    scatter = axes[i].scatter(features[:, 0], features[:, 1], \n",
    "                                            c=y_highdim, cmap='viridis', alpha=0.7)\n",
    "                    axes[i].set_title(f'{method_name.replace(\"_\", \" \").title()}\\nFeatures')\n",
    "                    axes[i].set_xlabel(f'Component 1')\n",
    "                    axes[i].set_ylabel(f'Component 2')\n",
    "                    axes[i].grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.colorbar(scatter, ax=axes[i])\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(learned_features), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Final Learned Feature Representations', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        save_unsupervised_figure(fig, \"feature_learning_final\", \n",
    "                               \"Final learned feature representations\", \"features\")\n",
    "        plt.close(fig)\n",
    "\n",
    "def save_all_unsupervised_models():\n",
    "    \"\"\"Save all trained models from unsupervised learning analysis.\"\"\"\n",
    "    print(\"💾 Saving all unsupervised models...\")\n",
    "    \n",
    "    # Save clustering models\n",
    "    if 'clustering_algorithms' in globals():\n",
    "        for name, algorithm in clustering_algorithms.items():\n",
    "            model_metadata = {\n",
    "                'algorithm': name,\n",
    "                'task_type': 'clustering',\n",
    "                'model_class': algorithm.__class__.__name__,\n",
    "                'parameters': algorithm.get_params() if hasattr(algorithm, 'get_params') else {}\n",
    "            }\n",
    "            \n",
    "            # Add performance metrics if available\n",
    "            if 'clustering_results' in globals():\n",
    "                performance_summary = {}\n",
    "                for dataset_name, results in clustering_results.items():\n",
    "                    if name in results and 'error' not in results[name]:\n",
    "                        performance_summary[dataset_name] = results[name]\n",
    "                model_metadata['performance_summary'] = performance_summary\n",
    "            \n",
    "            save_clustering_model(algorithm, name.lower().replace(' ', '_'), \n",
    "                                f\"Clustering model: {name}\", model_metadata)\n",
    "    \n",
    "    # Save dimensionality reduction models\n",
    "    if 'reduction_algorithms' in globals():\n",
    "        for name, algorithm in reduction_algorithms.items():\n",
    "            model_metadata = {\n",
    "                'algorithm': name,\n",
    "                'task_type': 'dimensionality_reduction',\n",
    "                'model_class': algorithm.__class__.__name__,\n",
    "                'parameters': algorithm.get_params() if hasattr(algorithm, 'get_params') else {}\n",
    "            }\n",
    "            \n",
    "            # Add performance metrics if available\n",
    "            if 'reduction_results' in globals() and name in reduction_results and reduction_results[name] is not None:\n",
    "                model_metadata['reduction_time'] = reduction_times.get(name, 'unknown')\n",
    "                model_metadata['output_shape'] = reduction_results[name].shape\n",
    "            \n",
    "            save_reduction_model(algorithm, name.lower().replace(' ', '_'), \n",
    "                               f\"Dimensionality reduction model: {name}\", model_metadata)\n",
    "    \n",
    "    # Save anomaly detection models\n",
    "    if 'anomaly_algorithms' in globals():\n",
    "        for name, algorithm in anomaly_algorithms.items():\n",
    "            if algorithm is not None:\n",
    "                model_metadata = {\n",
    "                    'algorithm': name,\n",
    "                    'task_type': 'anomaly_detection',\n",
    "                    'model_class': algorithm.__class__.__name__,\n",
    "                    'parameters': algorithm.get_params() if hasattr(algorithm, 'get_params') else {}\n",
    "                }\n",
    "                \n",
    "                # Add performance metrics if available\n",
    "                if 'anomaly_results' in globals() and name in anomaly_results and 'error' not in anomaly_results[name]:\n",
    "                    model_metadata['performance_metrics'] = anomaly_results[name]\n",
    "                \n",
    "                save_clustering_model(algorithm, f\"anomaly_{name.lower().replace(' ', '_')}\", \n",
    "                                    f\"Anomaly detection model: {name}\", model_metadata)\n",
    "    \n",
    "    # Save manifold learning models\n",
    "    if 'manifold_algorithms' in globals():\n",
    "        for name, algorithm in manifold_algorithms.items():\n",
    "            model_metadata = {\n",
    "                'algorithm': name,\n",
    "                'task_type': 'manifold_learning',\n",
    "                'model_class': algorithm.__class__.__name__,\n",
    "                'parameters': algorithm.get_params() if hasattr(algorithm, 'get_params') else {}\n",
    "            }\n",
    "            \n",
    "            # Add trustworthiness scores if available\n",
    "            if 'manifold_results' in globals():\n",
    "                trustworthiness_summary = {}\n",
    "                for dataset_name, results in manifold_results.items():\n",
    "                    if name in results and 'error' not in results[name]:\n",
    "                        trustworthiness_summary[dataset_name] = results[name].get('trustworthiness', 'unknown')\n",
    "                model_metadata['trustworthiness_summary'] = trustworthiness_summary\n",
    "            \n",
    "            save_reduction_model(algorithm, f\"manifold_{name.lower().replace(' ', '_')}\", \n",
    "                               f\"Manifold learning model: {name}\", model_metadata)\n",
    "    \n",
    "    # Save feature learning models\n",
    "    if 'feature_learner' in globals() and hasattr(feature_learner, 'feature_transformers'):\n",
    "        for name, transformer in feature_learner.feature_transformers.items():\n",
    "            model_metadata = {\n",
    "                'algorithm': name,\n",
    "                'task_type': 'feature_learning',\n",
    "                'model_class': transformer.__class__.__name__,\n",
    "                'parameters': transformer.get_params() if hasattr(transformer, 'get_params') else {}\n",
    "            }\n",
    "            \n",
    "            # Add feature learning metrics if available\n",
    "            if 'learned_features' in globals() and name in learned_features:\n",
    "                feature_info = learned_features[name]\n",
    "                model_metadata['output_dimensions'] = feature_info['features'].shape[1]\n",
    "                model_metadata['feature_variance'] = np.var(feature_info['features'])\n",
    "                \n",
    "                # Method-specific metrics\n",
    "                if name == 'dictionary' and 'sparsity' in feature_info:\n",
    "                    model_metadata['sparsity'] = feature_info['sparsity']\n",
    "                elif name == 'ica' and 'independence_score' in feature_info:\n",
    "                    model_metadata['independence_score'] = feature_info['independence_score']\n",
    "                elif name == 'factor_analysis' and 'log_likelihood' in feature_info:\n",
    "                    model_metadata['log_likelihood'] = feature_info['log_likelihood']\n",
    "            \n",
    "            save_feature_learning_model(transformer, name, \n",
    "                                      f\"Feature learning model: {name}\", model_metadata)\n",
    "\n",
    "def generate_unsupervised_learning_report():\n",
    "    \"\"\"Generate comprehensive unsupervised learning analysis report.\"\"\"\n",
    "    print(\"📄 Generating comprehensive report...\")\n",
    "    \n",
    "    report_content = f\"\"\"\n",
    "# Sklearn-Mastery Unsupervised Learning Report\n",
    "Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This report summarizes the comprehensive unsupervised learning analysis performed\n",
    "in the sklearn-mastery project, including advanced clustering algorithms, \n",
    "dimensionality reduction techniques, anomaly detection, manifold learning,\n",
    "and unsupervised feature learning methods.\n",
    "\n",
    "## Clustering Analysis\n",
    "\n",
    "### Datasets Tested\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    if 'test_datasets' in globals():\n",
    "        report_content += f\"Total clustering datasets: {len(test_datasets)}\\n\\n\"\n",
    "        for dataset_name, (X, y) in test_datasets.items():\n",
    "            report_content += f\"\"\"\n",
    "**{dataset_name}**\n",
    "- Shape: {X.shape}\n",
    "- Features: {X.shape[1]}\n",
    "- True Clusters: {len(np.unique(y))}\n",
    "- Characteristics: {\"2D visualization available\" if X.shape[1] == 2 else \"High-dimensional dataset\"}\n",
    "\"\"\"\n",
    "    \n",
    "    if 'clustering_results' in globals():\n",
    "        report_content += \"\\n### Clustering Performance Summary\\n\"\n",
    "        \n",
    "        # Find best performing algorithm per dataset\n",
    "        for dataset_name in clustering_results.keys():\n",
    "            successful_results = {k: v for k, v in clustering_results[dataset_name].items() \n",
    "                                 if 'error' not in v}\n",
    "            if successful_results:\n",
    "                best_algo = max(successful_results.keys(), \n",
    "                               key=lambda x: successful_results[x].get('adjusted_rand_index', -1))\n",
    "                best_ari = successful_results[best_algo].get('adjusted_rand_index', 'N/A')\n",
    "                best_sil = successful_results[best_algo].get('silhouette_score', 'N/A')\n",
    "                \n",
    "                report_content += f\"\"\"\n",
    "**{dataset_name}**\n",
    "- Best Algorithm: {best_algo}\n",
    "- Adjusted Rand Index: {best_ari:.3f if isinstance(best_ari, float) else best_ari}\n",
    "- Silhouette Score: {best_sil:.3f if isinstance(best_sil, float) else best_sil}\n",
    "\"\"\"\n",
    "    \n",
    "    report_content += \"\\n## Dimensionality Reduction Analysis\\n\"\n",
    "    \n",
    "    if 'reduction_results' in globals():\n",
    "        successful_reductions = {k: v for k, v in reduction_results.items() if v is not None}\n",
    "        report_content += f\"Successful reduction methods: {len(successful_reductions)}\\n\\n\"\n",
    "        \n",
    "        for method_name, result in successful_reductions.items():\n",
    "            time_taken = reduction_times.get(method_name, 'unknown')\n",
    "            report_content += f\"\"\"\n",
    "**{method_name}**\n",
    "- Output Shape: {result.shape}\n",
    "- Processing Time: {time_taken:.3f}s\n",
    "- Variance Preserved: {np.var(result):.3f}\n",
    "\"\"\"\n",
    "    \n",
    "    report_content += \"\\n## Anomaly Detection Analysis\\n\"\n",
    "    \n",
    "    if 'anomaly_results' in globals():\n",
    "        successful_anomaly = {k: v for k, v in anomaly_results.items() if 'error' not in v}\n",
    "        report_content += f\"Successful anomaly detection methods: {len(successful_anomaly)}\\n\\n\"\n",
    "        \n",
    "        if successful_anomaly:\n",
    "            best_anomaly_algo = max(successful_anomaly.keys(), \n",
    "                                   key=lambda x: successful_anomaly[x]['f1_score'])\n",
    "            best_f1 = successful_anomaly[best_anomaly_algo]['f1_score']\n",
    "            \n",
    "            report_content += f\"\"\"\n",
    "### Best Performing Anomaly Detector\n",
    "- Algorithm: {best_anomaly_algo}\n",
    "- F1-Score: {best_f1:.3f}\n",
    "- Precision: {successful_anomaly[best_anomaly_algo]['precision']:.3f}\n",
    "- Recall: {successful_anomaly[best_anomaly_algo]['recall']:.3f}\n",
    "\n",
    "### All Methods Performance\n",
    "\"\"\"\n",
    "            for algo_name, metrics in successful_anomaly.items():\n",
    "                report_content += f\"\"\"\n",
    "**{algo_name}**\n",
    "- F1-Score: {metrics['f1_score']:.3f}\n",
    "- Precision: {metrics['precision']:.3f}\n",
    "- Recall: {metrics['recall']:.3f}\n",
    "- Accuracy: {metrics['accuracy']:.3f}\n",
    "\"\"\"\n",
    "    \n",
    "    report_content += \"\\n## Manifold Learning Analysis\\n\"\n",
    "    \n",
    "    if 'manifold_results' in globals():\n",
    "        report_content += f\"Manifold datasets tested: {len(manifold_datasets)}\\n\\n\"\n",
    "        \n",
    "        # Calculate average trustworthiness per algorithm\n",
    "        algo_trustworthiness = {}\n",
    "        for dataset_name, results in manifold_results.items():\n",
    "            for algo_name, result in results.items():\n",
    "                if 'error' not in result:\n",
    "                    if algo_name not in algo_trustworthiness:\n",
    "                        algo_trustworthiness[algo_name] = []\n",
    "                    algo_trustworthiness[algo_name].append(result['trustworthiness'])\n",
    "        \n",
    "        if algo_trustworthiness:\n",
    "            report_content += \"### Manifold Learning Performance (Average Trustworthiness)\\n\"\n",
    "            for algo_name, scores in algo_trustworthiness.items():\n",
    "                avg_trust = np.mean(scores)\n",
    "                report_content += f\"- **{algo_name}**: {avg_trust:.3f}\\n\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
